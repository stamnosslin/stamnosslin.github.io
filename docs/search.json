[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RMS notes",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome-to-research-methods-1-and-stat-1",
    "href": "index.html#welcome-to-research-methods-1-and-stat-1",
    "title": "RMS notes",
    "section": "Welcome to Research Methods 1 and Stat 1",
    "text": "Welcome to Research Methods 1 and Stat 1\nIn this book I collect my notes to the courses Research Methods 1 and Statistics 1, both part of the Master program in psychology, at Department of Psychology, Stockholm University. Lecture notes for Method 1 are in odd numbered chapters, notes for Stat 1 are in even numbered chapters. I will update the notes as we go along.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#important-concepts",
    "href": "index.html#important-concepts",
    "title": "RMS notes",
    "section": "Important concepts",
    "text": "Important concepts\nThis is a test\n\n\nCode\nlibrary(readxl)\nlibrary(DT)\n\ng &lt;- read_excel(\"rms_concepts25.xlsx\", range = \"Method!A1:F108\")\ng &lt;- as.data.frame(g)\ndatatable(g, rownames = FALSE)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-method25.html",
    "href": "01-method25.html",
    "title": "1  M: Research Questions and Claims",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "01-method25.html#topics",
    "href": "01-method25.html#topics",
    "title": "1  M: Research Questions and Claims",
    "section": "",
    "text": "Science and everyday thinking\n\nTruth: Dichotomous (true or false)\nJustification: A matter of degree\nResist dichotomous thinking, embrace uncertainty\n\nResearch question\n\nGive context (research area)\nMotivate why important (research gap)\nFormulate precisely (exposure and outcome variables(s), study population)\nChose research design\n\nError\n\nRandom error\nSystematic error = bias\n\nValidity of research claims\n\nCampbell’s validity typology:\n\nStatistical conclusion validity\nInternal validity\nConstruct validity\nExternal validity\n\nThreats to validity\nDesign elements to counteract threats to validity\n\n\nTheoretical articles to read:\n\nSteiner et al. (2023) on frameworks for causal inference, the sections on Campbell’s validity typology/threats framework\nGelman et al. (2021) mentions internal and external validity as several places, see for example chapter 18, pp. 354-355, where they use the term “threats to validity” and discusses issues related to validity, such as the “Hawthorne effect” and experimenter effects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "01-method25.html#science-and-everyday-thinking",
    "href": "01-method25.html#science-and-everyday-thinking",
    "title": "1  M: Research Questions and Claims",
    "section": "1.1 Science and everyday thinking",
    "text": "1.1 Science and everyday thinking\n\nScience is nothing more than a refinement of everyday thinking.\nAlbert Einstein (cited in Haack, 2011)\n\n\nScience is nothing more than a refinement of everyday thinking. It’s the application of common sense, understood as “good sense” or “sound reason” or “logical thinking”. There is only one logic, and it applies to all rational activities, be it research, criminal investigations, medical diagnosis, or political decisions.\nUnfortunately, our intuitions often lead us wrong, and what may appear a sensible conclusion may turn out otherwise. The “refinement of everyday thinking” is the wisdom that researcher has gained from thinking about problems. Thinking a bit harder and in terms of alternative explanations may help. Let’s practice on this statement:\nIndustrial workers are healthier than the general population. Thus, work in industry is good for your health.\nPlease find alternative explanations.\n\nJustification\nResearch aims at increasing our knowledge, typically factual knowledge about the world. Philosophers may debate how to best define “knowledge”, but most would agree that truth and justification are the two key elements. To know something requires that it is true. You may believe that the earth is flat as a pancake, but you cannot know it because your belief is false. But truth is not enough. I may claim that there are living organisms on the planet Jupiter. If such organisms indeed are discovered a hundred years from now, I was right, but still I didn’t know, because my claim was unjustified, I lacked good reasons. Maybe in a dream I saw small germs crawling the surface of Jupiter. Dreams are not good reasons for believing things and therefor may claim would be dismissed as unjustified.\nWhat is more important, being true or being justified? For a researcher, the answer is straight forward, research is all about justification. A scientific claim is only taken seriously if justified. It happens that justified claims turn out to be false, because the agreement between truth and the presence of good reasons is not perfect. But, luckily, it is typically much easier to find good reasons for true than for false claims. As every defense attorney knows, it is typically easier to defend an innocent client than one guilty as charged. Simply because the prosecutor will have a harder time finding good reasons for guilt of innocent suspects.\nThe primacy of justification over truth is old news:\n\nHerodotus, in about 500 BC, discusses the policy decisions of the Persian kings. He notes that a decision was wise, even though it led to disastrous consequences, if the evidence at hand indicated it as the best one to make; and that a decision was foolish, even though it led to the happiest possible consequences, if it was unreasonable to expect those consequences.\nCited in Jaynes (1985) \n\n\nJustification is a matter of degree, so research claims are more or less justified. That is why researchers typically speak of their results as providing some degree of support of their hypothesis, not as conclusive evidence for or against. Here examples of phrases, from weak to strong support:\n\nOur results are consistent with the notion that …\nOur results may suggest that …\nOur results weakly support our hypothesis that …\nOur results provide clear evidence for our hypothesis that …\nOur results strongly supports our hypothesis that …\nOur results, together with previous research, leave little doubt that …\n\nThere will always be room for uncertainty in empirical research, and that is why researchers think and talk about there results as more or less in support of their hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "01-method25.html#research-question",
    "href": "01-method25.html#research-question",
    "title": "1  M: Research Questions and Claims",
    "section": "1.2 Research question",
    "text": "1.2 Research question\nAs essential as it is to justify our research claims, it is equally critical to ask the right question. The questions we pose drive the research design, guiding the collection and analysis of data from which we draw and justify our conclusions. However, formulating the right question is no small task; it demands both creativity and insight. This process involves identifying gaps in existing knowledge and intuitively exploring new, promising directions. If successful, these new paths can significantly advance our research field; if not, they may lead to another dead end, a common outcome in research.\nOnce a general research idea is in place, considerable effort must be invested in precisely articulating the research question. In research papers, this question is typically presented in the Introduction, following a structured approach:\n\nThe opening paragraph(s) establish the broader context for the research problem.\nA literature review identifies the specific knowledge gap that necessitates the question.\nThe research question is then concisely and precisely formulated, usually specifying the independent variable(s), dependent variable, and unit of analysis.\nA brief overview of the research design chosen to address the question is provided at the end of the Introduction.\n\nNote that the research question can also be introduced as a hypothesis. In such cases, the goal of the research is to evaluate this hypothesis.\nBelow is a practice question (1E1) that involves analyzing how the research problem was described and justified in the Introduction of a recent paper.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "01-method25.html#random-and-systematic-error",
    "href": "01-method25.html#random-and-systematic-error",
    "title": "1  M: Research Questions and Claims",
    "section": "1.3 Random and Systematic error",
    "text": "1.3 Random and Systematic error\nTwo types of error:\n\nRandom error. The sum of many unknown and independent errors decrease with study size (the number of observations) or reliability of measurements. We will talk about three types of random error:\n\nRandom measurement error. Measurements may underestimate or overestimate true scores, despite high test validity.\nSampling error. Random sampling of participants from a target population may still lead to unrepresentative samples.\nRandomization error. Random assignment of participants to treatment to groups may still lead to unbalanced groups.\n\nSystematic error, also known as bias. Stays constant no matter the study size or reliability of tests. Examples of bias:\n\nConfounding bias\nSelection bias\nMeasurement bias (related to validity of test scores)\n\n\n\n\nCode\n# Plot settings\npar(mar = c(2, 2, 0, 2), \n    mgp = c(0.2, 0.5, 0)) # First argument mgp, distance label to axis\n\n# Data to plot\nx &lt;- 1:1000\ny_random &lt;- 1/sqrt(x)\ny_systematic &lt;- rep(0.6, length(x))\n\n# Plot\nplot(log(x), y_random, pch = '', axes = FALSE, \n     xlab = \"N\", ylab = \"Error\", cex.lab = 0.9)\nlines(log(x), y_random, lty = 2, col = \"blue\")\nlines(log(x), y_systematic, col = \"red\")\naxis(1, at = c(0, 7), labels = c(\"\", \"\"), pos = 0, tck = 0)\naxis(2, at = c(0, 1), labels = c(\"0\", \"\"), pos = 0, tck = 0, las = 2,\n     cex.axis = 0.7)\n\n# Add text to figure\ntext(x = 1, y = 1, \"Random error\", cex = 0.8, col = \"blue\", font = 3)\ntext(x = 2.1, y = 0.65, \"Systematic error (bias)\", cex = 0.8, \n     col = \"red\", font = 3)\n\n\n\n\n\n\n\n\nFigure 1.1: Error and study size. Redrawn from Fig. 7.1 in Rothman (2012).\n\n\n\n\n\n\nN may refer to several things:\n\nThe number of tested individuals (sample size) over which scores are averaged.\n\nNumber of items used to create a sum score per individual.\n\nNumber of stimulus repetitions per individual. The participants response to a given stimulus is calculated as the average response to a specific stimulus.\n\nThus, the figure applies equally well to errors of group averages as to measurement at the individual level (reliability and validity of test scores.) Averaging over repeated measures (e.g., many items in a questionnaire or many repetitions of the same stimulus in a psychophysical experiment) reduces random error but not systematic error (see also Chapter 5).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "01-method25.html#validity-types-threats-and-tricks",
    "href": "01-method25.html#validity-types-threats-and-tricks",
    "title": "1  M: Research Questions and Claims",
    "section": "1.4 Validity: Types, Threats, and Tricks",
    "text": "1.4 Validity: Types, Threats, and Tricks\nCampbell and co-workers developed a qualitative approach to causal inference that has been very influential in psychology and related fields. It is a description of how researchers thinks (or should think) when designing a study or when evaluating evidence from a published study. It encourage us to think in terms of alternative explanations (threats to validity) and to counteract these threats with clever design elements (or design “tricks”).\nThree steps\n\nSpecify validity type.\nIdentify threats to validity.\nApply design elements (tricks) to counteract threats to validity. i.e., to rule out plausible alternative explanations.\n\n\nHere is how Steiner et al. (2023) define the validity types:\n\nInternal validity. The validity of inferences about whether the observed association between treatment status T and outcome Y reflects a causal impact of T on Y. Thus, internal validity is about the assumptions required for causal inference.\n\nStatistical conclusion validity. The validity of inferences about the association (covariation) between the presumed treatment T and outcome Y. Thus, this validity type is about the assumptions required for making statistical inference from the realized randomization or sampling outcomes to the underlying target populations of possible treatment assignments and participants.\n\nConstruct validity. The validity with which inferences are made from the operations and settings in a study to the theoretical constructs those operations and settings are intended to represent. It is about the correct labeling of variables and accurate language use.\n\nExternal validity. The validity of inferences about whether the observed cause-effect relationship holds over variations in participants, settings, treatments, outcomes, and times. External validity directly relates to the assumptions required for generalizing effects\n\nCampbell’s typology was developed in a time when psychology research methods and statistics largely focused on whether associations were significant or non-significant. This kind of dichotomous thinking has many problems (see for example, Amrhein et al., 2019).\nThis table is my attempt to apply the typology to research aiming at estimating effect sizes, rather than categorizing results as significant versus non-significant.\n\n\n\n\n\n\n\n\n\nValidity type\nQuestion\nIssue\n\n\n\n\nStatistical conclusion validity\nEstimates too uncertain?\nStatistical inference\n\n\nInternal validity\nAre causal estimates biased?\nCausal inference\n\n\nConstruct validity\nWhat was measured, really?\nLanguage use (labeling of variables)\n\n\nExternal validity\nRelevant outside the study context?\nGeneralization\n\n\n\n\n\nExample study\nWe conducted a between-subjects experiment where participants performed a simple task (proof-reading) either in quiet (control group) or while exposed to loud noise (treatment group) from loudspeakers in our sound-proof laboratory. We hypothesized that the treatment group would, on average, perform substantially worse than the control group on our outcome measure: the number of errors made on the simple task. This error count served as our measure of the construct ‘noise-induced distraction’.\nValidity:\n\nStatisticalInternalConstructExternal\n\n\n\nQuestion: Are effect-size estimates precise enough to say anything about direction and practical relevance of the results?\nThreat: Large compatibility intervals due to small sample sizes. Results inconclusive, as the observed data would be compatible with large as well as negligible or even reversed effects.\nDesign element: Increased sample size to improve the precision of the estimated group difference (i.e., to obtain narrower compatibility intervals).\n\n\n\n\nQuestion: Would we be justified in claiming that the observed group difference is a valid estimate of the average causal effect of the treatment on performance in our sample?\nThreat. Group-threat: Maybe the groups were unequal to start with on factors related to performance. This systematic error may lead to underestimation or overestimation of the causal effect in the sample.\nDesign element: Random assignment of participants to treatment groups, to obtain groups balanced on potentially confounding variables (measured and unmeasured).\n\n\n\n\nQuestion: Would we be justified in claiming that the observed effect is relevant to the construct of interest?\nThreat. Maybe performance errors in our laboratory is not a valid measure of our target construct (noise-induced distraction).\nDesign element: Multiple outcome measures, for example, self-reported distraction and eye-movements (to check how eyes follows the text). If they all point in the same direction, this would make us more justified in generalizing from measurements to construct.\n\n\n\n\nQuestion: Would we be justified in claiming that the size of the causal effect applies in a realistic environment (and not only to the participants in our experiment)?\nThreat. People may behave different in our unrealistic setting (sound laboratory) than in real life, such as in a work place.\nDesign element: Conduct the experiment in a real workplace setting using concealed loudspeakers, ensuring that the purpose of the study remains masked from the workers, as much as ethical guidelines permit.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "01-method25.html#practice",
    "href": "01-method25.html#practice",
    "title": "1  M: Research Questions and Claims",
    "section": "Practice",
    "text": "Practice\nThe practice problems are categorized as Easy (E), Medium (M), and Hard (H). Credit goes to Richard McElreath and his excellent book, “Statistical Rethinking” (2020), from which I adapted this layout of practice problems.\n\nEasy\n\n1E1. Read the Introduction section of Michal & Shah (2024) and identify the sentence(s) where they:\n\nIntroduce the research area.\n\nIdentify the knowledge gap that justifies the research.\n\nFormulate the research question or hypothesis.\n\nDescribe and justify their design strategy.\n\nRephrase (c) into a single question that incorporates the key independent and dependent variables, along with the study units.\n\nFind article here\n\n\n\n1E2. Explain the difference between random and systematic error, with reference to:\n\nScores on an exam in Research Methods.\nDifference in average grades between two schools in a specific year.\nAverage scores of control and treatment group in a randomized experiment.\n\n\n\n\n1E3. Please find alternative explanations:\n\n“Industrial workers are healthier than the general population. Thus, work in industry is good for your health.”\n“Children in schools exposed to aircraft noise have higher grades than children in non-exposed schools. Thus, aircraft noise cannot have adverse effects on children’s learning.”\n“Productivity increased after we improved the lighting. This proves that good lighting conditions is important for productivity.”\n“With increasing traffic volumes, exposure to residential road-traffic noise has increased substantially over the last decades. In the same period, we have seen a remarkable reduction in the number of heart attacks. Thus, road-traffic noise cannot cause heart attacks, as some noise researchers seem to suggest.”\n\n\n\n\n\n1E4. The design elements random selection of participants from a population and random assignment of participants to treatment conditions serve distinct purposes.\nExplain these purposes in the context of Campbell’s validity typology.\n This is an old exam question: \n\n\n\n\nMedium\n\n1M1.\n\nExplain with an example how the “placebo effect” may pose a threat to the validity of a research claim.\nThe placebo effect is often thought of as a threat to internal validity, but may be better viewed as a threat to construct validity. Explain.\n\n Note. And remember: Understanding why a threat to validity, like the placebo effect, might mislead us is crucial; accurately classifying it within Campbell’s typology is less important. \n\n\n\n1M2. Campbell’s validity typology can be simplified into two main categories: internal and external validity. In which of these categories would you place\n\n\n1M3. Please find alternative explanations:\n\n“The top 10 schools in terms of average performance on the national standardized tests were all small schools with fewer than 500 pupils. Reducing school size would increase school performance.”\n“A flurry of deaths by natural causes in a village led to speculation about some new and unusual threat. A group of priests attributed the problem to the sacrilege of allowing women to attend funerals, formerly a forbidden practice. The remedy was a decree that barred women from funerals in the area. The decree was quickly enforced, and the rash of unusual deaths subsided. This proves that the priests were correct.\n\n b. taken from the excellent book “How We Know What Isn’t So” Gilovich (2008).  :::\n\n\nHard\n\n1H1. Refer to Figure 1.1 to answer the following questions:\n\nWhat function, \\(Error = f(N)\\), was used to draw the blue line? (Check out the code behind the figure.)\n\nWhy do you think this function was used?\n\n\n\n\n1H2. “The reviewer raised concerns about the small sample size, but this should not be an issue, given that the difference I observed was highly statistically significant.”\nExplain why the reviewer had a point despite the statistical significance of the findings.\n Note. You may consult Gelman et al. (2021), Ch. 16.1: The winner’s curse in low-power studies.\n\n\n\n\n1H3.\n“Babies to smoking mothers have an increased risk of mortality within one year, compared to babies to non-smoking mothers. However, among babies born with low birth weight (LBW), the opposite seems to be true, so a smoking mother is protective for LBW babies”.\nPlease find an alternative explanation.\n Note. This one is hard, we will discuss it later on, but if you want to jump ahead, please check out Banack & Kaufman (2014).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "01-method25.html#session-info",
    "href": "01-method25.html#session-info",
    "title": "1  M: Research Questions and Claims",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.3   \n\n\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Retire statisticial significance. Nature, 567, 305–307.\n\n\nBanack, H. R., & Kaufman, J. S. (2014). The obesity paradox: Understanding the effect of obesity on mortality among individuals with cardiovascular disease. Preventive Medicine, 62, 96–102.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nGilovich, T. (2008). How we know what isn’t so. Simon; Schuster.\n\n\nHaack, S. (2011). Defending science-within reason: Between scientism and cynicism. Prometheus Books.\n\n\nJaynes, E. T. (1985). Bayesian methods: General background (J. H. Justice, Ed.). Cambridge University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nMichal, A. L., & Shah, P. (2024). A practical significance bias in laypeople’s evaluation of scientific findings. Psychological Science, 35(4), 315–327.\n\n\nRothman, K. J. (2012). Epidemiology: An introduction. Oxford university press.\n\n\nSteiner, P. M., Shadish, W. R., & Sullivan, K. J. (2023). Frameworks for causal inference in psychological science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>M: Research Questions and Claims</span>"
    ]
  },
  {
    "objectID": "02-stat25.html",
    "href": "02-stat25.html",
    "title": "2  S: Data Management and Screening",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>S: Data Management and Screening</span>"
    ]
  },
  {
    "objectID": "02-stat25.html#topics",
    "href": "02-stat25.html#topics",
    "title": "2  S: Data Management and Screening",
    "section": "",
    "text": "Reproducibility and Replicability\nData management\n\nRaw data file(s)\nCode book\nData analysis script(s)\n\nInitial data screening, to find errors in raw data. Helpful R functions:\n\nhead(), tail()\nstr()\nsummary()\nhist(), boxplot()\n\nProbability (basics)\n\nJoint probability\nConditional probability\nIndependence (not the same as disjoint!)\nBayes rule\n\n\nReadings:\n\nDo not forget to read Wilkinson (1999)! Although a bit old it is still an excellent guide to data analysis. For their view on initial data screening, see “Results: Complications” (pp. 597-598).\nGelman et al. (2021), ch. 3 on probability",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>S: Data Management and Screening</span>"
    ]
  },
  {
    "objectID": "02-stat25.html#data-management",
    "href": "02-stat25.html#data-management",
    "title": "2  S: Data Management and Screening",
    "section": "2.1 Data management",
    "text": "2.1 Data management\n\nOpen Science\nOpen science is an umbrella term for a collection of values, practices and movements. The two main principles of Open Science ” are\n\n\nTransparency, Open science aims at making the scientific processes more transparent.\n\n\nAccessibility, Open science aims at making research results more accessible.\n\n\n\n\n\nReproducibility and Replicability\nAmerican Statistical Association endorse the following definitions (Broman et al. (2017)):\n\nReproducibility: A study is reproducible if you can take the original data and the computer code used to analyze the data and reproduce all of the numerical findings from the study. This may initially sound like a trivial task but experience has shown that it’s not always easy to achieve this seemingly minimal standard.\nReplicability: This is the act of repeating an entire study, independently of the original investigator without the use of original data (but generally using the same methods).\n\n\n\n\nRaw data files\nManagement\n\nThere should only be one version of the raw data files(s). If several people are involved in the analysis, one should be assigned as data host, and be responsible for keeping the raw data.\nThe raw data file(s) should be consider as read-only files. Once the raw data files is finalized (obvious errors fixed), the raw data file should not be touched. Do never include created variables in the raw data file, and never remove outliers or impute new data in the raw data file.\nEvery new data-analysis sessions should start with importing the raw data into your statistical software (i.e., R). New variables, removal of outliers, etc. are coded in your analysis script, and repeated each time you start a new analysis session.\nOnly change the raw data file if obvious errors are discovered. If someone in an analysis team discovers an error, then he or she reports it to the data host who is the only one allowed to make changes to the raw data file.\n\nCode book\nA detailed description of the data set, with information on how to interpret column names. The more information the better (within reason), for example regarding measurement units and codes for missing data (e.g., -999, or NA).\nThe code book may be given in …\n\na separate document,\nthe script file,\n\nthe raw data file. The first lines may be devoted to code book. For example, lines starting with special character, such as #, may be ignored when reading data into R. Example\n\n\n\n\nData analysis script(s)\n\nOutlier exclusions, imputation of missing data (never in raw data file)\nDefinitions of created variables\nData analyses\nFigures and tables\nDON’T Do things by hand\n\ndon’t manually clean spreadsheet data\ndon’t move files around your computer (splitting/reformatting data files, …)\ndon’t edit tables or figures by hand\n\nDON’T Use point-and-click software. Ease of use can lead to non-reproducible analyses\nDON’T Save outputs during analysis work. Figures and tables should be defined in scripts and redrawn each analysis session. Save figures and tables only when it is time to publish your work.\n\n\n\n\nWrite nice code\nTo be reproducible in practice (and not only in theory), data files and code scripts have to be easy to understand for everyone, without background knowledge of your study and data. Note: This one may the future you. Therefore, spend time to write nice code:\n\nMeaningful variable names (prefer “anxiety_score” over “x”)\nAnnotate a lot!\nFollow coding guidelines, for example, Wickham’s\n\n\n\n\nLiterate programming\nPopular tools:\n\nRmarkdown was my first experience with literate programming. I have moved on to:\nQuarto The new Rmarkdown, try it out in Rstudio.\nJupyter is another tool.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>S: Data Management and Screening</span>"
    ]
  },
  {
    "objectID": "02-stat25.html#initial-data-screening",
    "href": "02-stat25.html#initial-data-screening",
    "title": "2  S: Data Management and Screening",
    "section": "2.2 Initial data screening",
    "text": "2.2 Initial data screening\nI will say this many times: Know your data! Or, as a professor once told me years ago, “You need to be on top of your data.” The very first step to a comprehensive understanding of your data is to screen the data to detect errors.\nLet’s create some fake data, with known errors to be detected. It is about hearing status in a sample of industrial workers aged between 20 and 65 years old.\nCode book:\n\nid Id-number unique to each participant\nage Age of participant, between 20 and 65 years\npta4 Hearing status measured in dB (average absolute tone threshold) with possible values between -10 and 80 dB (lower means better hearing).\n\n\n\nCode\n# Set seed to make the following reproducible!\nset.seed(123) \n\n# Number of participants in the study\nn &lt;- 760  \n\n# ID-number unique for each participants\nid &lt;- 1:n  \n\n# Sampling age from normal distributions\nage &lt;-  rnorm(n, mean = 45, sd = 5) \n\n# Sampling hearing status (pta4 [dB], low values is better) from normal\n# and making hearing gets worse with age, about 2 dB per 10 years \npta4 &lt;- rnorm(n, mean = 10, sd = 5) + 0.2*age  \n\n# Round to integers to make numbers more realistic\nage &lt;- round(age)\npta4 &lt;- round(pta4)\n\n# Add some errors\nage[126]  &lt;- 999  # ID 126 has age = 999 years\nage[231]  &lt;- 87   # ID 231 has age = 87 years\npta4[5]   &lt;- 227  # ID 5 has pta4 =  227 dB\npta4[543] &lt;- 999  # ID 543 has pta4 = 999 dB\n\n# Make data frame\nd &lt;- data.frame(id = id, age = age, pta4 = pta4)\n\n# Remove variables to just keep the data frame\nrm(n, id, age, pta4)\n\n\n\nI stored the fake data in a data frame called d. Please copy the code above,\nrun it in R and pretend that it’s a real data set imported to R. Let’s start with initial screening of the data frame.\n\n\nCode\n# Initial data screening of our data\nhead(d)  # Shows the first few lines of the data frame\n\n\n  id age pta4\n1  1  42   21\n2  2  44   22\n3  3  53   24\n4  4  45   11\n5  5  46  227\n6  6  54   26\n\n\nCode\ntail(d)  # Shows the last few lines of the data frame\n\n\n     id age pta4\n755 755  44   17\n756 756  44   22\n757 757  50   19\n758 758  44   22\n759 759  35   16\n760 760  44   17\n\n\nCode\nstr(d)   # R description of data frame\n\n\n'data.frame':   760 obs. of  3 variables:\n $ id  : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age : num  42 44 53 45 46 54 47 39 42 43 ...\n $ pta4: num  21 22 24 11 227 26 26 17 17 26 ...\n\n\nCode\nsummary(d) # Some basic stats, including information on missing data = NA\n\n\n       id             age              pta4       \n Min.   :  1.0   Min.   : 31.00   Min.   :  6.00  \n 1st Qu.:190.8   1st Qu.: 42.00   1st Qu.: 16.00  \n Median :380.5   Median : 45.00   Median : 19.00  \n Mean   :380.5   Mean   : 46.38   Mean   : 20.73  \n 3rd Qu.:570.2   3rd Qu.: 48.00   3rd Qu.: 23.00  \n Max.   :760.0   Max.   :999.00   Max.   :999.00  \n\n\n\nPlots may discover suspicious observations not shown by the screening above:\n\n\nCode\n# Boxplot\npar(mfrow = c(1, 2)) # Two plots next to each other\nboxplot(d$age, ylab = \"Age [years]\") \nboxplot(d$pta4, ylab = \"Hearing status, PTA4 [dB]\") \n\n\n\n\n\n\n\n\n\n\nFind data outside the specified age interval [20, 65], and hearing ability interval [-10, 80]:\n\n\nCode\n# Finding ids with age and hearing status that should not be in the sample\nage_out  &lt;- d$id[d$age &lt; 20 | d$age &gt; 65 ]  \npta4_out &lt;- d$id[d$pta4  &lt; -10 | d$pta4  &gt;  80] \n\n# Print data frame rows with\noutrows &lt;- d$id %in% c(age_out, pta4_out)  # Find rows with outlier data\nd[outrows, ]  # Look at data frame for these rows\n\n\n     id age pta4\n5     5  46  227\n126 126 999   21\n231 231  87   24\n543 543  49  999\n\n\n\nFix data (in script, not in raw data file!). You would put code like this at the top of your data analysis script. Note: Advanced users may put it in a separate script that is called by the main analysis script(s).\n\n\nCode\ng &lt;- d  # new data frame called g\ng$age[age_out] &lt;- NA  # Make impossible ages missing data, NA\ng$pta4[pta4_out] &lt;- NA  # Make impossible hearing-status missing data, NA\n\n# Screen data frame g\nstr(g)\n\n\n'data.frame':   760 obs. of  3 variables:\n $ id  : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age : num  42 44 53 45 46 54 47 39 42 43 ...\n $ pta4: num  21 22 24 11 NA 26 26 17 17 26 ...\n\n\nCode\nsummary(g)\n\n\n       id             age             pta4      \n Min.   :  1.0   Min.   :31.00   Min.   : 6.00  \n 1st Qu.:190.8   1st Qu.:42.00   1st Qu.:16.00  \n Median :380.5   Median :45.00   Median :19.00  \n Mean   :380.5   Mean   :45.07   Mean   :19.17  \n 3rd Qu.:570.2   3rd Qu.:48.00   3rd Qu.:22.75  \n Max.   :760.0   Max.   :61.00   Max.   :37.00  \n                 NA's   :2       NA's   :2      \n\n\nCode\n# Boxplots again, now without outliers\npar(mfrow = c(1, 2)) # Two plots next to each other\nboxplot(g$age, ylab = \"Age [years]\") \nboxplot(g$pta4, ylab = \"Hearing status, PTA4 [dB]\") \n\n\n\n\n\n\n\n\n\n\nAnd just for fun, some analysis.\n\n\nCode\n# Visualize result: sScatter plot\nplot(g$age, g$pta4, xlab = \"Age [years]\", ylab = \"Hearing status, PTA4 [dB]\") \n\n# Linear regression, stored in object mfit, I am using lm() here, later on we\n# will use stan_glm() in the rstanarm package\nmfit &lt;- lm(pta4 ~ age, data = g)  \nabline(mfit)   # Draw regression line\n\n\n\n\n\n\n\n\n\nCode\nmfit  # Regression equation\n\n\n\nCall:\nlm(formula = pta4 ~ age, data = g)\n\nCoefficients:\n(Intercept)          age  \n    10.4119       0.1942  \n\n\nCode\nconfint(mfit) # Confidence intervals around estimates\n\n\n                2.5 %     97.5 %\n(Intercept) 7.1094563 13.7142545\nage         0.1213409  0.2670488\n\n\n\n\nImport raw data from text file\nHere is data from Table 2.1 in Howell (2012).\nThe data is from a reaction-time experiment with one participant (Howell). The participant was first presented with a list of digits for a few seconds, e.g.,\n1, 5, 7\nAfter the list had been removed, Howell was shown a single digit, e.g.,\n2\nand was asked to determine whether the single digit was on the previously shown list (in the example above, correct answer would be No).\nCode book:\n\ntrial – Trial number, from 1 to 300 (here ordered according to stimulus condition, but I presume that stimuli were presented in random orders)\nnstim – Number of digits on the list (1, 3, or 5), we expect longer reaction times for longer lists\nyesno – Whether the single digit asked for was on the list (1, correct answer “Yes”) or not (2, correct answer “No”).\nrt – Reaction time in milliseconds\n\nNote that the data only contain trials in which the answer given was correct,\nFollow this link to find data Save (Ctrl+S on a pc) to download as text file. You may also download it from Athena.\n\n\nImport text-file\nDownload the file and make sure it is in the same folder as your current R-session. To check from where R is working, run getwd() in the Console. Use setwd(dir) to change the directory of the current session, where dir is a character string. Example setwd(\"C:/Users/MATNI/Documents/Stat1\") would set R to work in folder Stat1 on my computer. You may also store your raw data file in a subfolder (generally a good idea), and then import the file from this subfolder. This is what I did in the code below.\n\n\nCode\n# I have chosen to store the data in a subfolder called \n# \"datasets\" located in the directory from which R is working\n\n# Import data file rt_howell.txt, located in subfolder .datasets/  \nd &lt;- read.table(\"./datasets/rt_howell.txt\", header = TRUE, sep = \",\")\n\n\n\n\n\nInitial screening\n\n\nCode\nhead(d)\n\n\n  trial nstim yesno  rt\n1     1     1     1 400\n2     2     1     1 410\n3     3     1     1 470\n4     4     1     1 380\n5     5     1     1 400\n6     6     1     1 370\n\n\nCode\nstr(d)\n\n\n'data.frame':   300 obs. of  4 variables:\n $ trial: int  1 2 3 4 5 6 7 8 9 10 ...\n $ nstim: int  1 1 1 1 1 1 1 1 1 1 ...\n $ yesno: int  1 1 1 1 1 1 1 1 1 1 ...\n $ rt   : int  400 410 470 380 400 370 380 470 450 610 ...\n\n\nCode\nsummary(d)\n\n\n     trial            nstim       yesno           rt        \n Min.   :  1.00   Min.   :1   Min.   :1.0   Min.   : 360.0  \n 1st Qu.: 75.75   1st Qu.:1   1st Qu.:1.0   1st Qu.: 510.0  \n Median :150.50   Median :3   Median :1.5   Median : 595.0  \n Mean   :150.50   Mean   :3   Mean   :1.5   Mean   : 602.6  \n 3rd Qu.:225.25   3rd Qu.:5   3rd Qu.:2.0   3rd Qu.: 670.0  \n Max.   :300.00   Max.   :5   Max.   :2.0   Max.   :1250.0  \n\n\n\nCheck number of trials per condition, this should be a balanced design. Here a cross-tabualtion of number of stimuli (nstim) and whether target was present or not (yesno).\n\n\nCode\ntable(d$nstim, d$yesno)\n\n\n   \n     1  2\n  1 50 50\n  3 50 50\n  5 50 50\n\n\n\n\nLook at data per stimulus condition\nBox plots\nUse box plots to inspect data\n\n\nCode\n# Boxplot (there are many ways to make nicer boxplots)\n\npar(mfrow = c(1, 2))  # Two plots next to each other\n\n# Reaction time, all data\nboxplot(d$rt, xlab = \"All trials\", ylab = \"Reaction time (ms)\")\n\n# Reaction time, per condition\nboxplot(d$rt ~ d$yesno + d$nstim, col = c('green', 'red'),\n        xlab = \"Stimulus condition\", ylab = \"Reaction time (ms)\")\n\n\n\n\n\n\n\n\n\nNote that there are less boxplot-defined outliers in the figure with all data (left) than in the one with boxplots per stimulus condition (right). In general, if you plan to do grouped analyses, then you should inspect the data per group (condition).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>S: Data Management and Screening</span>"
    ]
  },
  {
    "objectID": "02-stat25.html#probability-1-basics",
    "href": "02-stat25.html#probability-1-basics",
    "title": "2  S: Data Management and Screening",
    "section": "2.3 Probability 1: Basics",
    "text": "2.3 Probability 1: Basics\nLet’s start with this old exam question:\n\nA women is pregnant with twins, and her doctor tells her that about one third of twin births are identical and the remaining two thirds are fraternal. On the next visit to the doctor, the sonogram shows that she is pregnant with twin boys. Her doctor tells her that about half identical twins are twin boys and the other half are twin girls, whereas for fraternal twins about one quarter are twin boys, one quarter are twin girls, and the rest are one of each sex. How should she update her belief in identical versus fraternal twins given this information?\n Old Stat1 exam question \n\n\nWhenever possible, drawing a probability tree can effectively summarize the information provided in the question\n\n\nCode\n# Function to draw probability tree (my apologize for clumsy code)\nptree &lt;- function(edgevalues) {\n  plot(NA, xlim = c(20, 95), ylim = c(0, 100), axes = FALSE, \n     xlab = \"\", ylab = \"\")\n\n  xpoints &lt;- c(25, 50, 50, 75, 75, 75, 75, 75)\n  ypoints &lt;- c(50, 80, 20, 90, 70, 30, 20, 10) \n\n  cex = 0.8\n\n  lines(c(25, 50), c(50, 80))\n  lines(c(50, 75), c(80, 90))\n  lines(c(50, 75), c(80, 70))\n\n  lines(c(25, 50), c(50, 20))\n  lines(c(50, 75), c(20, 30))\n  lines(c(50, 75), c(20, 20))\n  lines(c(50, 75), c(20, 10))\n\n  text(x = 25, y = 50, \"Twins\", pos = 2, cex = cex)\n  text(x = 50, y = 80, \"Identical\", pos = 2, cex = cex)\n  text(x = 50, y = 20, \"Fraternal\", pos = 2, cex = cex)\n\n  text(x = 75, y = 90, \"BB\", pos = 4, cex = cex)\n  text(x = 75, y = 70, \"GG\", pos = 4, cex = cex)\n  text(x = 75, y = 30, \"BB\", pos = 4, cex = cex)\n  text(x = 75, y = 20, \"Mix\", pos = 4, cex = cex)\n  text(x = 75, y = 10, \"GG\", pos = 4, cex = cex)\n\n  text(x = 32.5, y = 65, edgevalues[1], cex = cex)\n  text(x = 32.5, y = 35, edgevalues[2], cex = cex)\n\n  text(x = 67.5, y = 90, edgevalues[3], cex = cex)\n  text(x = 67.5, y = 70, edgevalues[4], cex = cex)\n\n  text(x = 67.5, y = 30, edgevalues[5], cex = cex)\n  text(x = 67.5, y = 23, edgevalues[6], cex = cex)\n  text(x = 67.5, y = 10, edgevalues[7], cex = cex)\n\n  points(xpoints, ypoints, pch = 21, bg = \"grey\", cex = 0.5)\n}\n\nedgeprob &lt;- c(\"1/3\", \"2/3\", \"1/2\", \"1/2\", \"1/4\", \"1/2\", \"1/4\")\nptree(edgevalues = edgeprob)\n\n\n\n\n\n\n\n\n\n\nBB – Twin boys\nMix – Twins of different sex\nGG – Twin girls\n\n\nIt is often easier to think clearly by replacing probabilities with frequencies. So, let’s consider a large number, such as 600 twin births:\n\n\nCode\nedgefreq &lt;- c(200, 400, 100, 100, 100, 200, 100)\nptree(edgevalues = edgefreq)\n\n\n\n\n\n\n\n\n\nor in table format:\n\n\n\n\nBB\nMix\nGG\nMarginal\n\n\n\n\nIdentical\n100\n0\n100\n200\n\n\nFraternal\n100\n200\n100\n400\n\n\nMarginal\n200\n200\n200\n600\n\n\n\n\nNow it is clear that the initial belief of fraternal twins being twice as likely as identical twins (2/3 versus 1/3 probability, that is, an odds of 2:1 in favor of fraternal twins) should be revised in view of the data. Among twin boys (BB), half are identical and half fraternal, so both outcomes are equally likely (an odds of 1:1). This is simple logic, made transparent by transforming the problem to frequencies and summarizing the result in a probability tree or a table as above. It also follows from Bayes’ rule (discussed in more detail in chapter 6 of these notes).\n\n\nProbability concepts\nProbability\nThere are different perspectives on the meaning of “probability”. Frequentists view probability as a long-run frequency, whereas Bayesians view it as a measure of uncertainty. Luckily, the math is the same no matter how probability is defined, so pragmatically we may leave it to the philosophers of probability to sort out what they really mean. We may avoid the whole problem by just stating that probability is a function that takes an outcome as input and spits out a value between 0 (impossible) and 1 (certain) in accordance with the sum rule described below (these conditions, stated much more rigorously, are known as the axioms of probability).\nThe Bayesian view fits nicely with the example above: The woman’s initial (prior) belief is 2/3 to 1/3 in favor of fraternal over identical twins. Objectively, she either has identical or fraternal twins, it is just that she is uncertain of which. Once she has given birth, she will know for certain, with probability 1 for fraternal and 0 for identical, or vice versa.\n\nProbability notation\n\nProbability: \\(Pr(A)\\), also known as marginal probability, see below\nJoint probability: \\(Pr(A, B)\\), \\(Pr(A \\ and \\ B)\\), \\(Pr(A\\cap B)\\)\nUnion probability, \\(Pr(A \\ or \\ B)\\), \\(Pr(A\\cup B)\\)\nConditional probability, \\(Pr(A|B)\\), defined below.\n\nSee Probability 2 (ch. 4), for the distinction between probability and density. We will use \\(p()\\) for density, and \\(Pr()\\) for probabilities.\n\nConditional probability\nDefinition: \\(Pr(B|A) = Pr(A \\ and \\ B)/Pr(A), \\ \\ \\ \\ Pr(A) &gt; 0\\)\nIn words, the probability that B will happen if A has happened. Example, the probability that someone will be diagnosed with lung cancer in the next 10 years (B), given that he or she is a heavy smoker (A).\n\nThe Product rule\n\\(Pr(A \\ and \\ B) = Pr(A)Pr(B|A) = Pr(B)Pr(A|B), \\ \\ \\ \\ Pr(A) &gt; 0, \\ Pr(B) &gt; 0\\)\n\nIndependence\nIf A and B are independent, then:\n\n\\(Pr(B|A) = Pr(B),  \\ \\ \\ Pr(A) &gt; 0\\)\n\\(Pr(A|B) = Pr(A),  \\ \\ \\ Pr(B) &gt; 0\\)\n\\(Pr(A \\ and \\ B) = Pr(A)Pr(B)\\) (from the product rule above)\n\nThe latter equation is often taken as the definition of independence.\n\nThe Sum rule\n\\(Pr(A \\ or \\ B) = Pr(A) + Pr(B) - Pr(A \\ and \\ B)\\)\nSpecial case, A and B are disjoint, i.e., \\(Pr(A \\ and \\ B) = 0\\):\n\\(Pr(A \\ or \\ B) = Pr(A) + Pr(B)\\)\n\nMarginal probability\nThe term marginal probability might sound complex, but it’s actually straightforward. It’s simply the probability of a particular event, without considering the level of other variables. This concept follows from the sum rule: for discrete variables, the marginal probability is the sum of relevant joint probabilities,\nand it’s often displayed in the “margins” of probability tables, as seen below.\nTwin births and sex. Note that entries are joint probabilities, for example the upper left is \\(Pr(BB \\ \\& \\ Identical) = 1/6\\).\n\n\n\n\nBB\nMix\nGG\nMarginal\n\n\n\n\nIdentical\n1/6\n0\n1/6\n1/3\n\n\nFraternal\n1/6\n2/6\n1/6\n2/3\n\n\nMarginal\n1/3\n1/3\n1/3\n1\n\n\n\nH: Type of twin birth (Identical, Fraternal); D: Sex composition (BB, Mix, GG)\n\\(Pr(H = Fraternal) = 1/6 + 2/6 + 1/6 = 2/3\\)\n\\(Pr(D = BB) = 1/6 + 1/6 = 1/3\\)\n\n\n\nBase rate neglect and the Prosecutor’s fallacy\n\n\nBase-rate neglect: Ignoring or undervaluing the prior probability of an event (the base rate) when evaluating the probability of an event in light of data.\nProsecutor’s fallacy: A logical error that occurs when the probability of hypothesis given the data, \\(Pr(H|D)\\), is confused with the probability of data given hypothesis , \\(Pr(D|H)\\).\n\nThis old exam question will illustrate both phenomena:\n\nFor symptom-free women age 40 to 50 who participate in screening using mammography, the following information is available:\n\n1 % has breast cancer,\nIf a women has breast cancer, the probability is 99 % that she will have a positive mammogram,\nIf a women does not have breast cancer, the probability is 97 % that she will have a negative mammogram.\n\nImagine a woman (40-50 years with no symptoms) who has a positive mammogram. What is the probability that she actually has breast cancer?\n Old Stat1 exam question \n\n\nThe test (mammography) is very accurate:\n\nSensitivity is 99 % (“sensitivity” also known as true-positive rate or hit rate) ,\nSpecificity is 97 % (“specificity” also known as true-negative rate or correct-rejection rate)\n\nGiven the high accuracy of test, it seem natural to interpret a positive test as disease with high probability. And indeed, most people will answer the question above with a high probability, well above 50 %. But high test accuracy (high sensitivity and specificity) is not enough, we should also consider the prior probability (or base rate or prevalence) of diseases, in this example it is low (1 %). Base rate neglect is the tendency to ignore the prior probability when updating beliefs in light of data (in this example, the data is a positive test).\n\n\nCode\n# Function to draw probability tree (my apologize for clumsy code)\nptree2 &lt;- function(edgevalues) {\n  plot(NA, xlim = c(10, 95), ylim = c(0, 100), axes = FALSE, \n     xlab = \"\", ylab = \"\")\n\n  xpoints &lt;- c(25, 50, 50, 75, 75, 75, 75)\n  ypoints &lt;- c(50, 80, 20, 90, 70, 30, 10) \n\n  cex = 0.8\n  lines(c(25, 50), c(50, 80))\n  lines(c(50, 75), c(80, 90))\n  lines(c(50, 75), c(80, 70))\n\n  lines(c(25, 50), c(50, 20))\n  lines(c(50, 75), c(20, 30))\n  lines(c(50, 75), c(20, 10))\n  \n  text(x = 25, y = 50, \"Symtom free\\ntest taker\", pos = 2, cex = cex)\n  text(x = 50, y = 80, \"Disease\", pos = 2, cex = cex)\n  text(x = 50, y = 20, \"Not disease\", pos = 2, cex = cex)\n\n  text(x = 75, y = 90, \"Positive test\", pos = 4, cex = cex)\n  text(x = 75, y = 70, \"Negative test\", pos = 4, cex = cex)\n  text(x = 75, y = 30, \"Positive test\", pos = 4, cex = cex)\n  text(x = 75, y = 10, \"Negative test\", pos = 4, cex = cex)\n\n  text(x = 32.5, y = 65, edgevalues[1], cex = cex)\n  text(x = 32.5, y = 35, edgevalues[2], cex = cex)\n\n  text(x = 67.5, y = 90, edgevalues[3], cex = cex)\n  text(x = 67.5, y = 70, edgevalues[4], cex = cex)\n\n  text(x = 67.5, y = 30, edgevalues[5], cex = cex)\n  text(x = 67.5, y = 10, edgevalues[6], cex = cex)\n\n  points(xpoints, ypoints, pch = 21, bg = \"grey\", cex = 0.5)\n}\n\nedgeprob &lt;- c(\"1 %\", \"99 %\", \"99 %\", \"1 %\", \"3 %\", \"97 %\")\nptree2(edgevalues = edgeprob)\n\n\n\n\n\n\n\n\n\n\nAgain, it is much easier to think clearly if we replace probabilities with frequencies, so let’s think of a large number, say 10,000 test takers:\n\n\nCode\nedgefreq &lt;- c(100, 9900, 99, 1, 9900*.03, 9900*.97)\nptree2(edgevalues = edgefreq)\n\n\n\n\n\n\n\n\n\nor in table format:\n\n\n\n\nPositive test\nNegative test\nMarginal\n\n\n\n\nCancer\n99\n1\n100\n\n\nNot cancer\n297\n9603\n9900\n\n\nMarginal\n396\n9604\n10000\n\n\n\n\nFrom the frequencies above, we see that among the 396 with a positive test, only 99 have cancer, corresponding to a 25 % probability of disease given the positive test.\nThis is an informal application of Bayes’ rule, to be discussed in greater detail in chapter 6 of these notes.\n\nDichotomous significance testing encourages base-rate neglect and the prosecutor’s fallacy! This is a psychological argument against using it (for more reasons, see Amrhein et al. (2019) and Gelman et al. (2021)). Let’s walk through a null-hypothesis significance test (NHST) with \\(\\alpha = 0.05\\):\n\n\\(H_1\\): The test taker has cancer.\n\\(H_0\\): The test taker does not have cancer.\n\\(D\\): The test taker tests positive.\n\\(p = Pr(D | H_0) = 0.03\\), which is \\(&lt;\\alpha\\), so we reject \\(H_0\\). (Note that p is defined as the probability of the obtained data or more extreme data; in this example, no data is more extreme than \\(D\\), a positive test.)\n\nIt is very tempting to go from a small p-value to the conclusion that “\\(H_0\\) is probably false”, but this is a logical fallacy induced by the human tendency to neglect the prior probability, base rate neglect, and confuse what we want to assess, \\(Pr(D|H)\\), with what we have \\(Pr(H|D)\\), prosecutor’s fallacy. In this example, even with a significant p-value, \\(H_0\\) remains three times more probable than \\(H_1\\): \\(Pr(H_0|D) = 0.75\\) vs.\\(Pr(H_1|D) = 0.25\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>S: Data Management and Screening</span>"
    ]
  },
  {
    "objectID": "02-stat25.html#practice",
    "href": "02-stat25.html#practice",
    "title": "2  S: Data Management and Screening",
    "section": "Practice",
    "text": "Practice\nThe practice problems are categorized as Easy (E), Medium (M), and Hard (H). Credit goes to Richard McElreath and his excellent book, “Statistical Rethinking” (2020), from which I adapted this layout of practice problems.\n\nEasy\n\n2E1. Describe the two main principles of Open Science.\n\n\n\n2E2. What is a code book?\n\n\n\n2E3. ometimes the code book is included in the raw data file, often in the first few lines. Why might this be a good practice?\n\n\n\n2E4. When we create new variables from those in the raw data file, should these new variables be added to the raw data file? Why or why not?\n\n\n\n2E5. Given the information on the twin birth problem stated above: Think of a random twin birth, and estimate these probabilities:\n\n\\(Pr(BB)\\)\n\\(Pr(BB \\ or \\ GG)\\)\n\\(Pr(BB \\ and \\ GG)\\)\n\\(Pr(Fraternal \\ and \\ BB)\\)\n\\(Pr(Identical \\ and \\ BB)\\)\n\\(Pr(Fraternal \\ or \\ BB)\\)\n\\(Pr(Identical \\ or \\ BB)\\)\n\\(Pr(BB | Fraternal)\\)\n\\(Pr(Fraternal | BB)\\)\n\\(Pr(Identical | BB)\\)\n\nFootnote: We are assuming twin births, so we don’t need to specify that we are conditioning on twin births. Thus, \\(Pr(BB)\\) is a short cut for \\(Pr(BB|twins)\\) which is short cut for \\(Pr(BB|twins, human \\ birth)\\), etc. Probabilities are always conditional on the implicit assumptions we make when estimating them.\n\n\n\n\nMedium\n\n2M1. What is the definition of an outlier according to a boxplot?\n\n\n\n2M2. (a) Describe a scenario where it would be appropriate to exclude an observation classified as an outlier based on the boxplot rule.\n(b) Describe a scenario where you would choose not to exclude such an observation.\n\n\n\n2M3. When analyzing data from distinct groups (e.g., experimental conditions), why may it be beneficial to perform data screening for each group separately rather than on the combined data?\n\n\n\n2M4. Here is a medical-test problem similar in structure to the example discussed above, but involving a much rarer disease and a more accurate screening test.\nFor a symptom-free individual who participate in screening for a rare disease, the following information is available:\n\nPrevalence: One in a million\nSensitivity of test: 100 % (everyone with the disease test positive)\nSpecificity of test: 99.9 % (of individuals with no disease, 999 of 1000 will test negative).\n\nMr X is a symptom-free individual and has tested positive.\nCalculate:\n\n\\(P(H_0|D)\\),\n\\(P(H_1|D)\\),\n\nwhere \\(H_0\\): “Mr X does not have the disease” and \\(H_1\\): “Mr X has the disease”, and \\(D\\) is the data = a positive test.\n Of course, in this problem, you don’t need to calculate both probabilities, since \\(P(H_1|D) = 1 - P(H_0|D)\\) and \\(P(H_0|D) = 1-P(H_1|D)\\). However, it’s always a good practice to compute both as a sanity check.\n\n\n\n\nHard\n\n2H1. Revisit the reaction time data from Howell and create boxplots using both ungrouped and grouped data. This time, use the logarithm of the reaction time instead of the untransformed reaction time. Comment on boxplot-defined outliers in your plots compared to those with untransformed reaction times.\n\n\n\n2H2. Extract the five summary statistics from the ungrouped boxplot in 2H1, representing log-transformed reaction times. Take the anti-log of each statistic, then compare these values with the summary statistics of the boxplot for untransformed reaction times. All but one statistic match-identify which one differs and explain why.\nHint: When drawing a boxplot, save it as an R object, e.g., mybox &lt;- boxplot(x), and then check it’s content str(mybox).\n\n\n\n2H3. Your friend rolls a fair six-sided die and secretly records the outcome; this number becomes the target T. You then put on a blindfold and roll the same six-sided die five times. You’re unable to see how it lands so, each time, your friend (under the watchful eye of a judge, to prevent any cheating) tells you only whether the number you just rolled was greater than, equal to, or less than T. Here is the results, with G, E, L representing a greater, equal or lesser roll:\nG, G, L, E, G\nEstimate the probability of each of the possible values of T (1, 2, …, 6) given this data, and explain the logic behind your estimation strategy.\nFootnote: The problem is taken from the book “Bernoulli’s fallacy” (Clayton (2021)) as a simplified version of the problem that Thomas Bayes used for illustrating the application of his rule.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>S: Data Management and Screening</span>"
    ]
  },
  {
    "objectID": "02-stat25.html#session-info",
    "href": "02-stat25.html#session-info",
    "title": "2  S: Data Management and Screening",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       rlang_1.1.6       jsonlite_2.0.0   \n[17] V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Retire statisticial significance. Nature, 567, 305–307.\n\n\nBroman, K., Cetinkaya-Rundel, M., Nussbaum, A., Paciorek, C., Peng, R., Turek, D., & Wickham, H. (2017). Recommendations to funding agencies for supporting reproducible research. American Statistical Association, 2, 1–4.\n\n\nClayton, A. (2021). Bernoulli’s fallacy: Statistical illogic and the crisis of modern science. Columbia University Press.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHowell, D. C. (2012). Statistical methods for psychology. Cengage Learning.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nWilkinson, L. (1999). APA task force on statistical inference. Statistical methods in psychology journals: Guidelines and explanations. American Psychologist, 54(8), 594–604.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>S: Data Management and Screening</span>"
    ]
  },
  {
    "objectID": "03-method25.html",
    "href": "03-method25.html",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "",
    "text": "Topics\nHere is a tentative definition of causality to get us started:\nX causes Y if a change in X would lead to a change in Y for at least one unit of observation.\nWe may ask different types of causal questions, for example:\nCausal inference is the pursuit of answering such questions. We will primarily focus on the 3. Estimation of causal effect(s). In this context, causal inference involves estimating the extent to which an outcome variable (Y) would change in response to a given change in the exposure variable (X). Note that question 3 encompasses question 2 as a special case. If the estimated effect differs from zero, the answer to question 2 is “yes”; otherwise, it is “no.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "03-method25.html#topics",
    "href": "03-method25.html#topics",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "",
    "text": "Causality\n\nCausal terms\nCausal questions\nCausal inference\nCausal claims\n\nIf you mean it, say it!\n\n\nCausal effect\n\nPotential outcomes\nSingle-unit causal effect\nAverage causal effect\n\nDirected Acyclical Graph (DAG)\n\nChain: \\(X \\rightarrow Z \\rightarrow Y\\) (Z is a mediator)\nFork: \\(X \\leftarrow Z \\rightarrow Y\\) (Z is a confounder)\nInverted fork: \\(X \\rightarrow Z \\leftarrow Y\\) (Z is a collider)\nTotal, indirect, and direct causal effect\n\nCause-probing research designs\n\nExperimental designs: Manipulation, Assignment mechanism known\nQuasi-experimental designs: Manipulation, Assignment mechanism unclear\nNon-experimental designs: No Manipulation, Assignment mechanism unclear\n\n\nTheoretical articles to read:\n\nHernán (2018) on how we speak about causality\nSteiner et al. (2023) on frameworks for causal inference, the sections on DAGs and potential outcomes\nHave a look at Dagitty homepage\n\nRohrer (2018) on causal inference in observational studies, main points made using DAGs\n\n\n\n\n\n\n\nWhat causes Y?\nDoes X cause Y?\nHow large is the causal effect of X on Y? (for a single unit, or on average)\nHow does X cause Y? (mediation)\nDoes the causal effect depend on other variables? (moderation = interaction)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "03-method25.html#speaking-about-causality",
    "href": "03-method25.html#speaking-about-causality",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "3.1 Speaking about causality",
    "text": "3.1 Speaking about causality\n\n“Association does not imply causation.”\n“Where there is association, there is causation.”\n“No correlational smoke, without causal fire.”\n\nTo justify the claim that an observed association between X and Y is a good estimate of the causal effect of X on Y requires a lot more than just reporting the association. There are typically plenty of alternative non-causal explanations of an association to be dealt with before the causal claim will be considered justified. This is true in particular for observational studies with no experimental manipulation and lack of information on how individuals’ came to be exposed to the causal factors under study. For example, observational studies have found that occupational noise exposure is associated with increased risk for heart disease. But maybe part or all of this is explained by other factors that may differ between people working in noisy environments compared to quiet environments. Even if we have tried to adjust for differences in some of these, such as education and income, there may always be unmeasured factors that we have no control over.\nSometimes researches prefer to play it safe and avoid the term cause or causality in favor of terms like associated, linked or predicts. And sometimes they forget themselves and use causal terms anyhow, like influence, change, increase, amplify, reduce, decrease, affect, effect, moderate, and so on. It is very hard to write an article on a causal research question without using any causal terms.\nHernán (2018) argues well for why we should say it if we mean it. In doing so, he cites Kenneth Rothman, who said it 40 years ago:\n\nSome scientists are reluctant to speak so blatantly about cause and effect, but in statements of hypothesis and in describing study objectives such boldness serves to keep the real goal firmly in focus and is therefore highly preferable to insipid statements about ‘association’ instead of ‘causation’.\n Rothman, cited in Hernán (2018)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "03-method25.html#causal-effects-and-potential-outcomes",
    "href": "03-method25.html#causal-effects-and-potential-outcomes",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "3.2 Causal effects and potential outcomes",
    "text": "3.2 Causal effects and potential outcomes\n“Causality” is a tricky concept; we all know what it is, but no one really can define it. The Potential Outcome Model of causality (PO) simply avoids defining causality, and instead defines “causal effect”:\n\nCausal effect is defined as a contrast between potential outcomes. Typically, \\(y_i^1 - y_i^0\\) or \\(\\frac{y_i^1}{y_i^0}\\), where \\(y_i^1\\) is the outcome of Treatment 1, and \\(y_i^0\\) is the outcome of Treatment 0 for study unit \\(i\\). Example: The causal effect of me drinking a cup of coffee on my perceived tiredness right now is how tired I would be if a drank the coffee right now compared to how tired I would be if I didn’t drink the coffee right now.\nFundamental Problem of Causal Inference (Holland, 1986): Only one of the two (\\(y_i^1\\), or \\(y_i^0\\)) can be observed, the other is counterfactual. This perspective reduces a hard problem (how to define “causality”) to the seemingly simpler problem of how to deal with missing data.\nSolution to the fundamental problem: Find a substitute that is equivalent, ceteris paribus, except for the treatment condition. Either on single-unit level (e.g., retest the same unit, or test an “identical twin”) or group level (two groups that are balanced on all relevant variables, making the group assignment “ignorable”)\nPO highlights the need to define what casual effect that is being estimated. We will mainly discuss two general types:\n\nSingle-unit causal effect, the causal effect in each individual unit\nAverage causal effect. The average single-unit causal effect in a sample or population.\n\n\n\nCeteris Paribus: “All other things being equal”\n\nWarning: Terminology and notation in the potential outcome business vary a lot. I try to stick to the notation used by Gelman et al. (2021) (Ch. 18).\n\n\nSometimes it’s easy\nIt’s important to note that the fundamental problem of causality doesn’t mean that causal inference is always difficult. In fact, sometimes it’s easy! For example, consider a situation where we are uncertain about the causal effect of a specific light switch in a hotel room. We turn the switch on (treatment) and observe that the bathroom light turns on. To confirm, we turn the switch off (control) and observe that the bathroom light turns off. This straightforward process is usually enough to convince us, beyond reasonable doubt, that the light switch has a causal effect on the bathroom light. We might refer to this as the “light-switch design,” but the more established term is the “Single-N design,” which will be discussed in upcoming seminars.\nAnother approach involves using identical copies of an object, exposing one to the treatment and the other to the control, and then comparing the outcomes. For instance, you could cut an iron bar into two pieces, place one piece in ordinary water and the other in salty water, and then compare the amount of rust on each piece. The difference in rust can be used to estimate the causal effect of exposure to salty versus non-salty water. We might call this the “identical twin design.”\nHolland (1986) referred to both of these strategies as “scientific” approaches to causal inference, in contrast to the “statistical” approach, which involves comparing groups of individuals who have received different treatments (as in a randomized experiment). A key drawback of the statistical approach is that it can only provide estimates of average causal effects across a population, rather than estimates of single-unit causal effects—unless one assumes that there is no variation in individual responses to the treatment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "03-method25.html#directed-acyclical-graphs-dags",
    "href": "03-method25.html#directed-acyclical-graphs-dags",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "3.3 Directed Acyclical Graphs (DAGs)",
    "text": "3.3 Directed Acyclical Graphs (DAGs)\nA Directed Acyclical Graphs (DAG) is a tool to help us think clearly about causal and non-causal relationships between variables of relevance to a specific causal research problem.\n\n\nCode\nvvg_dag &lt;- dagitty( \"dag {\n   VVG -&gt; AGG\n   VVG -&gt; PA -&gt; AGG\n   PA -&gt; BMI\n   VVG &lt;- Support -&gt; SProb -&gt; AGG\n   SProb &lt;- Genetic -&gt; AGG\n}\")\n\ncoordinates(vvg_dag) &lt;- list(\n  x = c(VVG = 1, Support = 1, BMI = 2, PA = 3, SProb = 3, Genetic = 4, AGG = 4),\n  y = c(VVG = 5, Support = 1, BMI = 3, PA = 4, SProb = 2, Genetic = 1, AGG = 5))\n\nplot(vvg_dag)\n\n\n\n\n\n\n\n\n\nDAGS are graphs that consist of nodes, representing variables, and arrows representing causal effects.\n\nDirected. Only single headed arrows allowed. Make sense because we typically assume that causality flows in one direction: Smoking \\(\\rightarrow\\) Lung-cancer.\nAcyclical. You can never follow the arrows and get back to where you started. Makes sense as we think of causal factors preceding outcomes in time.\nGraph. It is a graph, both in the everyday meaning of a figure, and in the mathematical meaning that allows interesting information to be derived from it using smart algorithms.\n\n\nThe simple DAG above show my model of how these variables are causally related:\n\nExposure: Amount of Violent video gaming in childhood (VVG)\n\nOutcome: Adult aggression (AGG)\n\nCovariates: Physical activity (PA), Parental support (Support), School problems (SProb), Weight (BMI), Genetic factor (Genetic).\n\nLongitudinal study: Exposure and covariates measured in childhood, outcome in adulthood.\n\nPlease draw the DAG above using the online tool at dagitty.net\n\n\n\nDrawn using tool on dagitty.net\n\n\nJumping ahead: The DAG is a mathematical object (a graph) that we may ask questions about, e.g., using dagitty. Here I use the function dagitty::adjustmentSets() to find small sets of variables that I need to conditioning on to estimate the total causal effect of the exposure, VVG, on the outcome AGG.\n\n\nCode\n# Get minimal sufficient adjustment sets\nadjustmentSets(vvg_dag, exposure = 'VVG', outcome = 'AGG', \n               type = \"minimal\", effect = \"total\")\n\n\n{ Genetic, SProb }\n{ Support }\n\n\n\n\nTotal, direct and indirect causal effect\nIn the DAG above, the total causal effect of VVG on AGG is the combined effect of the direct causal effect \\(VVG \\rightarrow AGG\\) and the indirect causal effect \\(VVG \\rightarrow PA \\rightarrow AGG\\).\nIf we assume linear causal relationships with \\(a\\), \\(b\\), and \\(c\\) as regression coefficients, that is, change in outcome (on average) for one unit change of the causal factor:\n\n\\(a\\), for \\(VVG \\rightarrow PA\\) (part of indirect effect),\n\\(b\\), for \\(PA \\rightarrow AGG\\) (part of indirect effect),\n\\(c\\), for \\(VVG \\rightarrow AGG\\) (direct effect),\n\nthen he causal effects would be:\n\nDirect causal effect: \\(c\\)\nIndirect causal effect: \\(a \\times b\\)\nTotal causal effect = direct effect + indirect effect = \\(c + a \\times b\\)\n\n\nHere is a simple simulation with three variables, \\(X, M, Y\\), and coefficients \\(a, b, c\\):\n\ndirect causal effect (\\(X \\rightarrow Y\\)): \\(c = -0.5\\).\nindirect causal effect (\\(X \\rightarrow M \\rightarrow Y\\)): \\(a \\times b = 0.5 \\times 0.6 = 0.3\\).\ntotal causal effect: \\(c + a \\times b =  -0.5 + 0.3 = -0.2\\). That is, increasing \\(X\\) with one unit will decrease \\(Y\\) with 0.2 units.\n\n\n\nCode\n# Simulate data\nset.seed(123)\nn &lt;- 1e5\nx &lt;- rnorm(n)  # Exposure variable\nm &lt;- rnorm(n) + 0.5 * x  # Mediator variable\ny &lt;- rnorm(n) + -0.5 * x  + 0.6 * m # Outcome variable\n\n# Run a regression analysis, to find total causal effect of X on Y\nfit0 &lt;- lm(y ~ x)\n\n# Adding m to the model, to find direct effect of X on y\nfit1 &lt;- lm(y ~ x + m)\n\n# Show causal effect estimates : coefficients for x\nround(c(total = coef(fit0)[2], direct = coef(fit1)[2]), 2)\n\n\n total.x direct.x \n    -0.2     -0.5 \n\n\n\n\n\nProperties of DAGs\n\nNon-parametric (does not assume any specific type of relationships between variables)\n\nNote: This implies that DAGs do not distinguish between additive effects and interactions (moderation) between variables. Thus, \\(X \\rightarrow Y \\leftarrow Z\\) is consistent with independent and additive effects of \\(X\\) and \\(Z\\) on \\(Y\\) as well as any type of interaction effect between \\(X\\) and \\(Z\\) on \\(Y\\).\nDAGs are related to Path analysis and Structural Equation Modelling (SEM), but unlike DAGs, these statistical methods do assume linear relationships between variables\n\nChains, forks and inverted forks are its main components\n\nChain: \\(X \\rightarrow Z \\rightarrow Y\\) (Z is a mediator)\nFork: \\(X \\leftarrow Z \\rightarrow Y\\) (Z is a confounder)\nInverted fork: \\(X \\rightarrow Z \\leftarrow Y\\) (Z is a collider)\n\nConsist of causal and non-causal paths:\n\nA causal path is a path where all arrows point toward the causal outcome. Example: \\(X \\rightarrow M \\rightarrow W \\rightarrow T \\rightarrow Y\\) is a causal path from \\(X\\) to \\(Y\\).\nA non-causal path is a path were at least one arrow point against the “causal flow”. Example: \\(X \\leftarrow M \\leftarrow W \\rightarrow T \\rightarrow Y\\) is a non-causal path between \\(X\\) and \\(Y\\) (\\(W\\) is a confounder of the relationship \\(X\\) and \\(Y\\), because it causes both, through the two “proxy” confounders \\(M\\) and \\(T\\))\n\nNo arrow between two nodes (variables) is a STRONG causal claim",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "03-method25.html#cause-probing-research-designs",
    "href": "03-method25.html#cause-probing-research-designs",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "3.4 Cause-probing research designs",
    "text": "3.4 Cause-probing research designs\nOne way to categorize cause-probing research designs is by their relative strength in terms of internal validity. The strongest designs involve a distinct manipulation of the causal factor of interest and fully understood assignment of participants to conditions (assignment is ‘ignorable’). Slightly weaker designs still involve manipulation of the causal factor, but the assignment mechanism may be unclear, potentially introducing bias. The weakest designs neither manipulate the causal factor distinctly nor employ a known or controlled assignment mechanism.\nThe list below ranks designs from strongest to weakest, though this ranking is tentative. A well-executed study with a lower-ranked design may yield more reliable results than a poorly executed study with a higher-ranked design.\n\nExperimental designs: Manipulation, Assignment mechanism known (“ignorable”)\n\nWithin-subject design\n\nTargeting single-unit causal effects (Single-N design)\nTargeting average causal effect\n\nBetween-subject design (randomized experiment)\nMixed within-between-subject design\n\nQuasi-experimental designs: Manipulation, Assignment mechanism unclear\n\nNatural experiment\n(Regression discontinuity designs)\n(Instrumental variable design)\n(Difference-in-difference designs)\n\nNon-experimental designs: No Manipulation, Assignment mechanism unclear\n\nLongitudinal designs\n\nCohort study\nCase-control study\n\nCross-sectional design\n\n\n(Designs in parentheses are not covered in this course.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "03-method25.html#practice",
    "href": "03-method25.html#practice",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), as in McElreath (2020).\n\nEasy\n\n3E1. I overheard this conversation:\nHolland: The fundamental problem is that you can never measure both. You’ll always have missing data.\nBellman: Nonsense, you just measure twice, once with and once without treatment.\nHeraclitus: You could not step twice into the same river.\nWhat was this all about?\n\n\n\nHeraclitus\n\n\n Footnote: This is an old exam question. Holland as in Holland (1986)\n\n\n\n3E2. “With increasing traffic volumes, exposure to residential road-traffic noise has increased substantially over the last decades. In the same period, we have seen a remarkable reduction in the the number of heart attacks. Thus, road-traffic noise cannot cause heart attacks, as some noise researchers seem to suggest.”\nWhat is wrong with this argument? Explain in terms of the Potential outcome model of causality.\n\n\n\n3E3. “Every 60 years, two cycles within the Asian zodiac calender - one over twelve months and over over five elements - generate a year of the ‘fire horse’. A folk belief exists that families who give birth to babies designated as fire horses will suffer untold miseries, particularly so if the baby is a girl” (Morgan & Winship (2015), p. 65).\nThe figure below is adopted from Morgan & Winship’s Fig. 2.1.\nGuess what year was a “fire horse”! (answer: 1966)\nMake a reasonable estimate of the causal effect of the folk belief on the birth rate in year 1966. Define causal effect, ce, as\n\\(ce_i = y_i^1 - y_i^0\\),\nwhere \\(y_i^1\\) is the observed birth rate in year \\(i=1966\\) and \\(y_i^0\\) is the counterfactual birth rate in year 1966 had this year not been a fire horse.\n\n\n\nCode\n# Data\nyear &lt;- 1951:1980\nbirth_rate &lt;- c(25.3, 23.4, 21.5, 20.0, 19.4, 18.4, 17.2, 18.0, 17.5, 17.2,\n                16.9, 17.0, 17.3, 17.7, 18.6, 13.7, 19.4, 18.6, 18.5, 18.8, \n                19.2, 19.3, 19.4, 18.6, 17.1, 16.3, 15.5, 14.9, 14.2, 13.6)\n\n# Plot\nplot(year, birth_rate, pch = '', ylim = c(12, 26), xlab = 'Year', \n     ylab = 'Birth rate (per 1,000 inhabitants)', las = 1, axes = FALSE )\naxis(1, at = year, las = 1, tck = 0.01, cex.axis = 0.8) \naxis(2, at = seq(12, 26, by = 2), las = 1, tck = 0.01, cex.axis = 0.8)\nlines(year, birth_rate)\npoints(year, birth_rate, pch = 21, bg = 'grey')\n\n# Prepare adding birth-rate number below data symbols in plot\nytext &lt;- birth_rate - 0.5\n\n# Special treatment of data points 1, 15 and 17 (to avoid numbers on line)\nytext[c(1, 15, 17)] &lt;- c(birth_rate[1], birth_rate[15] + 0.5, birth_rate[17] + 0.5)\nxtext &lt;- year\nxtext[1] &lt;- 1952\n\n# Add birth_rate numbers \ntext(xtext, ytext, birth_rate, cex = 0.5)\n\n# Add gridlines\nabline(v = year, col = \"gray\", lty = 3)\n\n\n\n\n\n\n\n\n\n\n\n3E4. he effect estimated in 3E3. Would you call it an average causal effect or a single-unit causal effect. Motivate.\n\n\n\n3E5. Draw Directed Acyclic Graphs (DAGs) involving three variables: X (exposure), Y (outcome), and Z (a third variable), to represent the following scenarios:\n\nX causes Y, and Z causes both X and Y.\nX causes Z, and Z causes Y.\nX and Y are independent, but both cause Z.\nBoth X and Z cause Y.\nFor each of the above, label the variable Z as a “confounder,” “mediator,” “collider,” or “competing exposure,” based on its role in the causal model.\n\n\n\n\n3E6. In the DAG below, X is the exposure variable and Y is the outcome variable.\n\nCount the number of causal paths between X and Y.\nCount the number of non-causal paths (backdoors linking X and Y) .\nSometimes, variables like Z3 are called proxy confounders. Why?\nOn what path is Z3 a collider.\n\n\n\n\nCode\ne5dag &lt;- dagitty( \"dag {\n   X -&gt; Y\n   X -&gt; Z1 -&gt; Y\n   X &lt;- Z2 -&gt; Z3 -&gt; Y\n   Z2 -&gt; Z3 -&gt; Y\n   Z3 &lt;- Z4 -&gt; Y\n}\")\n\ncoordinates(e5dag) &lt;- list(\n  x = c(X = 1, Z2 = 1, Z1 = 3, Z3 = 3, Z4 = 4, Y = 4),\n  y = c(X = 5, Z2 = 1, Z1 = 4, Z3 = 2, Z4 = 1, Y = 5))\n\nplot(e5dag)\n\n\n\n\n\n\n\n\n\n\n\n\nMedium\n\n3M1. Go back to the DAG in 3E6:\n\nWould the total average causal effect of X on Y be identified if you could adjust for Z2? (adjustment set: {Z2})\nAssume that Z2 is unmeasured and cannot be adjusted for, would it be sufficient to adjust for Z3 (adjustment set: {Z3})\nHow about adjustment set {Z3, Z4}?\n\n\n\n\n3M2.\n\nThink of an example in which you would expect all single-unit casual effects to be the same.\nThink of an example in which you would expect both positive and negative single-unit casual effects.\nThink of an example in which the average causal effect would be representative of most individuals.\nThink of an example in which the average causal effect would not be representative of any individual.\n\n\n\n\n3M3. Are you better off than you were four years ago? This question became famous during the 1980 U.S. presidential campaign. It was used by Ronald Reagan during a debate against the incumbent president, Jimmy Carter. Now, Vice-president Kamala Harris faces the same question from former president Donald Trump and Harris seems to struggle finding a convincing response. How would you have answered? Apply your understanding of the potential outcome model of causality!\n\n\n\n\nHard\n\n3H1.\nThink of a simple scenario with one direct and one indirect causal effect of exposure X on outcome Y (see DAG below).\n\nIs it possible for the total causal effect to be less than the direct effect?\nIs it possible for the total causal effect to be close to zero despite a strong direct effect\nIs it possible for the total causal effect to have a different sign than the indirect causal effect\n\n\n\n\nCode\nh1dag &lt;- dagitty( \"dag {\n   X -&gt; Y\n   X -&gt; M -&gt; Y\n}\")\n\ncoordinates(h1dag) &lt;- list(\n  x = c(X = 1, M = 2,  Y = 3),\n  y = c(X = 1, M = 0,  Y = 1))\n\nplot(h1dag)\n\n\n\n\n\n\n\n\n\n\n\n3H2. Construct Directed Acyclic Graphs (DAGs) of the following scenarios.\n\nZ is a mediator in the pathway from X to Y, but Z is also influenced by variable W that affects both Z and Y.\n\nBoth X and Z causes Y, and both X and Y are affected by a common cause W.\n\nX causes W that causes both Z and Y.\n\nFor each of the above, discuss how conditioning on Z would potentially bias or unbias the estimate of the causal effect of X on Y.\n\n\n\n\n3H3.\n\nSimulate a data set with observations from 100 individuals on three variables X, Y, and Z that is consistent with this DAG: \\(X \\leftarrow Z \\rightarrow Y\\)\nUse statistical analyses to check the degree of association between X and Y,\n… and between X and Y after controlling for Z.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "03-method25.html#session-info",
    "href": "03-method25.html#session-info",
    "title": "3  M: Tools for Thinking About Causality",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       yaml_2.3.10       rlang_1.1.6      \n[17] jsonlite_2.0.0    V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHernán, M. A. (2018). The c-word: Scientific euphemisms do not improve causal inference from observational data. American Journal of Public Health, 108(5), 616–619.\n\n\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945–960.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nMorgan, S. L., & Winship, C. (2015). Counterfactuals and causal inference. Cambridge University Press.\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42.\n\n\nSteiner, P. M., Shadish, W. R., & Sullivan, K. J. (2023). Frameworks for causal inference in psychological science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>M: Tools for Thinking About Causality</span>"
    ]
  },
  {
    "objectID": "04-stat25.html",
    "href": "04-stat25.html",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "04-stat25.html#topics",
    "href": "04-stat25.html#topics",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "",
    "text": "Central tendency (location)\n\nMean\nMedian\n\nDispersion (scale)\n\nStandard deviation\nMedian absolute deviation (MAD)\nInter Quartile Range (IQR)\n\nQuantiles\n\nQuartiles\nPercentiles\n\nExploratory visualizations. Displaying raw data to see things that you may otherwise miss. Three types of plots described below. When used for exploratory analysis, they don’t have to look pretty as they are not intended for publication (cf. Gelman et al., 2021, p. 30).\n\nBoxplot\nHistogram\nKernel density plot\nProfile plot\n\nTransformations\n\nLinear transformations, for example, reversing, centering and z-scores\nNon-linear transformations, for example, the log-transform\n\nProbability (continued form ch 2)\n\nBinomial distribution, \\(Binom(n, p)\\)\nProbability versus density\nNormal distribution, \\(Normal(\\mu, \\sigma)\\)\n\nProbability Density Function (PDF), in R: dnorm()\nCumulative Distribution Function (CDF), in R: pnorm()\nStandard Normal: \\(Normal(\\mu = 0, \\sigma = 1)\\)\n\n\n\nReadings.\nGelman et al. (2021) do not cover basic descriptive statistics. But read chapter 2 on data visualization in general, there are many good points that we may come back to. Sections 3.3-3.4 are helpful if you are rusty on linear relationships and logarithms. We will get back to transformations in the chapters on regression. But if you want to jump ahead, please read sections 12.1-12.5 in chapter 12. For probability distributions, see Chapter 3, Section 3.5.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "04-stat25.html#descriptive-statistics",
    "href": "04-stat25.html#descriptive-statistics",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "4.1 Descriptive statistics",
    "text": "4.1 Descriptive statistics\n\nCentral tendency\n\nMean. Assumes at least interval-scale data. The mean is efficient for well-behaved data, but not robust to outliers. May be harder to interpret than the median, as the mean value can be representative or unrepresentative of a typical observation.\nMedian. Can be used for rank-order-scale data. Easy to interpret, it is the middle value, half of the observations are below and half above the median (assuming no ties). It is less efficient for well-behaved data than the mean, but may be more efficient than the mean for non-normal data, and it is robust to outliers.\n\n\n\nDispersion\n\nStandard deviation, often reported together with the mean. Like the mean, the standard deviation is sensitive to outliers.\nInter-quartile range (IQR), often reported together with the median, give the range of the middle 50 % of the data (see below, quantiles).\nMedian absolute deviation (MAD), often reported together with the median. Note that the MAD typically is multiplied by a constant = 1.4826 to make the expected value of MAD for a normal distribution equal to its standard deviation, \\(\\sigma\\).\n\n\n\nQuantiles\nQuantiles are cut-off points dividing the data in parts, A quantile define how large proportion of the data that is below (or above) a given cut-off. Two types of quantiles are:\n\npercentiles. Divides the data in 100 parts. The 10th percentile give the value below which 10 % of the data is located, the 50th percentile give the value below which half of the data is located (i.e., the median), etc.\nquartiles. Divides the data in four quartiles: 25, 50 and 75 % of the data is located below the first, second and third quartile, respectively. These quartiles are represented by the vertical lines of the box in a boxplot (see below).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "04-stat25.html#visualization",
    "href": "04-stat25.html#visualization",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "4.2 Visualization",
    "text": "4.2 Visualization\n\nHistogram\nHistograms visualize a frequency distribution with rectangles representing the number (or proportion) of observations in intervals of the same size. Useful to get a feeling for the the shape of a distribution of values. Here a simple example using simulated data:\n\n\nCode\nn &lt;- 1000  \n\n# Simulate data by random draws from a skewed distribution (Gamma) \nx &lt;- round(100 * rgamma(100, 3, 3))  \n\npar(mfrow = c(1, 2))  # Two plots next to each other\n\n# Histogram with 50-unit bins (&lt;50, 50-99, 100-149, ...)\nhist(x, breaks = seq(0, 500, by = 50), main = \"Bin size = 50\")\n\n# Histogram with 10-unit bins (&lt;50, 50-99, 100-149, ...)\nhist(x, breaks = seq(0, 500, by = 10), main = \"Bin size = 10\")\n\n\n\n\n\n\n\n\n\n\n\nKernel density plot\nThe shape of a histogram depends on the analysts choice of bin size (see figure above). Kernel density plots are less dependent on such choices. (Example taken from chapter 2 of Howell, 2012)\nHere’s the general principle:\n\n\nCode\n# Some observations\nx &lt;- c(0.0, 1.0, 1.1, 1.5, 1.9, 2.8, 2.9, 3.5)\nx\n\n\n[1] 0.0 1.0 1.1 1.5 1.9 2.8 2.9 3.5\n\n\nCode\n# Set \"error\"\nss &lt;- 0.5 # standard deviation of normal\n\n# Plot normal distributions around observed values (+/- error)\nxvalues &lt;- seq(-3, 6, by = 0.1)\nz &lt;- numeric(length(xvalues))\n\nplot(xvalues, z, ylim = c(0, 2.7), pch = \"\", xlab = \"x\", \n     ylab = \"density\")\n\nz1 &lt;- z # Empty vetcor to start with, and then loop over vector x\nfor (j in 1:length(x)){\n  zout &lt;- dnorm(xvalues, mean = x[j], sd = ss)\n  lines(xvalues, zout, col = \"black\")\n  z1 &lt;- z1 + zout\n}\n\n\n\n\n\n\n\n\n\nCode\n# Sum densities vertically to obtain your kernel density estimator.\n# It is an estimator of the true probability density function (PDF) of \n# the population from which the data was sampled.\nplot(xvalues, z, ylim = c(0, 2.7), pch = \"\", xlab = \"x\", \n     ylab = \"density\")\nlines(xvalues, z1, col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nBoxplot\nUseful for illustrating the distributions of data. The three horizontal lines of the box show the 75th, 50th (median) and 25th percentiles. Thus, the length of the vertical side of the box is equal to the inter-quartile range (IQR). Data points located more than 1.5 IQR above or below the box are considered outliers and are indicated with symbols. The bottom and top horizontal lines of the figure indicate minimum and maximum value, once outliers have been removed.\n\n\nCode\nset.seed(888)\nx &lt;- rt(100, df = 4)  # Random numbers from the T-distribution with df = 4\nboxplot(x)\n\n\n\n\n\n\n\n\n\n\n\n\nProfile plot\nFor within-subject data, displaying individual response profiles for each participant can be informative. Below is an example using simulated data from an experiment with 12 participants, each tested under three conditions: A, B, and C.\nSimulate data.\n\n\nCode\n# This is a bit advanced, using the multivariate normal to simulate \n# varying effect\nset.seed(123)\n\n# Experimental setup\nn &lt;- 12  # Number of participants\ncond &lt;- rep(c(\"A\", \"B\", \"C\"), times = n)  # Experimental conditions\n \n# True (unobserved) values for each participant condition\nlibrary(MASS)  # for mvrnorm\nMu &lt;- c(50, 60, 55)  # Population mean\ns &lt;- 5  # Population sigma of varying effects, assumed the same for all conditions\ncv &lt;- s*s*0.7  # Covariance between mu\nSigma &lt;- matrix(c(s^2, cv, cv,   # Covariance matrix\n                  cv, s^2, cv,\n                  cv, cv,  s^2), nrow = 3, ncol = 3, byrow = TRUE)\ntruemean &lt;- mvrnorm(n, Mu, Sigma)  # Multivariate random number generator\n\n# Observed values\nss &lt;- 3  # SD within subject\nobs &lt;- t(apply(truemean, 1, function(x) rnorm(3, mean = x, sd = ss)))\nobs &lt;- cbind(1:12, obs)\ncolnames(obs) &lt;- c(\"id\", \"A\", \"B\", \"C\")\nobs &lt;- data.frame(obs)\n\n\n\nDraw profile plot\n\n\nCode\n# Trick to set limits of y-scale to even 5s\nymin &lt;- floor(min(obs[, 2:4])/5)*5\nymax &lt;- ceiling(max(obs[, 2:4])/5)*5\n\n# Empty plot\nplot(1:3, 1:3, pch = \"\", xlab = \"Experimental condition\",\n     ylab = \"Outcome\", axes = FALSE, xlim = c(0.5, 3.5), ylim = c(ymin, ymax))\n\n# Draw profiles for each id using a for-loop\nfor (j in obs$id){\n  idd &lt;- obs[obs$id == j, ]\n  lines(1:3, idd[, 2:4], lty = 1, col = \"grey\")\n  points(1:3, idd[, 2:4], pch = 21, bg = \"grey\")\n  \n}\n\n# Add axis\naxis(1, at = c(0.5, 1, 2, 3, 3.5), labels = c(\"\", \"A\", \"B\", \"C\", \"\"), pos = ymin)\naxis(2, at = seq(ymin, ymax, 5), pos = 0.5, las = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nExample data set: rt_howell.txt\nData from Table 2.1 in Howell (2012). (see also analysis that we did in 2: Data Management and Screening).\nThe data is from a reaction-time experiment with one participant (Howell). The participant was first presented with a list of digits for a few seconds, e.g.,\n1, 5, 7\nAfter the list had been removed, Howell was shown a single digit, e.g.,\n2\nand was asked to determine whether the single digit was on the previously shown list (in the example above, correct answer would be No).\nCode book:\n\ntrial – Trial number, from 1 to 300 (here ordered according to stimulus condition, but I presume that stimuli were presented in random orders)\nnstim – Number of digits on the list (1, 3, or 5), we expect longer reaction times for longer lists\nyesno – Whether the single digit asked for was on the list (1, correct answer “Yes”) or not (2, correct answer “No”).\nrt – Reaction time in milliseconds\n\nNote that the data only contain trials in which the answer given was correct,\nYou may download the data file “rt_howell.txt” from Athena or find the data on my Bitbucket repository.\nFollow this link to find data Save (Ctrl+S on a pc) to download as text file\n\n\n\nImport data\nOnce you have downloaded the file and made sure it is in the same folder as your current R-session (you may have done this already last time). To check from where R is working, run getwd() in the Console. Use setwd(dir) to change the directory of the current session, where dir is a character string. Example setwd(\"C:/Users/MATNI/Documents/Stat1/datafiles\") would set R to work in the folder datafiles on my computer.\n\n\nCode\n# Import data.  Note that I have chosen to store the data in a subfolder called \n# \"datasets\"\" located in the directory from which R is working)\nd &lt;- read.table(\"datasets/rt_howell.txt\", header = TRUE, sep = \",\")\n\n\n\n\n\nHistogram and Kernel density plots\nFrom last time: Use box plots to inspect data\n\n\nCode\n# Boxplot (there are many ways to make nicer boxplots)\n\npar(mfrow = c(1, 2))  # Two plots next to each other\n\n# Reaction time, all data\nboxplot(d$rt, xlab = \"All trials\", ylab = \"Reaction time (ms)\")\n\n# Reaction time, per condition\nboxplot(d$rt ~ d$yesno + d$nstim, col = c('green', 'red'),\n        xlab = \"Stimulus condition\", ylab = \"Reaction time (ms)\")\n\n\n\n\n\n\n\n\n\n\nHere a look specifically at the two stimulus conditions with the longest list (condition 1.5, and 2.5 in the boxplot above)\n\nTo simplify the following, this code make a reduced data frame g with only the longest-list conditions.\n\n\nCode\ng &lt;- d[d$nstim == 5, ]  # Reduce data frame to simplify the following work\n\n\n\nHistogram and density plot overlays for the “Yes” trials.\n\n\nCode\n# Histogram for yes trials\nhist(g$rt[g$yesno == 1], breaks = seq(275, 1400, by = 20), \n     freq = TRUE, main = '5-item list, Yes-trials', xlab = \"Reaction time (ms)\")\n\n\n\n\n\n\n\n\n\nCode\n# Same histogram, but note y-axis and argument freq = FALSE\nhist(g$rt[g$yesno == 1], breaks = seq(275, 1400, by = 20), \n     freq = FALSE, main = '', xlab = \"Reaction time (ms)\")\n\n# Add density for yes trials\ndens_yes &lt;- density(g$rt[g$yesno == 1])\npolygon(dens_yes, col = rgb(0, 1, 0, 0.5))\n\n\n\n\n\n\n\n\n\n\nHistogram overlays for “Yes” and “No” trials\n\n\nCode\n# Blank histogram (note argument lty = 'blank', col = \"white\" to make bars\n# invisible)\nhist(g$rt, breaks = seq(275, 1400, by = 20), lty = 'blank', col = \"White\",\n     main = '', xlab = \"Reaction time (ms)\")\n\n# Add histogram for Yes trials\nhist(g$rt[g$yesno == 1], breaks = seq(275, 1400, by = 20),\n     add = TRUE, col = rgb(0, 1, 0, 0.5))\n\n# Add histogram for No trials\nhist(g$rt[g$yesno == 2], breaks = seq(275, 1400, by = 20),\n     add = TRUE, col = rgb(1, 0, 0, 0.5))\n\n\n\n\n\n\n\n\n\n\nDensity overlays for “Yes” and “No” trials.\n\n\nCode\n# Blank histogram (note argument lty = 'blank')\nhist(g$rt, breaks = seq(275, max(d$rt) + 50, by = 20), freq = FALSE, lty = 'blank', \n     col = \"white\", main = '', xlab = \"Reaction time (ms)\")\n# Add density for Yes trials\ndens_yes &lt;- density(g$rt[g$yesno == 1])\npolygon(dens_yes, col = rgb(0, 1, 0, 0.5))\n\n# Add density for No trials\ndens_no &lt;- density(g$rt[g$yesno == 2])\npolygon(dens_no, col = rgb(1, 0, 0, 0.5))\n\n\n\n\n\n\n\n\n\n\nBelow some descriptive statistics for the 5-item list and No-trials (red density curve above)\nBoxplot\n\n\nCode\nx &lt;- g$rt[g$yesno == 2]  # Put data in vector called x, to simplify things\nboxplot(x)\nmtext(\"5-item list, No-trials\", 3, line = 0.5) # Add heading\n\n\n\n\n\n\n\n\n\nQuantiles\n\n\nCode\nsummary(x)  # Give five point summary, including quartiles\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  530.0   630.0   665.0   691.2   735.0  1250.0 \n\n\nCentral tendency (or Location)\n\n\nCode\n# Mean\nm &lt;- mean(x, na.rm = TRUE)\nm_rmout &lt;- mean(x[x &lt; 900], na.rm = TRUE) # Remove values &gt; 900 ms\n\n# Median\nmdn &lt;- median(x, na.rm = TRUE)\nmdn_rmout &lt;- median(x[x &lt; 900], na.rm = TRUE) # Remove values &gt; 900 ms\n\nlocation &lt;- c(m = m, m_rmout = m_rmout, mdn = mdn, mdn_rmout = mdn_rmout)\nround(location, 1)\n\n\n        m   m_rmout       mdn mdn_rmout \n    691.2     674.2     665.0     660.0 \n\n\nVariation (or Dispersion or Scale)\n\n\nCode\n# Standard deviation\nstd &lt;- sd(x, na.rm = TRUE)\nstd_rmout &lt;- sd(x[x &lt; 900], na.rm = TRUE) # Remove values &gt; 900 ms\n\n# Inter-quartile range\niqr &lt;- IQR(x, na.rm = TRUE)\niqr_rmout &lt;- IQR(x[x &lt; 900], na.rm = TRUE) # Remove values &gt; 900 ms\n\n# Median absolute deviation\n# NOTE: default for mad() is with correction factor\nmad &lt;- mad(x, na.rm = TRUE)\nmad_rmout &lt;- mad(x[x &lt; 900], na.rm = TRUE) # Remove values &gt; 900 ms\n\ndispersion &lt;- c(std = std, std_rmout = std_rmout, \n                iqr = iqr, iqr_rmout = iqr_rmout,\n                mad = mad, mad_rmout = mad_rmout)\nround(dispersion, 1)\n\n\n      std std_rmout       iqr iqr_rmout       mad mad_rmout \n    116.1      75.3     105.0      92.5      81.5      74.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "04-stat25.html#transformations",
    "href": "04-stat25.html#transformations",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "4.3 Transformations",
    "text": "4.3 Transformations\n\nLinear transformations\nLinear transformations changes the mean or the standard deviation or both of a set of values but will preserve the shape of the distribution of values. A kernel density plot would therefore have the same shape for a distribution of values as for a linear transformations of the same values.\nLinear transformations are applied for different reasons, some examples below:\n\ncentering. Remove the mean: \\(x_{center} = x - mean(x)\\). Often done before fitting a linear regression model to make the intercept meaningful or when fitting interaction models (more about this in later chapters). The mean of the centered variable will be zero, the standard deviation will remain the same.\nz-transformation. Remove the mean and set standard deviation to unity: \\(z_{x} = \\frac{x - mean(x)}{sd(x)}\\). Can be used for the same reasons as centering, but also to set variables with different units on a common scale (unit: sd).\ntransform range., e.g., set minimum = 0 and maximum = 1: \\(x_{01} = \\frac{x - min(x)}{max(x) - min(x)}\\). This may be a way to help interpretation when x-scale has no meaningful unit, and be used to set variables with different units on a common scale (for the example above, unit: range of data).\nchange unit. For example, from meters to centimeters or from USD to USDx1000 to obtain numbers that are easy to work with, or to transform from an unfamiliar to a familiar unit, for example, from degrees Fahrenheit (F) to Celsius (C): \\(C = \\frac{5}{9}(F - 32)\\)\n\nThe figure below illustrates the preserved shape of data distributions after linear transformations of a set of (simulated) temperature measurements. But first a scatter plot matrix to verify that the variables indeed are linearly related.\n\n\nCode\nset.seed(999)\n\n# A thousand temperature measurements, all around 60 F\nfahr &lt;- rnorm(1000, mean = 60, sd = 2) \n\n# Transform to Celsius\ncelc &lt;- (5/9) * (fahr - 32) \n\n# z-transform\nz &lt;-(fahr - mean(fahr))/sd(fahr)\n\n# Scatter plot matrix, just to show that they linearly related\npairs(~ fahr + celc + z)\n\n\n\n\n\n\n\n\n\nCode\n# Density plots in three panels\npar(mfrow = c(1, 3))  \nplot(density(fahr), main = \"Fahrenheit\", xlab = \"Temperatur [degrees Fahrenheit]\")\nplot(density(celc), main = \"Celcius\", xlab = \"Temperatur [degrees Celcius]\")\nplot(density(z), main = \"z-score\", xlab = \"Temperatur [standard deviation]\")\n\n\n\n\n\n\n\n\n\n\n\nNon-linear transformations\nUnlike linear transformations, non-linear transformations will change the shape of the distribution. We will talk more about reasons for non-linear transformations in regression analysis. Here I will only mention the log-transform, often used to handle skewed distributions. Reaction time is an example of a variable that typically has a positive skew, and therefore it is common to apply a log transform. Below illustrated using the reaction time data in rt_howell.txt.\nLinear and log-transformed reaction times for No-trials (yesno = 2) of the five-item list (nstim = 5).\n\n\nCode\nrt &lt;- d$rt[d$yesno == 2 & d$nstim == 5]\nlog_rt &lt;- log(rt)  # Natural log\n\npar(mfrow = c(1, 2)) # Two plots\n\n# Linear: histogram with density\nhist(rt, breaks = seq(275, 1400, by = 25), \n     freq = FALSE, main = \"linear\")\ndens_yes &lt;- density(rt)\npolygon(dens_yes, col = rgb(1, 0, 0, 0.5))\n\n# Log: histogram with density\nhist(log_rt, breaks = seq(log(275), log(1400), by = 0.05), \n     freq = FALSE, main = \"natural log\")\ndens_yes &lt;- density(log_rt)\npolygon(dens_yes, col = rgb(1, 0, 0, 0.5))\n\n\n\n\n\n\n\n\n\nMaybe not a big difference in shape, but note that the outlier in the left panel (around 1200 ms) has moved closer to the main bulk of the data in the right panel.\nThe base of the log doesn’t matter for the shape of the distribution, it will remain the same. This because log-transformed values of different bases are linearly related, for example, \\(log_{10}(x) = \\frac{log_e(x)}{log_e(10)}\\), and as we saw above, linear transformations doesn’t change the shape of a distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "04-stat25.html#probability-2-probability-distributions",
    "href": "04-stat25.html#probability-2-probability-distributions",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "4.4 Probability 2: Probability distributions",
    "text": "4.4 Probability 2: Probability distributions\nGelman et al. (2021) define “probability distribution” in an informal way that also defines “random variable”:\n\nA probability distribution corresponds to an urn with a potentially infinite number of balls inside. When a ball is drawn at random, the “random variable” is what is written on this ball.\n Gelman et al. (2021), Section 3.5, p. 40\n\n\nFor discrete variables, there is a finite set of balls (outcomes) in proportion to their probability (so twice as many balls for an outcome twice as probable as another outcome). For continuous variables, the set is infinite, because there are infinitely many numbers in any interval of a continuous variable. Still, there will be more balls with numbers in a given interval than in another.\n\nWe will focus on the Normal distribution in this course, but let’s start with the Binomial distribution.\n\nBinomial Distribution\nThe Binomial distribution may be thought of as the “coin-tossing” distribution: It gives you the probability of the number of outcomes classified as “successes” (e.g., “heads”) in \\(n\\) independent trials with two outcomes (e.g., a coin toss), each with success probability \\(p\\) (\\(p = 0.5\\) for a fair coin, \\(p \\neq 0.5\\) for an unfair coin).\nNotation\nThe notation \\(Y \\sim Binomial(n, p)\\) means that the random variable \\(Y\\) is distributed as the Binomial distribution with \\(n\\) independent trials, each with probability \\(p\\) of success. Y can take on the values 0, 1, …, n. \nA special case of this distribution occurs when \\(n = 1\\), known as the Bernoulli distribution. The notation \\(Y \\sim Bernoulli(p)\\) means that \\(Y\\) is distributed as the Binomial distribution with \\(n = 1\\), where \\(Y\\) can take on only the values 0 or 1.\n\\(P(Y = y) = \\binom{n}{y}p^y(1-p)^{n-y}\\) gives the probability of \\(y\\) successes in \\(n\\) trials, where \\(\\binom{n}{y}\\) is the number of ways you can draw \\(y\\) objects from a set of \\(n\\) objects, if order doesn’t matter (try out the choose() function in R).\nExpected value (mean value): \\(E(Y) = np\\)\nStandard deviation: \\(SD(Y) = \\sqrt{np(1-p)}\\)\n\nExample: Calculate the probability of two successes in three trials, if \\(p = 0.6\\)\n\\(Y \\sim Binomial(n = 3, p = 0.6)\\)\n\\(P(Y = 2) = \\binom{3}{2}p^2(1-p)^{1} = 3 * 0.6 * 0.6 * 0.4 = 0.432\\)\n\nCalculate by hand in R\n\n\nCode\n# Using equation\np &lt;- 0.6\nyprob &lt;- choose(3,2) * p^2 * (1-p)^1\nyprob \n\n\n[1] 0.432\n\n\nCode\n# Using R's binomial function\ndbinom(x = 2, size = 3, p = 0.6)\n\n\n[1] 0.432\n\n\n\nProbability mass function (PMF)\nA Probability mass function gives the probability for each outcome of a discrete random variable. It sums to one.\nExample: Plot PMF for n = 19, and p = 0.2, 0.5, 0.8\n\n\nCode\nn &lt;- 19\np &lt;- c(0.2, 0.5, 0.8)\n\n# Panels next to each other\npar(mfrow = c(1, 3))\n\n# Plots\nbinpmf &lt;- function(n = 19, p) {\n  y &lt;- 0:n  # Possible outcomes\n  m1 &lt;- dbinom(y, size = n, prob = p)\n  plot(y, m1, type = 'h', xlab = \"y\",\n     ylab  = \"P(Y = y)\", ylim = c(0, 0.3), lwd = 3)\n  mtext(sprintf(\"n = %d, p = %0.1f\", n, p), 3, cex = 1)\n  lines(c(-1, n+1), c(0, 0), col = \"grey\")\n}\n\nbinpmf(p = p[1])\nbinpmf(p = p[2])\nbinpmf(p = p[3])\n\n\n\n\n\n\n\n\n\nCumulative distribution function (CDF)\nThe cumulative distribution function (CDF) gives the probability that a random variable is less than or equal to a specific value.\nExample: Plot CDF for n = 19, and p = 0.2, 0.5, 0.8\n\n\nCode\nn &lt;- 19\np &lt;- c(0.2, 0.5, 0.8)\n\n# Panels next to each other\npar(mfrow = c(1, 3))\n\n# Plots\nbincdf &lt;- function(n = 19, p) {\n  y &lt;- 0:n  # Possible outcomes\n  m1 &lt;- pbinom(y, size = n, prob = p)\n  plot(y, m1, type = 's', xlab = \"y\",\n     ylab  = \"P(Y &lt;= y)\", ylim = c(0, 1), lwd = 1.5)\n  mtext(sprintf(\"n = %d, p = %0.1f\", n, p), 3, cex = 0.8)\n  lines(c(-1, n+1), c(0, 0), col = \"grey\")\n}\n\nbincdf(p = p[1])\nbincdf(p = p[2])\nbincdf(p = p[3])\n\n\n\n\n\n\n\n\n\n\nThe Binomial distribution becomes approximately normal as \\(n\\) gets very large. Here illustrated for \\(p\\) = 0.2 and \\(n\\) = 6, 60, and 600.\n\n\nCode\nn &lt;- c(6, 60, 600)\np &lt;- 0.2\n\n# Panels next to each other\npar(mfrow = c(1, 3))\n\n# Plots\nbinpmf2 &lt;- function(n, p, nmin, nmax) {\n  y &lt;- nmin:nmax  # Possible outcomes\n  m1 &lt;- dbinom(y, size = n, prob = p)\n  plot(y, m1, type = 'h', xlab = \"y\",\n     ylab  = \"P(Y = y)\", ylim = c(0, max(m1)), lwd = 1)\n  mtext(sprintf(\"n = %d, p = %0.1f\", n, p), 3, cex = 0.8)\n  lines(c(-1, n+1), c(0, 0), col = \"grey\")\n}\n\nbinpmf2(n = n[1], p = p, nmin = 0, nmax = 6)\nbinpmf2(n = n[2], p = p, nmin = 0, nmax = 30)\nbinpmf2(n = n[3], p = p, nmin = 85, nmax = 155)\n\n\n\n\n\n\n\n\n\n\n\n\nNormal Distribution\nThe sum of many small independent random variables will be approximately distributed as the Normal distribution (this follows from the Central Limit Theorem).\nNotation\n\\(Y \\sim N(\\mu, \\sigma)\\), random variable Y is distributed as a normal distribution with location (mean) \\(\\mu\\) and scale (sd) \\(\\sigma\\) (Note, the second parameter is often given as the variance \\(\\sigma^2\\).)\n\nExample: Y is scores on an IQ-test, normally distributed with mean = 100, sd = 15: \\(Y \\sim N(\\mu=100, \\sigma=15)\\)\n\nProbability density function (PDF)\nFor a discrete variable, the probability mass function (PMF) sums to one (see examples above). For a continuous variable, the probability density function (PDF) has an area under the curve equal to one.\n\n\nCode\niq &lt;- seq(from = 50, to = 150, by = 1)\niqdens &lt;- dnorm(iq, mean = 100, sd = 15)\nplot(iq, iqdens, type = 'l', xlab = \"y [IQ-score]\", ylab = \"density\")\n    lines(c(40, 160), c(0, 0), col = \"grey\")\n\n\n\n\n\n\n\n\n\nNOTE: Density is not the same as probability. It can be greater than 1!\n\n\nCode\n# Denisty for uniform(0, 0,5)\nx &lt;- seq(from = -0.1, to = 0.6, by = 0.0001)\nxdens &lt;- dunif(x, min = 0, max = 0.5)\nplot(x, xdens, type = 'l', xlab = \"x\", ylab = \"density\", ylim = c(0, 2.2))\nlines(c(-0.2, 0.7), c(0, 0), col = \"grey\")\n\n\n\n\n\n\n\n\n\nWhat is the probability that a randomly selected individual has an IQ-score of exactly 100, if \\(IQ \\sim N(\\mu=100, \\sigma=15)\\)? The probability correspond to an area under the curve (PDF) of the normal distribution. The answer to the question depends on what we mean with “exactly”. If we really mean it, as in \\(100.000000...\\), then the answer is \\(P(IQ = 100.000...) = 0\\), because the area under the curve has zero width. But I guess that IQ-scores are measured without decimals, if so, \\(IQ = 100\\) can be interpreted as \\(99.5 &lt; IQ &lt; 100.5\\). This is an interval, and an area under the curve can be calculated: \\(P \\approx .03\\).\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Draw PDF, and add line for mu = 100.0000...\niq &lt;- seq(from = 50, to = 150, by = 1)\niqdens &lt;- dnorm(iq, mean = 100, sd = 15)\nplot(iq, iqdens, type = 'l', xlab = \"y [IQ-score]\", ylab = \"density\",\n     xlim = c(70, 130))\nlines(c(40, 160), c(0, 0), col = \"black\")\nlines(c(100, 100), c(0, dnorm(100, mean = 100, sd = 15)), \n      col = \"grey\", lwd = 0.2, lty = 3)\n\n# Draw PDF, and add area for 99.5 &lt; mu &lt; 100.5\niq &lt;- seq(from = 50, to = 150, by = 1)\niqdens &lt;- dnorm(iq, mean = 100, sd = 15)\nplot(iq, iqdens, type = 'l', xlab = \"y [IQ-score]\", ylab = \"density\",\n     xlim = c(70, 130))\nlines(c(40, 160), c(0, 0), col = \"black\")\nmm &lt;- seq(99.5, 100.5, 0.01)\nd &lt;- dnorm(mm, mean = 100, sd = 15)\npolygon(x = c(mm, rev(mm)), y = c(rep(0, length(d)), rev(d)),\n          col = rgb(0, 0, 0, 0.1), border = FALSE)\n\n\n\n\n\n\n\n\n\n\nCalculate probability (area under the curve):\n\n\nCode\n# P(mu == 100)  \npnorm(100, mean = 100, sd = 15) -  pnorm(100, mean = 100, sd = 15)\n\n\n[1] 0\n\n\nCode\n# P(mu &gt; 99.5 | mu &lt; 100.5)\npnorm(100.5, mean = 100, sd = 15) -  pnorm(99.5, mean = 100, sd = 15)\n\n\n[1] 0.02659123\n\n\nCumulative distribution function (CDF)\nThe CDF has the same definition for discrete and continuous variables: It gives the probability that a random variable is less than or equal to a specific value of a random variable.\n\n\nCode\niq &lt;- seq(from = 50, to = 150, by = 1)\niqcdf &lt;- pnorm(iq, mean = 100, sd = 15)\nplot(iq, iqcdf, type = 'l', xlab = \"y [IQ-score]\", ylab = \"P(Y &lt; y)\")\nlines(c(40, 160), c(0, 0), col = \"grey\")\n\n\n\n\n\n\n\n\n\nLocation and scale of normal distribution\nThis plot shows three normal distributions with…\n\nLeft panel: Same location (mean), different scale (standard deviation)\nRight panel: Different location, same scale\n\n\n\nCode\nx &lt;- seq(from = 0, to = 100, by = 1)\n\npar(mfrow = c(1, 2))\n\n# Different scales, same location \nplot(x, dnorm(x, 50, 10), ylim = c(0, 0.2), pch = \"\", xlab = \"x\", ylab = \"density\")\nlines(x, dnorm(x, 50, 2), col = \"red\")\nlines(x, dnorm(x, 50, 10))\nlines(x, dnorm(x, 50, 20), col = \"blue\")\n\n# Different location, same scale \nplot(x, dnorm(x, 50, 10), ylim = c(0, 0.2), pch = \"\", xlab = \"x\", ylab = \"density\")\nlines(x, dnorm(x, 30, 3), col = \"red\")\nlines(x, dnorm(x, 50, 3))\nlines(x, dnorm(x, 55, 3), col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nStandard normal\nRemember that linear transformations doesn’t change the shape of distributions. The standard normal is just a linear transformation (z-scores) of another normal distribution with the purpose of setting \\(\\mu = 0\\) and \\(\\sigma = 1\\).\n\\(Y \\sim N(\\mu, \\sigma)\\) Original (non-standard) normal,\n\\(Z = (Y - \\mu)/\\sigma\\) Linear transformation,\n\\(Z \\sim N(0, 1)\\) Standard normal.\n\n\nCode\npar(mfrow = c(1, 2))\nz &lt;- seq(-4, 4, 0.1)\n\n# PDF\ndd &lt;- dnorm(z)\nplot(z, dd, type = 'l', ylab = \"density\")\nlines(c(-5, 5), c(0, 0), col = \"grey\")\nmtext(\"PDF\", 3, cex = 0.8)\n\n# CDF\ndd &lt;- pnorm(z)\nplot(z, dd, type = 'l', ylab = \"probability, P(Z &lt;= z)\")\nlines(c(-5, 5), c(0, 0), col = \"grey\")\nmtext(\"CDF\", 3, cex = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\nThe 68-95-99.7 rule\nThe 68-95-99.7 rule, also known as the empirical rule, describes the distribution of data in a normal distribution. According to this rule:\n\nApproximately 68 % of the data falls within one standard deviation of the mean. Put equivalently, the probability that a random observation, \\(x\\) falls within one standard deviation of the mean is approximately 0.68, that is, \\(Pr(\\mu - \\sigma &lt; x &lt; \\mu + \\sigma) \\approx 0.68)\\)\nAbout 95 % of the data lies within two standard deviations of the mean.\nAbout 99.7 % of the data is found within three standard deviations of the mean.\n\nFor the standard normal, \\(N(\\mu = 0, \\sigma = 1)\\), these three intervals corresponds to the intervals [-1, 1], [-2, 2], and, [-3, 3], respectively.\n\n\n\n\n\n\n\n\n\nIn R, you can calculate the exact probabilities like this:\n\n\nCode\n# Calculate (pm1sd for +/- 1 sd, etc.)\npm1sd &lt;- pnorm(1) - pnorm(-1)\npm2sd &lt;- pnorm(2) - pnorm(-2)\npm3sd &lt;- pnorm(3) - pnorm(-3)\n\n# Round and print\nround(c(pm1sd = pm1sd, pm2sd = pm2sd, pm3sd = pm3sd), 5)\n\n\n  pm1sd   pm2sd   pm3sd \n0.68269 0.95450 0.99730 \n\n\n\n\n\n\nHow normal is the Normal?\nExample from Gelman et al. (2021), Fig. 3.6:\nAssume that height is normally distributed, with different means for men and women:\n\nMen: \\(h_{male} \\sim N(\\mu = 180, \\sigma = 8)\\), unit cm.\nwomen: \\(h_{female} \\sim N(\\mu = 168, \\sigma = 8)\\), unit cm.\n\n\n\nCode\nset.seed(999)\nmale &lt;- rnorm(5e4, 180, 8)\nfemale &lt;- rnorm(5e4, 168, 8)\ncombined &lt;- c(male, female)\n\n# Blank histogram (note argument lty = 'blank')\nhist(male, breaks = seq(120, 230, by = 20), freq = FALSE, lty = 'blank',\n     col = \"white\", main = '', xlab = \"Height (cm) \", ylim = c(0, 0.06))\n# Add density for Yes trials\ndens_male &lt;- density(male)\npolygon(dens_male, col = rgb(0, 1, 0, 0.5))\n\n# Add density for No trials\ndens_female &lt;- density(female)\npolygon(dens_female, col = rgb(1, 0, 0, 0.5))\n\n\n\n\n\n\n\n\n\nHeight in the combined population of men and women is however not normally distributed.\n\n\nCode\n# Blank histogram (note argument lty = 'blank')\nhist(combined, breaks = seq(120, 230, by = 20), freq = FALSE, lty = 'blank', \n     col = \"white\", main = '', xlab = \"Height (cm) \", ylim = c(0, 0.06))\n# Add density for Yes trials\ndens_combined &lt;- density(combined)\npolygon(dens_combined, col = rgb(0, 0, 1, 0.5))\n\n\n\n\n\n\n\n\n\n The density plot for the combined population looks normal, but actually it is not. If normally distributed, about 84 % of values are less than 1 standard deviation above the mean. This is true for our simulated male and female populations, but not for the combined population.\n\n\nCode\n# Exact probability of x &lt; 1, if X ~ N(mean = 0, sd = 1)\npnorm(1)  \n\n\n[1] 0.8413447\n\n\nCode\n# Estimate from our simulated male population\nmean(male &lt; mean(male) + sd(male))\n\n\n[1] 0.8416\n\n\nCode\n# Estimate from our female popualtion\nmean(female &lt; mean(female) + sd(female)) \n\n\n[1] 0.84062\n\n\nCode\n# Estimate from the combined popualtion\nmean(combined &lt; mean(combined) + sd(combined))\n\n\n[1] 0.83395\n\n\nA common method for evaluating normality is to visualize using so called QQ-plots (points fall on the red line if the data is perfectly normal). QQ-plots are not part of the course, but I use them here to drive home the point that the (simulated) combined population (vector combined) is not normally distributed.\n\n\nCode\n# QQ-plot\nzmale &lt;- (male - mean(male))/sd(male)\nzfemale &lt;- (female - mean(female))/sd(female)\nzcombined &lt;- (combined - mean(combined))/sd(combined)\nylim &lt;- c(-4, 4)\npar(mfrow = c(1, 3))\nqqnorm(zmale, main = \"Population male weight, z-score\", cex = 0.7, ylim = ylim)\nqqline(zmale, col = \"red\")\nqqnorm(zfemale, main = \"Population female weight, z-score\", cex = 0.7, ylim = ylim)\nqqline(zfemale, col = \"red\")\nqqnorm(zcombined, main = \"Population combined, z-score \", cex = 0.7, ylim = ylim)\nqqline(zcombined, col = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "04-stat25.html#practice",
    "href": "04-stat25.html#practice",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "Practice",
    "text": "Practice\nPractice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n4E0. Identify two advantages and two disadvantages of using the arithmetic mean compared to the median as sample statistics for measuring central tendency.\n\n\n\n4E1. The median absolute deviation (MAD) is often multiplied by a constant, why and what is the constant?\nNote: In the regression outputs from rstanarm::stan_glm(), they call MAD multiplied by this constant for MAD_SD.\n\n\n\n4E2. Below a frequency table of number of children (0, 1, 2, 3, or 4) in a sample 200 families.\nCalculate by hand the following statistics for number of children in these families:\n\nMedian\nMaximum\nInter-quartile range\nMedian absolute deviation (Hint. It is the same with or without normalizing constant)\n\n\n\n\nCode\nset.seed(123)\nn_child &lt;- sample(0:4, size = 200, replace = TRUE, prob = c(5, 10, 1, 1, 0))\nn_child_f &lt;- factor(n_child, levels = 0:4)\ntable(Children = n_child_f)\n\n\nChildren\n  0   1   2   3   4 \n 54 119  16  11   0 \n\n\n\n\n4E3. Below is a boxplot.\nApproximate the following by eye:\n\nInter-quartile range (IQR).\nMedian (IQR).\nRange\nWhether distribution is symmetric, positively or negatively skewed\n\n\n\n\nCode\nset.seed(123)\ny &lt;- rlnorm(60, log(10), log(2.5))\nboxplot(y, xlab = \"X\", horizontal = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# round(c(iqr = IQR(y)), 0)\n\n\n\n\n4E4. This dataset is from an experiment with 80 participants, evenly split into two groups of 40 participants: A control group (group = 0) and an experimental group (group = 1). The results are visualized using boxplots.\nApproximate by eye (and brain!):\n\nDifference in median scores between groups\nApproximately how many in the experimental group that performed better than the median of the control group\nVariability of scores: Was it similar in the two groups?\n\n\n\n\nCode\nset.seed(123)\nn &lt;- 40\ngroup &lt;- c(rep(0, n), rep(1, n))\ny &lt;- rnorm(length(group), (10 + 2*group), (3 - 1*group))\n\n# plot\nboxplot(y ~ group, xlab = \"Group\", ylab = \"Outcome\", horizontal = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n4E5. Below is a kernel density plot of a dataset with an arithmetic mean of 26. Which of the following values is closest to the median: 7, 10, 21, 27, 41?\n\n\n\nCode\nset.seed(123)\ny &lt;- 10 * rlnorm(100, mean = log(2), sd = log(2))\nplot(density(y), xlab = \"X\", main = \"\")\n\n\n\n\n\n\n\n\n\nCode\n# median(y)\n\n\n\n\n4E6. One advantage of a kernel density plot over a histogram is that its shape is not influenced by the choice of bin size. Explain.\n\n\n\n4E7. Simulate a sample of reaction times [ms] using this code:\nset.seed(123)\nrt &lt;- rlnorm(1e3, meanlog = 6, sdlog = 0.69)\nCalculate:\n\nMean\nMedian\nStandard deviation\nInter-quartile range\nMedian absolute deviation\n\n\n\n\n4E8. Visualize the distribution of data from 4E7 (vector rt) using a histogram overlaid with a kernel density plot (see examples above). Locate the bins of the histogram at multiples of \\(50 \\pm 25 \\ \\ ms\\).\n\n\n\n4E9. Repeat 4E8 but now with log reaction time log(rt). Make as many bins as in 4E8.\nNote: In hist(), the argument breaks = k creates k equally spaced bins.\n\n\n\n4E10.\n\nMake two boxplots, one for rt and the other for log(rt), using data from 4E7.\nHow many observations were classified as outliers according to the boxplot rule for rt and for log(rt).\n\n\n\n\n4E11. Assume that the waist circumference among Swedish middle aged men (40-60 y) is normally distributed with mean = 100 cm and standard deviation = 7 cm.\nCalculate the probability that a randomly selected man form this population has a waist circumference …\n\nabove 94 cm,\nwithin \\(100 \\pm 10\\) cm,\neither below 85 cm or above 115 cm,\nwithin the middle 50 % of the distribution.\n\nHint: A useful function in R is pnorm().\n\n\n\n4E12. Below a standard normal distribution. For each panel, approximate by eye (and brain!) the probability that a random observation would fall in the shaded region. Use the 68-95-99.7 rule where appropriate.\n\n\n\n\n\n\n\n\n\n\n\n\n4E13. Use R to calculate exact answer (to 5 decimals) for the probabilities in 4E12.\nHint: A useful function in R is pnorm().\n\n\n\nMedium\n\n4M1.\n\nSearch the internet for “geometric mean” and then explain in your own words what it is and when to use it.\nCalculate the geometric mean of the sample in 4E7 (vector rt). Here is one way to do it in R: exp(mean(log(rt))).\nWas the geometric mean for the sample close to the population geometric mean? To find the latter, decipher the simulation code in 4E7.\n\n\n\n\n4M2. Student X is taking an exam with 10 multiple choice questions, each with 5 response alternatives of which one is the correct answer. X has no clue and responds randomly.\n\nIllustrate the Probability Mass Function (PMF), that is, make a diagram with the probabilities of each of the possible outcomes 0, 1, …, 10 correct answers (see examples above, the function dbinom() is helpful)\nCalculate the probability that he answers correct on exactly 4 questions\nCalculate the probability that he answers correct on 4 or more questions\n\n\n\n\n4M3. Below is a kernel density plot of a data set.\n\nBased on the plot, suggest an interval within which you are 50 % confident that the arithmetic mean value of the dataset lies. That is, you would feel equally surprised whether your interval includes the mean or not.\nSuggest an interval within which you are 95 % confident that the mean value of the dataset lies. So you would be much more surprised if your interval does not include the mean than if it does.\n\nI realize it’s a somewhat unusual question, but think of it as an exercise in thinking about subjective probability.\n\n\n\nCode\nset.seed(321)\ny1 &lt;- -10 * rlnorm(1000, mean = log(5), sd = log(1.5)) + 199\ny2 &lt;- rnorm(100, mean = 100, sd = 10)\ny &lt;- c(y1, y2)\nplot(density(y), xlab = \"X\", ylab = \"\", main = \"\", axes = FALSE)\naxis(1, at = seq(40, 200, 5), cex.axis = 0.8)\n\n\n\n\n\n\n\n\n\nCode\n# median(y)\n# mean(y)\n\n\n\n4M4. This is a data set of 7 observations: \\(y = [1, 3, 3, 4, 4, 5, 8]\\), with\n\\(mean(y) = 4\\).\n\nCalculate the minimum and maximum values after mean-centering this dataset.\nApply the transformation ynew = (y - mean(y))/(0.1*sd(y)). What is the mean and standard deviation of ynew.\nTransform the data to have a minimum = 0 and maximum = 70. What is the median value of this transformed data.\n\n\n\n\n4M5. Create a data set with 7 observations that has mean = 100 and sd = 15 (exactly). Describe your method by providing R-code.\n\n\n\nHard\n\n4H1. Go back to 4E11 and answer this question: What is the probability that a randomly selected man has a waist-circumference of 96 cm? Answer in a way that also explains the difference between probability and density.\n\n\n\n4H2.\n\nDraw the distribution from which the reaction time data in 4E7 was simulated. (You may look at the R-code that I used to draw normal distributions in the notes above).\nCalculate the probability that a randomly drawn observation from the distribution would be greater than 1000 ms.\nCalculate the probability that a randomly drawn observation would be between 200 and 800 ms.\n\n\n\n\n4H3. Simulation is fun! And useful! 4E11(d) was maybe too easy (answer = 0.5). A harder version would ask for the waist-circumference interval corresponding to the middle 50 % of the data. My answer is \\([95.3, 104.7]\\) (rounded to 1 decimal). Use simulation to verify (or falsify) my answer.\n\n\n\n4H4. Use pnorm() and qnorm() to calculate the exact interval in 4H3. Report with four decimals.\n\n\n\n4H5. Here another simulation exercise:\nDraw 10,000 samples of size \\(n = 16\\) from a normal distribution with \\(\\mu = 100\\) and \\(\\sigma = 15\\). For each sample, calculate a t-value, that is, calculate\n\\(t = \\frac{\\mu - \\bar{x}}{s_{x}/\\sqrt(n)}\\),\nwhere \\(\\bar{x}\\) is the sample mean and \\(s_{x}\\) is the sample standard deviation.\n\nPlot the distribution of your 10,000 t-values in a histogram overlaid with a kernel density plot.\nFigure out a way to demonstrate that the distribution is not normal (although it looks pretty normal).\nIn fact (as you know), your sample means follows a T-distribution with df = n-1. Use the function dt() in R to draw this distribution on your histogram from (a).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "04-stat25.html#session-info",
    "href": "04-stat25.html#session-info",
    "title": "4  S: Descriptive Statistics and Visualization",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] MASS_7.3-61\n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.2    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.4.2       htmltools_0.5.8.1 rstudioapi_0.17.1 yaml_2.3.10      \n [9] rmarkdown_2.29    knitr_1.50        jsonlite_2.0.0    xfun_0.52        \n[13] digest_0.6.37     rlang_1.1.6       evaluate_1.0.3   \n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHowell, D. C. (2012). Statistical methods for psychology. Cengage Learning.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>S: Descriptive Statistics and Visualization</span>"
    ]
  },
  {
    "objectID": "05-method25.html",
    "href": "05-method25.html",
    "title": "5  M: Measurement and Data Quality",
    "section": "",
    "text": "Topics\nConstruct validity. The validity with which inferences are made from the operations and settings in a study to the theoretical constructs those operations and settings are intended to represent. It is about the correct labeling of variables and accurate language use.  Steiner et al. (2023), p.25",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#topics",
    "href": "05-method25.html#topics",
    "title": "5  M: Measurement and Data Quality",
    "section": "",
    "text": "Measurement\nScale levels\nVariables: Manifest and latent\nMeasurement error and bias\n\nTest reliability\nTest validity\nSelection, including missing data\n\nMisclassification (a type of measurement error)\nExamples of biases related to measurement and to causal interpretation of measures\n\nResponse bias\nMono-method bias\nExperimenter bias\nDemand characteristics\nRegression fallacy\nPlacebo effect\nHawthorn effect\n\nExamples of design tricks to reduce the risk of measurement-related bias:\n\nReliable and valid measures\nRepeated testing (averaging over many stimulus repetitions)\nBlind, double blind, triple blind tests\nMasked purpose\nFiller items\nRandom stimulus order\nNegative controls, i.e., experimental controls designed to ensure that no effect is observed when no effect is expected.\n\nNegative exposure controls: Involve exposures that are not expected to cause the outcome of interest.\nNegative outcome controls: Measure outcomes that are not expected to be affected by the exposure of interest.\n\n\n\nTheoretical articles to read:\n\nWilkinson (1999), measurement and data quality discussed at several places\nGelman et al. (2021) chapter 2 devotes a large part to measurement and data quality\n\n\n\n\n\n\nMeasurement\nVariables. Explicitly define the variables in the study, show how they are related to the goals of the study, and explain how they are measured. The units of measurement of all variables, causal and outcome, should fit the language you use in the introduction and discussion sections of your report.\nWilkinson (1999), p. 595",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#measurement",
    "href": "05-method25.html#measurement",
    "title": "5  M: Measurement and Data Quality",
    "section": "5.1 Measurement",
    "text": "5.1 Measurement\nMeasurement is fundamental to empirical science. However, there seem to be no single agreed upon definition of “measurement”. In physics, measurement is typically defined as the estimation of ratios of quantities (or quantifiable attributes). For example, if an object is twice as long as the standard meter [m], then its length is 2 m (length being an attribute of the object, 2 being its magnitude relative to the standard).\nIn psychology (and similar fields), it may, of course, be hard to establish ratios: What would it mean to say that someone is twice as depressed as a standard case of depression? It seem that we need a weaker definition of “measurement” to be able to claim that we can measure things like degree of depression, happiness, annoyance, trust, empathy, health, …\nThe most famous weak definition was proposed by the psychologist S.S. Stevens:\n\n… measurement, in the broadest sense, is defined as the assignment of numerals to objects or events according to rules.\nStevens (1946).\n\nThis definition is admittedly weak, and we can likely improve upon it. However, let’s leave that task to the measurement theorists (e.g., Michell (1997)). For our purposes, Stevens’ definition will suffice.\n\nScale levels\nStevens identified four levels of measurement scales:\n\nNominal. Naming attributes: same attribute (e.g., nationality), same name (“Denmark”, “Norway,”Sweden”, …).\n\nPermissible transformations: any change of names consistent with the “same attribute, same name” rule, for example, from “Denmark”, “Norway,”Sweden” to “DK”, “NO”, “S”, or from “male” and “female” to “1” and “2”.\n\nOrdinal. Rank-order of attributes, for example, 1st, 2nd, 3rd,… place in a contest.\n\nPermissible transformations: Any monotonic transformation that keeps the rank-order intact.\n\nInterval. Determining equality of intervals or differences. Equal scale steps means equal distance in term of the attribute. For example, if the 2nd place came in 5 seconds after the winner, and the 3rd placed come in 10 seconds after the winner, then the scale values 0, 5, 10 seconds for, 1st, 2nd, and 3rd place, respectively, would be on an interval scale level. Note: the zero point of an interval scale is arbitrary, therefore we cannot say that 2nd place was twice as fast as the 3rd place, but we can say that the distance between 1st and 3rd was twice as large (10 s) as the distance between 1st and 2nd (5 S). Interval scales implies a ratio scale of differences.\n\nPermissible transformations: Linear transformation: \\(x' = b_0 + b_1x\\). For example, if \\(b_0 = 50\\) and \\(b_1= 2\\), then \\(x'\\) would be 50, 60 and 70 for the 0, 5, 10 seconds values above. The differences between 1st and 2nd is still the same as between 2nd and 3rd, and the difference between 3nd and 1st is still twice as large as between 2nd and 3rd.\n\nRatio. Determining equality of ratios. The scale has a meaningful zero, so it is meaningful to talk about ratios, as well as differences. If the finishing times were 100, 105, and 110 seconds for 1st, 2nd, and 3rd place, respectively, then we may say that the 2nd place was 5 % and the 3rd place was 10 % slower than the winner.\n\nPermissible transformations: Linear transformation with zero intercept, \\(x' = b_1x\\), preserving the meaningful zero point. The finishing times 100, 105, and 110 seconds may for example be transformed to milliseconds by \\(x'  \\ [ms] \\  = 1000 x \\ [s]\\).\n\n\nThe view that measurement is the estimation of ratios of quantities would include measurement at the interval or ratio scale level, but would not accept rank-ordering or naming (nominal) as measurement. But since these are examples of “assignment of values according to rule”, they are measurement according to S.S. Stevens.\nThree remarks:\n\nSteven’s four categories are not exhaustive, other type of scales exist that do not fit into these four categories. For example, the decibel scale is a log-interval scale, where equal steps means equal ratios.\nIt is not a matter of either or. Many measurement scales are somewhere between categories. In psychology, many self-report scales are somewhere between ordinal and interval levels, for example, Visual Analogue Scale (VAS) and Likert scales.\nScale values (scores) may be on well-defined level, but that does not necessarily mean that the underlying latent construct is. For example, reaction time (RT) in millisecond is on a ratio scale, but if used to measure “discrimination ability” (latent construct) it does not necessarily follow that RT measures “discrimination ability” on a ratio scale, see next on the distinction between variables and latent construct.\n\n\nAlways try to measure your key variables on well-defined scales, in particular your outcome variable. This is crucial if your goal is to assess causal effects, that is, to assess a difference (or ratio, or some other contrast) between potential outcomes. Differences require at least interval-scale data, so you need either to show that this is at least approximately true for your outcome measure, or you need to state this as an explicit assumption of your assessment.\n\nMatching strategy to obtain meaningful units\nSometimes it is possible to use a matching strategy to transform a less well-defined outcome variable on a well-defined physical scale. Here is an example:\n\n\nCode\n# Simulate data\nset.seed(3212)\nx &lt;- rep(seq(35, 85, 5), 2)\nd &lt;- rep(c(0, 1), each = length(x)/2 )\ny &lt;- 1 + 0.8*x + 4.5 * d + rnorm(length(x), 0, 1)\n\n# Fit regression model\nmfit &lt;- lm(y ~ x + d)\ncf &lt;- mfit$coefficients\n\n# Empty plot\nplot(x, y, pch = \"\", ylim = c(1, 100), xlim = c(30, 90), \n     xlab = \"Sound pressure level, dB(A)\", ylab = \"Loudness rating (mm VAS)\")\n\n# Add prediction lines\nxx &lt;- 35:85\nlines(xx , cf[1] + cf[2]*(xx ), col = \"black\", lty = 3)\nlines(xx , cf[1] + +cf[3] + cf[2]*xx, col = \"black\", lty = 3)\n\n# Add points\npoints(x[d == 0], y[d == 0], pch = 21, bg = \"white\")\npoints(x[d == 1], y[d == 1], pch = 21, bg = \"darkgrey\")\n\n# Add legend\npoints(35, 95, pch = 21, bg = \"darkgrey\", cex = 0.8)\ntext(36, 95, labels = \"Low-frequency noise\", cex = 0.8, pos = 4)\npoints(35, 90, pch = 21, bg = \"white\", cex = 0.8)\ntext(36, 90, labels = \"High-frequency noise\", cex = 0.8, pos = 4)\n\n# Add regression equation\neq &lt;- paste(\"y = \", round(cf[1], 1), \" + \", round(cf[2], 1), \" * Level + \", \n            round(cf[3], 1), \" * Type\", sep = \"\")\ntext(30, 20, labels = eq, cex = 0.8, pos = 4)\n\n\n\n\n\n\n\n\n\nA listener rated the loudness of sounds on a visual analogue scale (VAS) with endpoints labeled ‘very soft’ and ‘very loud.’ The sounds were dominated by either low-frequency or high-frequency components, with their overall levels systematically varied in terms of A-weighted sound pressure level, dB(A). On average, low-frequency sounds were rated as louder than high-frequency sounds, with a difference of about 5 units (mm) on the loudness VAS. However, the interpretation of these units is unclear—what does a difference of 5 units really signify?\nA more meaningful comparison can be made by expressing this difference in decibels, specifically along the x-axis. Since the regression lines for the two frequency groups were parallel, the vertical difference was constant, corresponding to approximately 6 dB(A). In psychoacoustics, dB(A) is a well-understood measure, and a 6 dB(A) difference is considered substantial. Therefore, the results can be interpreted in terms of loudness matching: at the same dB(A) level, low-frequency sounds are perceived as louder, and the high-frequency noise needs to be increased by 6 dB(A) to be perceived as equally loud.\n\nHere is another example form an observational study of the causal effect of aircraft noise exposure of schools on the pupils’ performance on a test of reading comprehension. The study was conducted around airports in three countries, UK, Netherlands, and Spain, and reading comprehension was assessed using nationally standardized and normed tests (different for the three countries). To compare across countries, outcome scores from each country was standardized so a score of 0 is equal to the average score of all pupils, 1 is equal to a performance corresponding to one standard deviation above the average score, etc.\n\n\n\nFig. 1 from Stansfeld et al. (2005)\n\n\nThe figure show a decline in performance (z-scores) with higher aircraft noise exposure (linear regression with a slope of about -.01 per dB) and the author’s concluded that “Our findings indicate that a chronic environmental stressor, aircraft noise, could impair cognitive development in children, specifically reading comprehension” Stansfeld et al. (2005).\nBut how large was the estimated causal effect? It is not easy to judge from z-scores from a reading-comprehension test. The data was from three countries, and the authors used national data to express the result in months of reading delay as a function of exposure: “A 5 dB difference in aircraft noise was equivalent to a 2-month reading delay in the UK and a 1-month reading delay in the Netherlands. There are no national data available for Spain”.\n\n\n\n\nObserved and latent variables\nVariables are set of values assigned to attributes of objects. For instance, we may define “age” as a variable that may take integer values between 20 and 80. Our sample may have been restricted to participants in that age range and we did not bother to measure age to decimals of years. Despite these limitations, it make sense to treat this as a continuous variable at ratio level. We may define a variable “eye-color” to have one of four values “blue”, “brown”, “green”, “other”, and this variable would be at the nominal scale level, because there is no meaning in saying that one value is greater or lesser than another. This would of course still be true if we recoded the variable to let the integers 1, 2, 3, and 4 represent “blue”, “brown”, “green”, and “other”, respectively.\nAll measured values can be viewed as reflections of an underlying and unobservable latent variable (or construct). Sometimes, this distinction is trivial and therefore ignored, as we can be pretty sure that the measured value is just proportional to the latent construct. For example, it seem safe to assume that measurements in centimeters are proportional to the latent variable length. It is much less obvious for latent variables such as “intelligence” or “depression”. In order to avoid ambiguity, it is a good idea to stick to what is observable and prefer “IQ-score” over “intelligence”, or “depression-score” over “depression”.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#effect-size-measures",
    "href": "05-method25.html#effect-size-measures",
    "title": "5  M: Measurement and Data Quality",
    "section": "5.2 “Effect-size” measures",
    "text": "5.2 “Effect-size” measures\nRead Wilkinson et al. !\n\nEffect sizes. Always present effect sizes for primary outcomes. If the units of measurement are meaningful on a practical level (e.g., number of cigarettes smoked per day), then we usually prefer an unstandardized measure (regression coefficient or mean difference) to a standardized measure (r or d). It helps to add brief comments that place these effect sizes in a practical and theoretical context.\nWilkinson (1999), p. 599,\n\n“Effect” is a causal term, and the statement that a treatment has an effect on an outcome is a causal statement. Thus, “effect size” implies (or should imply) causality. However, the term “effect size” is typically used in a broader sense than this, including contrasts (Cohen’s \\(d\\)) or associations (e.g., Pearson’s \\(r\\)) between variables that we do not believe to be causally related. In this course, we will restrict our use of the term “effect size” to estimates of a causal effect sizes, and use “difference”, “contrast”, or “association” when we talk about relationships without claims of causality.\n\n\nCode\n# Cohen's d simulation ... under construction ...\nset.seed(999)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#measurment-error",
    "href": "05-method25.html#measurment-error",
    "title": "5  M: Measurement and Data Quality",
    "section": "5.3 Measurment error",
    "text": "5.3 Measurment error\n\nRandom error: Reliability of measurement.\nSystematic error (bias): Validity of measurement.\n\nRandom error generally adds to the variability of results and thereby to the uncertainty of estimated effects. Averaging over repeated measures (e.g., many items in a questionnaire) reduces random error (increase reliability) but not systematic error. An invalid test will bias effect estimates away from the true effect also when random measurement error is low.\n\n\nMisclassification\nClassification is a kind of measurement (in Stevens’s sense), typically at the nominal or ordinal level. For instance, individuals may be classified in terms of a categorical outcome variable (e.g., Dead versus Alive, or Cases with versus Cases without disease) or exposure variable (e.g., Exposed versus Non-exposed, or Noise exposure categories: &lt;45, 45-49, 50-54, 55-59, 60-64, &gt;= 65 dB).\nRandom misclassification, also called nondifferential misclassification, will generally attenuate effect size estimates. For example, the relative risk of disease for exposed versus non-exposed individuals will be lower with nondifferential misclassification of exposure or outcome or both.\nSystematic misclassification, also called differential misclassification, is categorization errors that are related to other variables of interest to the causal question. This may introduce bias in either direction, that is, it may lead to effect estimates that over- or underestimate the true effect.\nAs an example, Diagnostic suspicion bias (e.g. https://catalogofbias.org/biases/diagnostic-suspicion-bias/) may lead to differential misclassification. Example: Emphysema is diagnosed more frequently in smokers than in non-smokers. However, smokers may visit the doctor more often for other conditions (e.g. bronchitis) than non-smokers, which means that a reason smokers could be diagnosed with emphysema more often is simply because they go to the doctor more often – not because they actually have higher odds of getting the disease. Unless steps are taken to control for this possibility, emphysema will be under-diagnosed in non-smokers, which is a classification error because the diagnosis is related to the variable “how often smokers visit the doctor, versus non-smokers”. (I borrowed this example from another source, but unfortunately, I’ve lost the reference.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#fooled-by-measurment",
    "href": "05-method25.html#fooled-by-measurment",
    "title": "5  M: Measurement and Data Quality",
    "section": "5.4 Fooled by measurment",
    "text": "5.4 Fooled by measurment\nIn this section, I will give some examples how we may be mislead by measurement. Note that for some of these examples, the issue may not be with quality of measurement, but rather with the interpretation of measured values. For example, performance of a work unit may be measured as the number of items produced on a day, and this measure may have high reliability and high validity. However, we may still question it as a measure of the effect of our treatment (e.g., improved lighting conditions) as we might have obtained the same result no matter the treatment. Maybe the fact that the workers knew they were participants in a study made all the difference (the so called Hawthorne effect).\n\nExamples of biases related to measurement and to causal interpretation of measures:\n\nResponse bias. General term for systematic errors in self-reports, for instance a tendency to avoid (or prefer) end-points of response scales.\nExperimenter bias. General term for how an experimental leader unconsciously may influence the results, for example, by subtle hints to favor results in line with the experimenters hypothesis.\nSocial desirability bias. Respondents may answer in a way that make them look better than they are.\nMono-method bias. Measuring all variables with the same method may introduce spurious association, for example, a questionnaire may introduce spurious associations related to systematic biases in how respondents answer questions (response bias).\nDemand characteristics, general term for the risk that participants may respond in a way influenced by their interpretation of the purpose of the experiment.\nWrong mechanism bias. This is my term for causal misinterpretation of measurement.\n\nHawthorne effect. The effect of treatment is fully or partly caused by the participants awareness of being under study.\nPlacebo effect Real effect after receiving a fake or inactive treatment, known as a placebo treatment.\n\n\n\nExamples of design tricks to reduce the risk of bias:\n\nRandom order of stimuli or questionnaire items, to reduce risk for Item-order bias.\nMasked purpose to reduce risk of bias related to demand characteristics. Example: Filler items or stimuli.\nBlinding: single, double, triple blind testing, to reduce risk for experimenter bias, and to be able to assess Placebo effects\nNegative control conditions, that is, a control condition where we do not expected a difference between control and treatment group. Such diverging evidence is a valuable compliment to converging evidence of effects where expected.\n\n\n\nMono-method bias\n\n\n\n\n\n\n\n\n\nLatent variables (unmeasured): JobSatisfaction, LunchRoomSatisfaction, Response bias\nManifest variables (measured): JSscore, LRSscore",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#fooled-by-randomness",
    "href": "05-method25.html#fooled-by-randomness",
    "title": "5  M: Measurement and Data Quality",
    "section": "5.5 Fooled by randomness",
    "text": "5.5 Fooled by randomness\nIn this section, I will give some examples how we may be mislead by random error. Note that these examples may relate to sampling errors or to measurement errors or both in combination.\n\nSmall study, uncertain effect: Point estimates based on small studies (and/or based on irreliable measures) may substantially over- or underestimation the true effect. It is therefore important to not only focus on point estimates, but always discuss uncertainty, typically in relation to a compatibility interval around the point estimate. The term statistical power, understood broadly, could be used here. But I will avoid it, as it has a very specific meaning related to null-hypothesis significance testing (\\(power = 1 - \\beta\\), where \\(\\beta\\) is the Type-II-error rate). See Gelman et al. (2021), chapter 16 for a detailed discussion statistical power.\nSmall study, large effect: Small sample fallacy, see below\nRegression fallacy: Failing to appreciate that a large random error typically is followed by a smaller random error on a repeated test, see below\n\n\nSmall sample fallacy\nYou will find the largest and the smallest effects in studies with small samples, because random errors are more likely to cancel out in large than in small samples. Sometimes this statistical phenomenon is mistaken for a real causal effect. For instance, the average performance on standardized tests will probably be highest in small schools, but this is not because small schools make students better but because of random variability. If you looked at the other end, the worst average performance would probably also be from small schools. Here is a simulation of this phenomenon illustrated in a so called funnel plot. Here a sneaked in the sapply() function that is useful for repeated use of a function with different arguments each time.\n\n\nCode\nset.seed(999)\n\n# Size of 250 schools, from n = 10 to n = 1000\nnn &lt;- sample(10:1000, size = 250, replace = TRUE)  # Random uniform\n\n# Function that simulates a mean score for a school of size n \nmean_score &lt;- function(n){\n  out &lt;- mean(rnorm(n, mean = 100, sd = 15))\n  out \n }\n\n# Apply function on each of the school sizes in nn, using the sapply() function\nx &lt;- sapply(nn, mean_score)  # User-made function as argument!\n\n# n for worst school\nlargest_neg &lt;- nn[x == min(x)]  # Size of school with the worst performance\n\n# n for large effect sizes\nlargest_pos &lt;- nn[x == max(x)] # Size of school with the best performance\nss &lt;- c(worst_score = min(x), n_worst = largest_neg, \n        best_score = max(x), n_best = largest_pos)\n\n# Funnel plot\nplot(x, nn, xlim = c(90, 110), ylab = \"School size\", \n     xlab = \"Mean performance on test\")\n\n\n\n\n\n\n\n\n\nSchool-size of worst and best average performance\n\n\nCode\nround(ss, 1)\n\n\nworst_score     n_worst  best_score      n_best \n       95.2        54.0       106.4        36.0 \n\n\n\n\nRegression fallacy\nRegression fallacy: Regression to the mean interpreted as a causal effect.\n\nSports Illustrated Jinx. A successful athlete that make it to the front page of the prestigious sport journal Sport Illustrated will performance less well in the coming season (from Gilovich (2008)).\nA flurry of deaths by natural causes in a village led to speculation about some new and unusual threat. A group of priests attributed the problem to the sacrilege of allowing women to attend funerals, formerly a forbidden practice. The remedy was a decree that barred women from funerals in the area. The decree was quickly enforced, and the rash of unusual deaths subsided. This proves that the priests were correct (from Gilovich (2008)).\nOld exam question: “Children of chess world-champions are likely to become proficient chess players, but so far none have reached the top 100, presumably because chess world champions tend to be selfish persons unwilling to share their knowledge with others, including their children”. Find an alternative explanation.\nSuccess: Talent + Hard Work + Luck\nGreat success: a little more Talent + more Hard Work + much more Luck.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#practice",
    "href": "05-method25.html#practice",
    "title": "5  M: Measurement and Data Quality",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\n\nEasy\n\n5E1. Here some common measurement scales. Discuss how they relate to Steven’s classification of nominal, ordinal, interval, and ratio scales.\n\nIQ-scores measured on a scale with mean = 100 and standard deviation of 15\nWeight in kilograms\nWhat’s your gender:\n\n Male\n Sex\n Other\n\n“It is great fun to study Research methods”\n\n  Disagree completely\n  Disagree somewhat\n  Agree somewhat\n  Agree completely\n\nNumber of children less than 18 years living in your home\nSound pressure level in decibels\n\n\n\n\n5E2. What is the difference between intelligence and IQ-score?\n\n\n\n5E3.\n\nMeasurement error of nominal-level measurements may be called misclassification, explain?\n\nWhat is the difference between differential and non-differential misclassification?\n\n\n\n\n5E4. Construct validity is about “correct labeling of variables and accurate language use” (Steiner et al. (2023)). Explain with an example.\n\n\n\n5E5. “Children of chess world champions are often skilled players, but none have yet reached the top 100. This because chess champions are typically focused on their own success and are reluctant to share their expertise with others, including their own children”. Please find an alternative explanation.\n\n\n\nMedium\n\n5M1. Reaction time [ms] as a measure of time is clearly on a ratio scale, but what if its used as a measure of intelligence? Discuss with reference to the concepts “manifest” and “latent” variable.\n\n\n\n5M2. From Wikipedia: Construct validation is the accumulation of evidence to support the interpretation of what a measure reflects. What does this mean? Interpret in terms of latent and observable variables.\n\n\n\n5M3. An influential book considered the Placebo effect and the Hawthorne effect as threats to construct validity, that is, related to the interpretation of measurement scores. Do you agree?\n\n\n\n5M4. Explain how negative controls may be used in support for a causal interpretation of data.\n\n\n\n5M5. Demand characteristics is a curious term in psychology research. Define what it means and provide your own examples to illustrate how demand characteristics might influence the outcomes of a study.\n\n\n\n5M6. A visual analogue scale (VAS) typically consists of a straight horizontal line with labels at each end representing the extremes of the characteristic being measured (e.g., ‘no pain’ to ‘worst possible pain’). Respondents indicate their answer by marking a point on the line. The score is calculated by measuring the distance, in millimeters, from the left end of the line to the respondent’s mark.\nDiscuss the scale level of VAS scores. Specifically, consider whether these scores should be interpreted as ordinal, interval, or ratio data, and explain the implications of each interpretation in terms of statistical analyses that might be conducted using these scores.\n\n\n\n5M7. Speed cameras were installed at traffic sites that had experienced eight or more accidents over the last three years. In the following three-year period, the number of accidents at these sites dropped from 225 to 125. At sites where no cameras were installed, no reduction in accidents was observed. Based on this data, it was concluded that the speed cameras had prevented about 100 accidents. Explain why this conclusion may overestimate the impact of the speed cameras.\n Inspired by David Hand’s book, The Improbability Principle (Hand (2014)), where he references Mountain (2006), highlighting the challenges in assessing the impact of speed cameras.\n\n\n\nHard\n\n5H1. Pearson’s coefficient of correlation, \\(r\\), is a common measure of the strength of the linear relationship between two variables. Discuss the scale level of \\(r\\) using Stevens’s terminology.\n\n\n\n5H2. In a randomized experiment involving a sample of men and women aged between 18 and 75 years, the average weight loss under my new diet plan was 6 kg (SD = 5 kg), compared to 4 kg (SD = 5 kg) under a conventional diet. Therefore, the unstandardized causal effect size was 2 kg.\n\nCalculate Cohen’s d for this effect.\n\nIn a follow-up study with a more homogeneous sample (only middle-aged men), the unstandardized effect size remained 2 kg, but Cohen’s d was larger. Explain how this is possible.\n\nBased on the results, would you conclude that the average causal effect of the diet was the same or different in the two studies? Discuss.\n\n\n\n\n5H4. “… when signal is low and noise is high, statistically significant patterns in the data are likely to be wrong, in the sense that the results are unlikely to replicate” (Gelman et al. (2021), p. 292) Explain and relate to the Small-sample fallacy.\n\n\n\n5H5. “Researchers wanted to compare kids from a low and a high socioeconomic area on their response to a new pedagogic method for teaching math. They first administered a short math test to all kids in each area and noted the the average score among the kids from the high socioeconomic area was much higher than the average score among the kids from the low socioeconomic area. To achieve balanced groups, they took a random sample of 50 kids from the low socioeconomic area and then matched each kid with a kid from the high socioeconomic area with a similar score on the math test. Both groups were then subjected to the new pedagogic method, after which they were tested again on a similar test. The 50 low socioeconomic kids had about the same average result as before, whereas the 50 high socioeconomic kids had a higher average score after than before. The conclusion was that the method seems to work, but only for kids with a high socioeconomic background. Give an alternative explanation in terms of random error”. (This is an old exam question)\n\n\n\n5H6. Use R or some other software to simulate mono-method bias. Simulate two latent variables T1 and T2 that are independent and measures of these X1 and X2 that are associated. Explain in words the mechanism of your simulated response bias.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "05-method25.html#session-info",
    "href": "05-method25.html#session-info",
    "title": "5  M: Measurement and Data Quality",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       rlang_1.1.6       jsonlite_2.0.0   \n[17] V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nGilovich, T. (2008). How we know what isn’t so. Simon; Schuster.\n\n\nHand, D. J. (2014). The improbability principle: Why coincidences, miracles, and rare events happen every day. Scientific American/Farrar, Straus; Giroux.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nMichell, J. (1997). Quantitative science and the definition of measurement in psychology. British Journal of Psychology, 88(3), 355–383.\n\n\nMountain, L. (2006). Safety cameras: Stealth tax or life-savers? Significance, 3(3), 111–113.\n\n\nStansfeld, S. A., Berglund, B., Clark, C., Lopez-Barrio, I., Fischer, P., Öhrström, E., Haines, M. M., Head, J., Hygge, S., Van Kamp, I., et al. (2005). Aircraft and road traffic noise and children’s cognition and health: A cross-national study. The Lancet, 365(9475), 1942–1949.\n\n\nSteiner, P. M., Shadish, W. R., & Sullivan, K. J. (2023). Frameworks for causal inference in psychological science.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677–680.\n\n\nWilkinson, L. (1999). APA task force on statistical inference. Statistical methods in psychology journals: Guidelines and explanations. American Psychologist, 54(8), 594–604.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>M: Measurement and Data Quality</span>"
    ]
  },
  {
    "objectID": "06-stat25.html",
    "href": "06-stat25.html",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#topics",
    "href": "06-stat25.html#topics",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "",
    "text": "Point estimates\nInterval estimates\n\nCompatibility interval. A range of parameter values compatible with the observed data and the assumptions of the chosen statistical model. The term “compatibility interval” serves as an umbrella for intervals such as “confidence intervals” and “credible intervals.”\nResist dichotomous thinking! Whether “… intervals include or exclude zero should play no role in their interpretation, …”, Amrhein et al. (2019); “Forget about p-values, and forget about whether your confidence intervals exclude zero.” Gelman et al. (2021) (p. 493)\n\nType M and Type S errors.\nSampling distribution and standard error\n\nConfidence intervals. The classical (frequentist) compatibility interval, derived analytically or using simulation (bootstrapping).\n\nLikelihood function\n\nCredible intervals. The Bayesian compatibility interval, it is the likelihood function weighted by prior probability.\n\nBayesian inference\n\nGeneral idea: Method for updating beliefs in light of new data using Bayes’ rule\nPrior distribution\nLikelihood function\nPosterior distribution\n\n\nReadings.\n\nAmrhein et al. (2019)\nGelman et al. (2021)\n\nCh 3: Section 3.5, focus on normal distribution, linear transformation, and binomial distribution. See also “comparing distributions”, for use of percentiles\nCh 4: You may skip section 4.6\nCh 5: Section 5.1, for a simulation involving the binomial distribution,\nCh 5, section 5.4 on bootstrapping.\nAppendix B “Forget about statistical significance”\n\nWilkinson (1999), in particular section on hypothesis tests, sizes and interval estimates.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#estimation",
    "href": "06-stat25.html#estimation",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "6.1 Estimation",
    "text": "6.1 Estimation\nRead Wilkinson et al. !\n\nHypothesis tests. It is hard to imagine a situation in which a dichotomous accept-reject decision is better than reporting an actual p value or, better still, a confidence interval. Never use the unfortunate expression “accept the null hypothesis.” Always provide some effect size estimate when reporting a p value.\nEffect sizes. Always present effect sizes for primary outcomes. If the units of measurement are meaningful on a practical level (e.g., number of cigarettes smoked per day), then we usually prefer an unstandardized measure (regression coefficient or mean difference) to a standardized measure (r or d). It helps to add brief comments that place these effect sizes in a practical and theoretical context.\nInterval estimates. Interval estimates should be given for any effect sizes involving principal outcomes. Provide intervals for correlations and other coefficients of association or variation whenever possible.\nWilkinson (1999), p. 599.\n\n\n\n\n\nExample from Eriksson et al. (2014)\n\n\nFrom the inspirational article for Data set 3. The table show regression coefficients (point estimates) with 95 % confidence intervals.\n\n\n\n\nExample from Van Hedger et al. (2019)\n\n\nThis is the inspirational article for Data set 1. The figure show mean z-scores (point estimates) with standard errors of the mean (i.e., 68 % confidence intervals).\n\n\nWhat is estimated?\nUnknown and typically unobservable parameter value to be estimated:\n\nSingle parameter value, Population proportion, mean, median, mode, … For example, the median income in a population.\nContrast: Difference, ratio or some other comparison of two parameter values that may not be related to a causal research question. For example, estimating the difference in income between males and females.\nMeasure of association: Degree of association between variables in the population. For example, the population coefficient of correlation between age and income in a population.\nEffect size: A population contrast or measure of association (in population) related to a specific causal research question. Note: The term “effect size” is typically used in a broader sense than this, including contrasts between variables that we do not believe to be causally related (see also notes on measurement, ch.5). In this course, we will restrict our use of the term “effect size” to estimates of causal effect sizes (which makes a lot of sense given that “effect” is a causal term). The Population average treatment effect (PATE, see Gelman et al., 2021, p. 343) is an example of a population effect size defined as a contrast between unknown parameters.The Sample average treatment effect (SATE, see Gelman et al., 2021, p. 343) may also be included here. Although it is about properties of a sample, it is defined as a contrast between unknown parameters: unobserved single-unit casual effects among sample units.\n\n\n\n\nPoint and interval estimate\nPoint Estimate\n\nSample statistic: proportion, mean, median, mode, …\nSample contrast or association:\n\nUnstandardized contrast: Difference between means (or medians), regression coefficient, …\nRelative proportions: Relative risk, Risk difference, Odds ratio …\nStandardized contrast: Cohen’s d, Glass’ \\(\\delta\\), Hedge’s g, …\nStandardized association: Pearson’s r, Spearman’s \\(\\rho\\), \\(R^2\\), \\(\\eta^2\\), …\n(Case-wise proportions: Probability of superiority, … not part of the course)\n\nSample effect size: Sample contrast or association related to a specific causal research question. Again: The term sample “effect size” is typically used in a broader sense than this, including measures of contrasts between variables that we do not believe to be causally related. But I will try to restrict the use of “effect” to statements about causal effects.\n\n\nInterval\nPoint estimates should be accompanied by interval estimates. Three general types of intervals are\n\nConfidence intervals, from the Frequentist statistic perspective, derived from sampling distribution, e.g., \\(95\\%\\;CI \\approx \\bar{X} \\pm 2 * SE_{\\bar{X}}\\), or by simulation (bootstrapping).\nCredible interval, from the Bayesian perspective, derived from the posterior distribution (often using simulation methods) estimated from prior distribution and likelihood function.\nLikelihood intervals, similar to credible interval, but without incorporating prior beliefs (i.e., assuming “flat priors”).\n\nThe good news: For the same data, confidence intervals, credible intervals, and likelihood intervals tend to be quite similar, especially when derived from large datasets.\n\nCompatibility interval: a generic term that recently has become popular:\n\n” .. we can think of the confidence interval as a “compatibility interval” …, showing effect sizes most compatible with the data according to their P-values, under the model used to compute the interval. Likewise, we can think of a … Bayesian “credible interval,” as a compatibility interval showing effect sizes most compatible with the data, under the model and prior distribution used to compute the interval … ” (from Amrhein et al., 2019)\n\n\nGelman et al. (2021) discusses the term on pp. 110-111 (Gelman seem to prefer the term “uncertainty interval”, but I prefer “compatibility interval” as it makes no claims about our state of certainty; “compatibility” only refer to the relation between, on the one hand, possible parameter values, and, on the other, our data and statistical model).\n\n\n\nType M (magnitude) and Type S (sign) errors\nGelman et al. (2021), p. 59, defines:\n\nType S error occurs when the sign of the estimated effect is of the opposite direction to the true effect.\nType M error occurs when the magnitude of the estimated effect is much different from the true effect.\n\nThis classification of errors seem more relevant than the old Type I and Type II error classification which was based on null hypothesis significance testing and its dichotomization of results as either statistically significant or non-significant.\n\n“We do not generally use null hypotheses significance testing in our work. In the field in which we work, we do not generally think null hypotheses can be true: in social science and public health, just about every treatment one might consider will have some effect, and no comparison or regression coefficient of interest will be exactly zero. We do not find it particular helpful to formulate and test null hypotheses that we know ahead of time cannot be true. Testing null hypothesis is just a matter of data collection: with sufficient sample size, any hypothesis can be rejected and there is no real point in gathering a mountain of data just to reject a hypothesis that we did not believe in the first place.”\nGelman et al. (2021), p. 59.\n\n\n\nVariability versus Compatibility\nIt is important to distinguish between intervals that refer to the variability in our data (e.g., \\(\\pm 1 SD\\)) and to intervals that refer to our parameter estimates (i.e. \\(\\pm 1 SE\\)). I will call the former Variability intervals (a new term that I just invented) and the latter compatibility intervals for reasons discussed above.\n\nVariability intervals, e.g., \\(\\pm 1 SD\\), \\(IQR\\), \\(Max-Min\\), describe the variability (dispersion) of the data, the wider the interval the larger variability in the data. The size of the interval does not depend on sample size (but may vary a lot for small samples due to sampling error).\nCompatibility intervals, e.g., \\(\\pm 1 SE\\), \\(95\\%\\;CI\\), tell us about the compatibility of our estimate with possible parameter values, given our statistical model. The compatibility interval says little about the variability of the data, because the width of the interval depends strongly on sample size.\n\n\nSimple simulation of variability of scores and compatibility of mean scores in a small and a large study, sampled from the same population of test takers:\n\n\nCode\nerrbar_plot &lt;- function(y){\n  # Plot point estimate +/- 1 SD\n  plot(NA, xlim = c(0, 3), ylim = c(-2, 4), axes = FALSE, xlab = '', ylab = '')\n  arrows(x0 = 1, x1 = 1, y0 = mean(y) - sd(y), y1 = mean(y) + sd(y),\n       length = 0.01, angle = 90, code = 3)\n  points(1, mean(y), pch = 21, bg = 'grey')\n  text(x = 0.8, y = -2.0, \"+/- 1 SD\", pos = 4)\n\n  # Add point estimate with 95 % CI\n  ci &lt;- confint(lm(y ~ 1))\n  arrows(x0 = 2, x1 = 2, y0 = ci[1], y1 = ci[2],\n       length = 0.01, angle = 90, code = 3)\n  points(2, mean(y), pch = 21, bg = 'grey')\n  text(x = 1.8, y = -2.0, \"95 % CI\", pos = 4)\n  \n  # Add y-axis\n  axis(2, las = 2, pos = 0.5, tck = 0.01)\n  mtext(text = \"outcome variable\", side = 2, line = -2.5, cex = 1)\n  \n  # Add text sample size\n  stext &lt;- paste(\"Sample size =\", as.character(length(y)))\n  mtext(text = stext, side = 3, cex = 1)\n}\n\n\n\n\nCode\nset.seed(123)\n\n# Small sample study\nn &lt;- 20\ny &lt;- rnorm(n, mean = 1, sd = 2)  # Difference score in pre-post experiment\nerrbar_plot(y)\n\n\n\n\n\n\n\n\n\nCode\n# Large sample study\nn &lt;- 800\ny &lt;- rnorm(n, mean = 1, sd = 2)  # Difference score in pre-post experiment\nerrbar_plot(y)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#interpretation-of-compatibility-interval",
    "href": "06-stat25.html#interpretation-of-compatibility-interval",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "6.2 Interpretation of compatibility interval",
    "text": "6.2 Interpretation of compatibility interval\nResist dichotomous thinking!\n\n“… whether [compatibility] intervals include or exclude zero should play no role in their interpretation, because even with only random variation the intervals from different studies can easily be very different” (Amrhein et al., 2019)\n\n\n\n\nAmrhein et al. (2019)\n\n\n\nExample in Amrhein et al. (2019): Result from study on unintended effects of anti-inflammatory drugs: Risk-ratio (RR) of 1.2 [-0.03, 0.48]. This non-significant result (p = 0.091) led the authors to conclude that exposure to the drugs was not associated with the outcome (atrial fibrillation), in contrast to results from an earlier study (RR = 1.2) with a statistically significant outcome.\nCompare this to Amrhein et al’s interpretation of the result:\n\n“Like a previous study, our results suggest a 20% increase in risk of new-onset atrial fibrillation in patients given the anti-inflammatory drugs. Nonetheless, a risk difference ranging from a 3% decrease, a small negative association, to a 48% increase, a substantial positive association, is also reasonably compatible with our data, given our assumptions.” (Amrhein et al., 2019)\n\nThis is a good example of how to interpret point estimates and intervals!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#sampling-distribution-and-standard-error",
    "href": "06-stat25.html#sampling-distribution-and-standard-error",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "6.3 Sampling distribution and Standard error",
    "text": "6.3 Sampling distribution and Standard error\nA sampling distribution is the distribution of a sample statistic, for example the mean, that would be observed if we could repeat the sampling procedure infinitely many times, and for each time calculate the statistic. The standard error of the statistic is the standard deviation of its sampling distribution.\n\nStandard error of the mean\nHere simulation of random draws of samples from a normal distribution with mean = 100 and sd = 15. The figure shows the distribution of sample means for three sample sizes.\n\n\nCode\n# Create a function that does what you want to simulate one time\nmy_sample &lt;- function(n = 25) {\n  x &lt;- rnorm(n, mean = 100, sd = 15)\n  out &lt;- mean(x)\n  out\n}\n\n# Repeat my_sample() many times with the replicate() function\nn25 &lt;- replicate(n = 1e4, my_sample(n = 25))  # Replicate 100,000 times\nn100 &lt;- replicate(n = 1e4, my_sample(n = 100))\nn900 &lt;- replicate(n = 1e4, my_sample(n = 900))\n\n# Plots\npar(mfrow = c(1, 3))\nhist(n25, breaks = 200, xlim = c(85, 115), main = \"n = 25\")\nhist(n100, breaks = 200, xlim = c(85, 115), main = \"n = 100\")\nhist(n900, breaks = 200, xlim = c(85, 115), main = \"n = 900\")\n\n\n\n\n\n\n\n\n\n\nThe standard deviation of the sampling distribution of means is the standard error of the mean: \\(\\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the standard deviation in the population add \\(n\\) is the sample size.\nThe simulations were based on 10,000 random samples, so the standard deviation from the simulations would be a good approximation of the standard deviation (i.e., standard error) of the true sampling distribution (based on infinitely many random draws).\n\n\nCode\n# Standard errors (sd of sampling distributions)\nse &lt;- c(se_n25_approx = sd(n25), se_n100_approx = sd(n100), se_n900_approx = sd(n900))\nround(se, 3)\n\n\n se_n25_approx se_n100_approx se_n900_approx \n         2.951          1.495          0.499 \n\n\nCode\nse_exact &lt;- 15 / sqrt(c(25, 100, 900))  # SE = SD/sqrt(n)\nnames(se_exact) &lt;- c(\"se_n25\", \"se_n1005\", \"se_n900\")\nround(se_exact, 3)\n\n\n  se_n25 se_n1005  se_n900 \n     3.0      1.5      0.5 \n\n\n\nTypically, we don’t know \\(\\sigma\\), and therefore use the sample standard deviation to estimate the (true) standard error. In most cases when people, including myself, use the term “standard error” (SE) they refer to an estimate of the true standard error. The estimate of the standard error of the mean, \\(SE_{\\bar{X}}\\), is\n\\(SE_{\\bar{X}} = \\frac{SD(X)}{\\sqrt{n}}\\),\nwhere \\(X\\) is a sample of observations \\(x_1, x_2, ..., x_n\\), \\({\\bar{X}}\\) is the sample mean, and \\(SD(X)\\) is the sample standard deviation.\nnote: The term “standard error” may also be used with a more general meaning:\n“… in our current practice we usually summarize uncertainty using simulation, and we give the term”standard error” a looser meaning to cover any measure of uncertainty that is compatible to the posterior standard deviation”, Gelman et al. (2021), p. 51 on the variability of the posterior distribution estimated using Bayesian statistics. Gelman et al. advocate the median absolute deviation (MAD) as measure of standard error (in the lose meaning of the word) as this measure of dispersion is more robust to extreme values.\n\n\n\nSimulating a sampling distribution\nHere a simulation of a sampling distribution: Samples of 25 observations drawn independently from a normal distribution with \\(\\mu = 100\\) and \\(\\sigma = 15\\). The standard deviation of the distribution of sample means, \\(\\sigma_{\\bar x}\\) is called the standard error:\n\\(\\sigma_{\\bar x} = \\frac{\\sigma_x}{\\sqrt{n}}\\)\nSo for this simulation with \\(\\sigma = 15\\) and \\(n = 15\\), the estimated standard error from our simulation should be close to its true value:\n\\(\\sigma_{\\bar x} = \\frac{15}{\\sqrt{25}} = 3\\).\n\n\nCode\n# Create a function that for one simulation\nmy_sample &lt;- function(n = 25) {\n  x &lt;- rnorm(n, mean = 100, sd = 15)\n  out &lt;- mean(x)\n  out\n}\n\n# Repeate my_sample() many times with the replicate() function\nmy_simul &lt;- replicate(n = 1e5, my_sample())  # Replicate 100,000 times\n\n# Illustrate results and calculate some stats\nhist(my_simul, breaks = 200, main = \"Sample means, n = 25\")\n\n\n\n\n\n\n\n\n\nCode\nmy_stats &lt;- c(mean = mean(my_simul), se = sd(my_simul), \n              median = median(my_simul), mad = mad(my_simul))\nround(my_stats, 1)\n\n\n  mean     se median    mad \n   100      3    100      3 \n\n\n\nAnd here is what happens if you sample from a non-normal distribution, in this example a uniform distribution between 50 and 150:\n\\(X \\sim Unif(a = 50, b = 150)\\),\nfor which \\(E(X) = 100\\), \\(SD(X) = 100/\\sqrt{12} \\approx 28.9\\). With \\(n = 25\\), the standard error would be \\(28.9/5 \\approx 5.8\\).\n\n\nCode\n# As above, but with a uniform distribution between 50 and 150\nmy_sample &lt;- function(n = 25) {\n  x &lt;- runif(n, min = 50, max = 150)\n  out &lt;- mean(x)\n  out\n}\n\n# Plot uniform distribution\nplot(20:180, dunif(20:180, min = 50, max = 150), type = 'l', ylim = c(0, 0.015),\n     xlab = \"x\", ylab = \"probability density\")\n\n\n\n\n\n\n\n\n\nCode\n# Repeat my_sample() many times with the replicate() function\nmy_simul &lt;- replicate(n = 1e5, my_sample())  # Replicate 100,000 times\n\n# Illustrate results and calculate some stats\nhist(my_simul, breaks = 200, main = \"Sample means, n = 25\")\n\n\n\n\n\n\n\n\n\nCode\nmy_stats &lt;- c(mean = mean(my_simul), se = sd(my_simul), \n              median = median(my_simul), mad = mad(my_simul))\nround(my_stats, 1)\n\n\n  mean     se median    mad \n 100.0    5.8  100.0    5.8 \n\n\n\n\n\nConfidence intervals\nThe conventional confidence intervals around the mean are calculated using the T-distribution, with n-1 degrees of freedom, df (Gelman et al. (2021), p. 53). The T-distribution with few df has heavier tails than the normal distribution, but already at moderate sample sizes, say df = 49, the two distributions are very similar. This is the standard analytic method for calculating the confidence interval:\n\\(CI = \\bar{X} \\pm t_{\\alpha/2}SE_{\\bar{X}}\\),\nwhere \\(t_{\\alpha/2}\\) is the critical t-value for a specified \\(\\alpha\\) level. For example, if \\(\\alpha = .05\\), then the critical t-value define the two cut-off points between which the middle 95 % of the probability is located. This is roughly -2 and 2 SEs from the mean, and that is why \\(\\pm 2SE\\) often is a good approximation of a 95 % confidence interval. For example, the cutoffs would be \\(-1.96SE\\) and \\(1.96SE\\) for the normal distribution, and \\(-2.01SE\\) and \\(2.01SE\\) for a sample size of n = 50 (df = 49).\nExample: Sample mean = 101, sd = 21, sample size = 50.\n\\(95\\%CI = mean(x) \\pm t_{.05/2}SE_x =\\)\n\\(=101 \\pm 2.01*\\frac{21}{\\sqrt{50}} =\\)\n= [95, 107].\nHere is how you may use lm() in R to obtain the interval.\n\n\nCode\nset.seed(879)\n# Simulate sample with mean = 101, sd = 21, and n = 50\nm = 101  # sample mean\ns = 21  # sample sd\nn = 50  # sample size\nx &lt;- rnorm(n)  # 50 observations\nz &lt;- (x - mean(x))/sd(x)  # transform to z-scores: mean = 0, sd = 1\ny &lt;- (z*s) + m  # transform the z-scores to get mean = 101, sd = 21\n\n# Use lm() to derive 95 % confidence interval, i.e., do\n# linear regression without predictor, only the intercept = mean is estimated.\nmfit &lt;- lm(y ~ 1)\nconfint(mfit)\n\n\n               2.5 %   97.5 %\n(Intercept) 95.03187 106.9681\n\n\nNote: Please try t.test(y), it is the same thing!\n\n\n\nBootstrapping\nAnother common method for estimating the confidence intervals is to do bootstrapping. This is a very powerful and general method that may be used to derive confidence intervals when no analytic solution exists or when assumptions behind analytic solutions seem hard to accept. See Gelman et al. (2021) chapter 5, on how bootstrapping is used to simulate a sampling distribution. Here is code to do it for the data simulated in the previous section.\n\n\nCode\nbootstrap &lt;- function(y) {\n  # Random draw from the sample with replacement, same n\n  b &lt;- sample(y, size = length(y), replace = TRUE)\n  meanb &lt;- mean(b)\n  meanb\n}\n\n# Draw 10,000 bootstraps\nbb &lt;- replicate(1e4, bootstrap(y))\n\nhist(bb, breaks = seq(84.5, 115.5, by = 1), main = \"10k bootstraps\",\n     xlab = \"Mean of bootstrapped sample\")\nbootstrap_summary &lt;- c(mean = mean(bb), \n                       quantile(bb, probs = c(0.025, 0.975)))\nlines(bootstrap_summary[c(2, 3)], c(40, 40), col = \"blue\")\npoints(bootstrap_summary[1], 40, pch = 21, bg = \"blue\")\n\n\n\n\n\n\n\n\n\nCode\nround(bootstrap_summary, 1)\n\n\n mean  2.5% 97.5% \n101.0  95.0 106.7",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#likelihood-funcion",
    "href": "06-stat25.html#likelihood-funcion",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "6.4 Likelihood funcion",
    "text": "6.4 Likelihood funcion\nThe likelihood is the probability of the observed data considered as a function of unknown model parameter(s) \\(\\theta\\)\nExample: Likelihood function of the success probability \\(\\theta\\) (theta) in a binomial experiment (“coin tosses”) with \\(s = 6\\) successes in \\(n = 9\\) trials. To make it more realistic, you may think of a sample of 9 students from a specific student population, 6 of which answered correctly on a given exam question. Your goal is to use this data to estimate the proportion of correct answers had the full population been tested.\n\n\nCode\n# Function that derives the likelihood function using brute force\nbinom_likelihood &lt;- function(n, s) {\n  # Grid approximation\n  theta &lt;- seq(from = 0, to = 1, by = 1/300) # Grid of parameters\n  likelihood &lt;- dbinom(x = s, size = n, prob = theta) # Likelihood\n  ylab = 'likelihood'\n\n  # Plot likelihood function\n  plot(theta, likelihood, type = 'l', axes = FALSE, ylab = ylab)\n  axis(1, pos = 0)\n  axis(2, pos = 0, las = 2)\n}\n\n# Data\nn &lt;- 9\ns &lt;- 6\n\n# Plot likelihood\nbinom_likelihood(n = n, s = s)\nmtext(text = \"Data fixed: 6 successes in 9 trials\", side = 3, cex = 0.8)\nlines(x = c(6/9,6/9), y = c(0, dbinom(x = 6, size = 9, prob = 6/9)),\n      lty = 2, col = \"blue\")\n\n\n\n\n\n\n\n\n\nThe blue line show the maximum of the likelihood function (maximum likelihood) found at \\(\\theta = 6/9\\). So, the data is most compatible with a true proportion of 6/9. But also highly compatible with proportions around 6/9.\n\nLikelihood function and sampling distribution are different things, yet based on the same probability distribution. But the likelihood function treats data as fixed and parameters as variable, whereas sampling distributions treats the parameter as fixed and the data as variable. Here sampling distributions for \\(\\theta\\) of 1/10, 1/2, 9/10\n\n\nCode\n# Probabilities for all possible outcomes, s = 0, 1, ..., n\nn = 9\npar(mfrow = c(1, 3))\n\n# Function for ploting sampling distributions figures\nsdist &lt;- function(prob) {\n  p &lt;- dbinom(0:n, size = n, prob = prob)\n  plot(0:n, p, type = 'h', xlab = 'data', ylab = 'P(data|theta)')\n}\n\nsdist(prob = 1/10)\nmtext(text = \"Parameter fixed, theta = 1/10\", side = 3, cex = 0.7)\n\nsdist(prob = 1/2)\nmtext(text = \"Parameter fixed, theta = 1/2\", side = 3, cex = 0.7)\n\nsdist(prob = 9/10)\nmtext(text = \"Parameter fixed, theta = 9/10\", side = 3, cex = 0.7)\n\n\n\n\n\n\n\n\n\n\n\nAdvanced: Likelihood 2-parameter distribution\nIn the above example, there was only one parameter, \\(\\theta\\), representing the proportion of correct answers to an exam question had the full population of students been tested. Assume instead that we have a sample of exam scores from 16 students, and we want to estimate the mean and standard deviation had we tested the full population of students. We may treat exam scores as a continuous variable and assume that it is normally distribution,\n\\(scores \\sim N(\\mu, \\sigma)\\). Here are our 16 exam scores.\n\n\nCode\nset.seed(123)\nscore &lt;- rnorm(16, 50, 10) # Simulate exam scores from 16 students\nround(score)\n\n\n [1] 44 48 66 51 51 67 55 37 43 46 62 54 54 51 44 68\n\n\nCode\nround(c(mean = mean(score), sd = sd(score)), 2)\n\n\n mean    sd \n52.55  9.13 \n\n\n\nThe likelihood function is 2-dimensional, because we have to parameters, \\(\\mu\\) and \\(\\sigma\\). (Code only for nerds.)\n\n\nCode\n# Use grid approximation to obtain likelihood function\n\n# Define grid\nmu &lt;- seq(from = 20, to = 80, length.out = 500)\nsigma &lt;- seq(from = 2, to = 18, length.out = 500)\ngrid &lt;- expand.grid(mu = mu, sigma = sigma)\n\n# Calculate likelihood (some tricks needed here using log = TRUE)\ngrid$loglike &lt;- sapply(1:nrow(grid), function(i) sum(dnorm(\n                                       score, \n                                       mean = grid$mu[i],\n                                       sd = grid$sigma[i],\n                                       log = TRUE)))\n# Normalize to max = 1, and antilog\ngrid$likelihood &lt;- exp((grid$loglike) - max(grid$loglike))\n\n# Plot likelihood function, using contour_xyz from library rethinking\nrethinking::contour_xyz(grid$mu, grid$sigma, grid$likelihood, \n            xlim = c(45, 60), ylim = c(5, 15), xlab = 'mu', ylab = 'sigma')\nmtext(text = \"Data fixed: n = 16, mean = 52.5, sd = 9.1\", side = 3, cex = 0.9)\n\n\n\n\n\n\n\n\n\nThis contour plot shows the two-dimensional likelihood function. The maximum likelihood estimate is the mode (highest point) of this function.\n\n\nCredible intervals\nWhereas the previously discussed methods for deriving confidence intervals rely heavily on sampling distributions, Bayesian methods (covered below) use the likelihood function to estimate the corresponding interval, often referred to as a credible interval.\nThis code shows how to derive a credible interval in R using rstanarm, using the same simulated data as above, i.e., sample mean = 101, sd = 21, and n = 50.\n\n\nCode\n# Use library(rstanarm) and function stan_glm() to derive 95 % credible interval. \n# Variable y was created in the previous code block. \n\ny &lt;- data.frame(y)  # stan_glm wants data in a data frame\nsfit &lt;- stan_glm(y ~ 1, data = y, seed = 123, refresh = 0)\nprint(sfit)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n observations: 50\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 101.0    3.0 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 21.2    2.1  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nEstimated mean = 101 with a standard error (in the lose meaning of the term) of 3.0. So a rough 95 % interval would be \\(101 \\pm 2*3\\), i.e. \\([95, 107]\\). We can also ask stan_glm for the 95 % interval of the posterior distribution:\n\n\nCode\nposterior_interval(sfit, prob = 0.95)\n\n\n                2.5%     97.5%\n(Intercept) 94.90063 107.33378\nsigma       17.54435  26.45381\n\n\n\n95 % interval around the mean: \\([94.9, 107.3]\\)\nNote that stan_glm also provide a confidence interval for the standard deviation (sigma): \\([17.5, 26.5]\\), point estimate = \\(21.2\\). Note also that this interval is not symmetrical around the point estimate, because the posterior distribution of sigma is not symmetrical. This is an example of where the simple rule \\(\\pm 2 SE\\), may be misleading.\nPosterior distribution sigma estimated using stan_glm()\n\n\nCode\n# Extract samples from the posterior\nsamples &lt;- as.data.frame(sfit)\nsigma &lt;- samples$sigma\n\n# Plot density curve for sigma\nplot(density(sigma), xlab = \"Sigma\", \n     main = \"Posterior probability: sigma, with 95 and 50 % CI\")\n\n# Derive point estimate and compatibility interval\npoint_estimate &lt;- median(sigma)\nci95 &lt;- quantile(sigma, prob = c(0.025, 0.975))\nci50 &lt;- quantile(sigma, prob = c(0.25, 0.75))\n\n# Plot point estimate and compatibility interval\narrows(x0 = ci95[1], x1 = ci95[2], y0 = 0.01, y1 = 0.01, \n       length = 0, col = \"blue\")\narrows(x0 = ci50[1], x1 = ci50[2], y0 = 0.01, y1 = 0.01, \n       length = 0, col = \"darkblue\", lwd = 3)\npoints(point_estimate, 0.01, pch = 21, bg = 'lightblue')  # Add point\"\n\n\n\n\n\n\n\n\n\nHere I also added a 50 % CI (thick line) to the 95 % interval (thin line), to emphasize that the values closer to the point estimate are more compatible with the data than those close to the end of the 95 % interval (as always: given our statistical model). See Gelman et al. (2021) Fig. 19.4, p.367 for example of how they use combined 50 and 95 % interval.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#probability-3-bayesian-inference",
    "href": "06-stat25.html#probability-3-bayesian-inference",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "6.5 Probability 3: Bayesian inference",
    "text": "6.5 Probability 3: Bayesian inference\nThe general goal of Bayesian inference is to update prior beliefs (hypotheses) in light of data to arrive at posterior beliefs (updated hypotheses). The key idea is to think about uncertainty as probabilities and use probability theory to be consistent in the way beliefs in hypotheses are updated.\n\n\nBayes rule for estimating a discrete parameter\nRemember the twin problem:\n\n\n\nA women is pregnant with twins, and her doctor tells her that about one third of twin births are identical and the remaining two thirds are fraternal. On the next visit to the doctor, the sonogram shows that she is pregnant with twin boys. Her doctor tells her that about half identical twins are twin boys and the other half are twin girls, whereas for fraternal twins about one quarter are twin boys, one quarter are twin girls, and the rest are one of each sex. How should she update her belief in identical versus fraternal twins given this information? (old Stat1 exam question)\n\n\n\n\nHere the information in frequency format (think of 600 twin births):\n\n\n\n\nBB\nMix\nGG\nMarginal\n\n\n\n\nIdentical\n100\n0\n100\n200\n\n\nFraternal\n100\n200\n100\n400\n\n\nMarginal\n200\n200\n200\n600\n\n\n\n\nSolve using Bayes rule:\n\n\\(H_0\\): Fraternal twins\n\\(H_1\\): Identical twins\n\\(D\\): Twin boys\n\\(Pr(H_0) = 2/3\\), Prior probability fraternal\n\\(Pr(H_1) = 1/3\\), Prior probability identical\n\\(Pr(D|H_0) = 1/4\\), Likelihood fraternal\n\\(Pr(D|H_§) = 1/2\\), Likelihood identical\n\\(Pr(D) = 1/3\\), Probability of data\nBayes’ rule: \\(Pr(H_i|D) = \\frac{Pr(H_i)Pr(D|H_i)}{Pr(D)}\\)\n\\(Pr(H_0|D) = (2/3 * 1/4)/(1/3) = 1/2\\), Posterior probability fraternal\n\\(Pr(H_1|D) = (1/3 * 1/2)/(1/3) = 1/2\\), Posterior probability identical\n\n\nSolve with relative version of Bayes rule:\n\nBayes’ rule: \\(Pr(H_i|D) \\propto Pr(H_i)Pr(D|H_i)\\)\n\\(Pr(H_0|D) \\propto (2/3 * 1/4) = 1/6\\), Relative posterior fraternal\n\\(Pr(H_1|D) \\propto (1/3 * 1/2) = 1/6\\), Relative posterior identical\n\nSo we can see that the posterior probability is equal for the two alternatives, although we know that the absolute posterior probability is not 1/6.\nHere is a graphical illustration of the result:\n\n\nCode\n# Plot posterior, prior, likelihood\npar(mfrow = c(1, 3))\n\nprior &lt;- c(2/3, 1/3)\nlikelihood &lt;- c(1/4, 1/2)\nrelposterior &lt;- c(1/6, 1/6)\n\n# Make plot\nmybar &lt;- function(x, main, ylab) {\n  plot(NULL, xlim = c(-0.3, 1.3), ylim = c(0, 1), axes = FALSE, \n     xlab = \"Parameter\", ylab = ylab, main = main)\n  points(c(0, 1), x, type = \"h\", lwd = 2)\n  axis(1, at = c(-0.3, 0, 1, 1.2), labels = c(\"\", \"Fraternal\", \"Identical\", \"\"),\n       cex.axis = 0.8, pos = 0)\n  axis(2, las = 2, pos = -0.3)\n}\n\n# Draw plot\nmybar(relposterior, main = \"Relative posterior\", ylab = \"Relative posterior probaility\")\nmybar(prior, main = \"Prior\", ylab = \"Prior probaility\")\nmybar(likelihood, main = \"Likelihood\", ylab = \"Likelihood\")\n\n\n\n\n\n\n\n\n\nNote: The shape of the posterior is all we need, the absolute heights of the bars in the left-hand panel is not used for inference, only their relative height. In this case, the two bars have the same height, meaning that the two outcomes have the same posterior probability.\nIn this example, it was fairly easy to derive the absolute posterior because we could calculate \\(Pr(D)\\). But for most problems, \\(Pr(D)\\) is very hard or impossible to calculate, and that is why Bayesian inference typically only uses the relative version of Bayes’ rule.\n\n\n\nBayes rule for estimating a continous parameter\nIn the above example, we updated our beliefs in two discrete outcomes. More often we want to update our belief in a continuous parameter. For example, the probability of success in a binomial experiment with a known number of trials.\nHere is a simple example: A logical problem that can be scored as either correct or incorrect. We have given the test to nine individuals, six of which answered correctly. We want to use this data to estimate the proportion of individuals who would answer correctly in the population from which the individuals were drawn. We will model the problem using the Binomial distribution, with parameters n = 9 (known), and p (unknown, to be estimated): \\(y \\sim Binom(n = 9, p)\\)\nBayes rule: posterior \\(\\propto\\) prior \\(\\times\\) likelihood\nWe will first work with a “flat” prior, essentially saying that before we saw the data we would consider every possible value of \\(p\\) as equally likely.\n\n\nCode\n# Data (fixed)\nn &lt;- 9\ns &lt;- 6\n\n# Function to draw three plots\nbayes_plot &lt;- function(pgrid, prior) {\n  # Likelihood\n  likelihood &lt;- dbinom(x = s, size = n, prob = pgrid)\n\n  # Relative posterior\n  relative_posterior &lt;- prior * likelihood \n\n  # Plot posterior, prior, likelihood\n  par(mfrow = c(1, 3))\n  plot(pgrid, relative_posterior, type = 'l', \n     xlab = \"Parameter, p\", ylab = \"Relative posterior probability\",\n     main = \"Reltive posterior\")\n  plot(pgrid, prior, type = 'l', xlab = \"Parameter value, p\", ylab = \"Prior probability density\",\n       main = \"Prior\", xlim = c(-0.01, 1.01))\n  plot(pgrid, likelihood, type = 'l', xlab = \"Parameter value, p\", ylab = \"Likelihood\",\n       main = \"Likelihood\")\n}\n\npgrid &lt;- seq(from = 0, to = 1, by = 0.01) # Parameters grid\nflat &lt;- rep(1, length(pgrid))# Prior (flat)\nbayes_plot(pgrid = pgrid, prior = flat)\n\n\n\n\n\n\n\n\n\n\nHere with a weakly informed prior: true proportion probably around 0.5, but values less or greater than this are also plausible, but we know that the true proportion is not zero and not one (maybe because we know of at least one individual in the population who can solve the problem, and at least one who could not).\n\n\nCode\n# Data (fixed)\nn &lt;- 9\ns &lt;- 6\n\n# Prior \nbeta22 &lt;- dbeta(pgrid, shape1 = 2 , shape2 = 2)\nbayes_plot(pgrid = pgrid, prior = beta22) # pgrid defined above\n\n\n\n\n\n\n\n\n\nHere with a stronger prior: True proportion probably around 0.2 (maybe based on previous test results), but values less or greater than this are also plausible.\n\n\nCode\n# Data (fixed)\nn &lt;- 9\ns &lt;- 6\n\n# Prior \nbeta28 &lt;- dbeta(pgrid, shape1 = 2 , shape2 = 8)\nbayes_plot(pgrid = pgrid, prior = beta28) # pgrid defined above\n\n\n\n\n\n\n\n\n\n\nHere another informed prior: Maybe the test was a multiple-choice question with two alternatives, so proportion correct could not be worse than 50 % correct (assuming that everyone did their best), but we consider every value of \\(p\\) between 0.5 and 1 as equally likely.\n\n\nCode\n# Data (fixed)\nn &lt;- 9\ns &lt;- 6\n\n# Prior \nlim50 &lt;- ifelse(pgrid &lt; 0.5, 0, 2)  # pgrid defined above\nbayes_plot(pgrid = pgrid, prior = lim50)\n\n\n\n\n\n\n\n\n\n\n\nAdvanced: Sampling from the posterior\nThe relative posterior distribution may be hard or impossible to estimate analytically. However, it may be estimated using clever algorithms that draws a large number of random samples from the posterior and then uses these samples to estimate properties of the relative posterior, such as its median, median absolute deviation, and 95 % compatibility interval. stan_glm() uses an algorithm called Hamiltonian Monte Carlo. The code below show how to extract samples and use them to derive statistics of the posterior. The example uses the same simulated data as above, 16 test scores modeled as independent random draws from a normal distribution with unknown mean and standard deviation.\nTest scores, n = 16.\n\n\nCode\nset.seed(123)\nscore &lt;- rnorm(16, 50, 10) # Simulate exam scores from 16 students\nround(score)\n\n\n [1] 44 48 66 51 51 67 55 37 43 46 62 54 54 51 44 68\n\n\nCode\nround(c(mean = mean(score), sd = sd(score)), 2)\n\n\n mean    sd \n52.55  9.13 \n\n\n\nFit model:\n\\(y_i \\sim Normal(\\mu, \\sigma)\\)\n\\(y_i = b_0\\)\nusing stan_glm(), and print output:\n\n\nCode\ndd &lt;- data.frame(score)  # stan_glm() likes data in a data frame\nmfit &lt;- stan_glm(score ~ 1, data = dd, refresh = 0, seed = 123, iter = 1e4)\n\n\nFound more than one class \"stanfit\" in cache; using the first, from namespace 'rstan'\n\n\nAlso defined by 'rethinking'\n\n\nFound more than one class \"stanfit\" in cache; using the first, from namespace 'rstan'\n\n\nAlso defined by 'rethinking'\n\n\nCode\nprint(mfit)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      score ~ 1\n observations: 16\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 52.5    2.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 9.3    1.6   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nExtract samples, plot them, and do some stats to estimate properties of the posterior.\n\n\nCode\n# Save samples in data frame\nsamples &lt;- as.data.frame(mfit)\n# Rename columns, so \"(intercept)\"\"is called b0 (not needed)\nnames(samples) &lt;- c(\"b0\", \"sigma\")  # Rename columns, not needed\nhead(samples)  # Show first rows of data frame\n\n\n        b0     sigma\n1 52.32408 10.830253\n2 48.21844  8.267484\n3 54.49953 13.032090\n4 55.00462 13.487413\n5 53.30425 14.204507\n6 52.14543 13.041902\n\n\nCode\n# Plot samples, using transparent colors. stan_glm() point estimates: white circle\nplot(samples$b0, samples$sigma, pch = 19, col = rgb(0, 0, 1, 0.1),\n     xlim = c(45, 60), ylim = c(5, 15))\npoints(median(samples$b0), median(samples$sigma), pch = 21, bg = \"white\", cex = 1)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 2))\n# plot distribution for b0\nplot(density(samples$b0), main = \"Estimated posterior b0\", xlab = \"b0\",\n     ylab = \"Posterior probability\")\n# plot distribution for sigma\nplot(density(samples$sigma), main = \"Estimated posterior sigma\", xlab = \"sigma\",\n     ylab = \"Posterior probability\")\n\n\n\n\n\n\n\n\n\n\nCalculate median and median absolute deviation (mad) for b0 and sigma, compare to stan_glm() output, and derive 95 % compatibility intervals for the parameters. Note: “(Intercept)” is parameter b0.\n\n\nCode\n# Median and MAD\nmdn_b0 &lt;- median(samples$b0)\nmad_b0 &lt;- mad(samples$b0)\nmdn_sigma &lt;- median(samples$sigma)\nmad_sigma &lt;- mad(samples$sigma)\nstats &lt;- c(mdn_b0 = mdn_b0, mad_b0 = mad_b0, \n           mdn_sigma = mdn_sigma, mad_sigma = mad_sigma)\nround(stats, 1)\n\n\n   mdn_b0    mad_b0 mdn_sigma mad_sigma \n     52.5       2.3       9.3       1.6 \n\n\nCode\n# Compare with stan_glm() output\nprint(mfit)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      score ~ 1\n observations: 16\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 52.5    2.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 9.3    1.6   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nCode\n# Compatibility intervals, 95 %\nb0_ci &lt;- quantile(samples$b0, prob = c(0.025, 0.975))\nsigma_ci &lt;- quantile(samples$sigma, prob = c(0.025, 0.975))\nround(c(b0_ci = b0_ci, sigma_ci = sigma_ci), 1)\n\n\n    b0_ci.2.5%    b0_ci.97.5%  sigma_ci.2.5% sigma_ci.97.5% \n          47.7           57.3            6.8           13.8",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#practice",
    "href": "06-stat25.html#practice",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\n\nEasy\n\n6E1.\n\nGiven the information in this table, would you say that previous research is in agreement or disagreement with regard to the effect of physical activity on working memory capacity? Motivate your answer.\n(This is an old exam question)\n\n\n\n6E2. Meta-analysis is widely consider the best method for accumulation of scientific knowledge. If a meta-analysis was conducted on the studies in 6E1, what use would it have of the p-values in the fourth column of the table above?\n\n\n\n6E3. I simulated data from a randomized experiment with a control group and a treatment group (n = 30 per group). The conditions were nature sound (treatment) versus no sound exposure (control) during a test of directed attention. The test has been used in previous research and is known to have a mean around 50 with a standard deviation around 10; a difference greater than 5 units on this scale is generally considered a substantial difference, whereas a difference of less than 1 unit is generally considered negligible. The between-group difference was 5 units in favor of the treatment group with 95 % confidence interval of [-1, 12].\nIn a single sentence, summarize the result for the treatment effect, with reference to the point estimate and the compatibility interval. Phrase your sentence in line with the recommendations of Amrhein et al. (2019) , and with reference to the general understanding of the outcome measure described above.\n\n\n\n6E4. The estimated treatment effect in 6E3 was not considered statistically significant (p &gt; .05). However, it would be a mistake to conclude from this that the data supported no effect of treatment. Explain.\n\n\n\n6E5. Which of the following tends to decrease as sample size increases: standard deviation, standard error, median absolute deviation, 95% confidence interval, interquartile range?\n\n\n\nMedium\n\n6M1. The code below simulates data from a randomized experiment with one control group and one experimental group. Run the code to obtain the data, then:\n\nCalculate a 50 % and 90 % compatibility interval around each group mean. Feel free to use whatever method you like in R to calculate point estimates and intervals (t.test(), lm(), glm(), stan_glm(), …)\nVisualize point estimates and intervals (50 and 90 %) for the two groups in a single graph (cf. Gelman et al. (2021), Fig. 4.2).\nCalculate a 50 % and 90 % compatibility interval around the between-group difference.\nVisualize point estimate and intervals (50 and 90 %) for he between-group difference in a single graph.\n\nR.code:\nset.seed(987)\ncontrol   &lt;- rnorm(30, mean = 20, sd = 2)\ntreatment &lt;- rnorm(30, mean = 22, sd = 2)\n\n\n\n6M2. In your own words: What is the difference between a sampling distribution and a likelihood function?\n\n\n\n6M3. Below is the binomial probability distribution for number of successes in 7 trials, \\(X \\sim Binom(n = 7, p = 0.3)\\). Does this remind you about a sampling distribution or a likelihood function. Motivate.\n\n\n\nCode\nplot(0:7, dbinom(0:7, size = 7, prob = 0.3), type = \"h\", xlab = \"x = Number of successes\",\n     ylab = \"Pr(X = x)\")\n\n\n\n\n\n\n\n\n\n\n\n6M4. Look at the figure in 6M3, and estimate by eye:\n\nPr(X = 3)\nPr(X &lt; 3)\nPr(X &gt; 3)\nPr(1 &lt; X &lt; 4)\n\n\n\n\nHard\n\n6H1. Run the simulation and stan_glm() analysis from above, section “Advanced: Sampling from the posterior”, and calculate a 50 % and 90 % compatibility interval.\n\n\n\n6H2.\n(a) Search the internet to find the difference between “percentile interval” (PI) and “highest density interval” (HDI). Explain the difference in your own words. (b) Repeat 6H1, but now calculate a HDI. (The R package HDInterval may be useful.)\n\n\n\n6H3.\n\nExplain, in your own words, step by step, how you would derive a 90 % confidence interval for a Pearson correlation coefficient using bootstrapping.\nSuggest R-code for each step, and evaluate using simulated data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "06-stat25.html#session-info",
    "href": "06-stat25.html#session-info",
    "title": "6  S: Point Estimates and Their Uncertainty",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] rstanarm_2.32.1 Rcpp_1.0.14    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     dplyr_1.1.4          farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] shinystan_2.6.0      promises_1.3.3       shinyjs_2.1.0       \n[10] digest_0.6.37        mime_0.13            lifecycle_1.0.4     \n[13] StanHeaders_2.32.10  processx_3.8.6       survival_3.7-0      \n[16] magrittr_2.0.3       posterior_1.6.1      compiler_4.4.2      \n[19] rlang_1.1.6          tools_4.4.2          igraph_2.1.4        \n[22] yaml_2.3.10          knitr_1.50           htmlwidgets_1.6.4   \n[25] pkgbuild_1.4.8       curl_6.4.0           cmdstanr_0.9.0.9000 \n[28] plyr_1.8.9           RColorBrewer_1.1-3   dygraphs_1.1.1.6    \n[31] abind_1.4-8          miniUI_0.1.2         grid_4.4.2          \n[34] stats4_4.4.2         xts_0.14.1           xtable_1.8-4        \n[37] inline_0.3.21        ggplot2_3.5.2        rethinking_2.42     \n[40] scales_1.4.0         gtools_3.9.5         MASS_7.3-61         \n[43] mvtnorm_1.3-3        cli_3.6.5            rmarkdown_2.29      \n[46] reformulas_0.4.1     generics_0.1.4       RcppParallel_5.1.10 \n[49] rstudioapi_0.17.1    reshape2_1.4.4       minqa_1.2.8         \n[52] rstan_2.32.7         stringr_1.5.1        shinythemes_1.2.0   \n[55] splines_4.4.2        bayesplot_1.13.0     parallel_4.4.2      \n[58] matrixStats_1.5.0    base64enc_0.1-3      vctrs_0.6.5         \n[61] V8_6.0.4             boot_1.3-31          Matrix_1.7-1        \n[64] jsonlite_2.0.0       crosstalk_1.2.1      glue_1.8.0          \n[67] nloptr_2.2.1         ps_1.9.1             codetools_0.2-20    \n[70] distributional_0.5.0 DT_0.33              shape_1.4.6.1       \n[73] stringi_1.8.7        gtable_0.3.6         later_1.4.2         \n[76] QuickJSR_1.8.0       lme4_1.1-37          tibble_3.3.0        \n[79] colourpicker_1.3.0   pillar_1.10.2        htmltools_0.5.8.1   \n[82] R6_2.6.1             Rdpack_2.6.4         evaluate_1.0.3      \n[85] shiny_1.11.1         lattice_0.22-6       markdown_2.0        \n[88] rbibutils_2.3        backports_1.5.0      threejs_0.3.4       \n[91] httpuv_1.6.16        rstantools_2.4.0     coda_0.19-4.1       \n[94] gridExtra_2.3        nlme_3.1-166         checkmate_2.3.2     \n[97] xfun_0.52            zoo_1.8-14           pkgconfig_2.0.3     \n\n\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Retire statisticial significance. Nature, 567, 305–307.\n\n\nEriksson, C., Hilding, A., Pyko, A., Bluhm, G., Pershagen, G., & Östenson, C.-G. (2014). Long-term aircraft noise exposure and body mass index, waist circumference, and type 2 diabetes: A prospective study. Environmental Health Perspectives, 122(7), 687–694.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nVan Hedger, S. C., Nusbaum, H. C., Clohisy, L., Jaeggi, S. M., Buschkuehl, M., & Berman, M. G. (2019). Of cricket chirps and car horns: The effect of nature sounds on cognitive performance. Psychonomic Bulletin & Review, 26(2), 522–530.\n\n\nWilkinson, L. (1999). APA task force on statistical inference. Statistical methods in psychology journals: Guidelines and explanations. American Psychologist, 54(8), 594–604.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>S: Point Estimates and Their Uncertainty</span>"
    ]
  },
  {
    "objectID": "07-method25.html",
    "href": "07-method25.html",
    "title": "7  M: Within-participant Experiments",
    "section": "",
    "text": "Topics\nWithin-subject designs are experimental designs where all participant take part in every condition of the experiment. These designs are therefor also called Repeated-measures designs.\nOne may distinguish between two reason for such designs:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "07-method25.html#topics",
    "href": "07-method25.html#topics",
    "title": "7  M: Within-participant Experiments",
    "section": "",
    "text": "Two categories of within-participant designs:\n\nWithin-participant Experiments: These designs aim to estimate average causal effects by using within-participant measurements. By comparing participants against themselves, individual differences in baseline performance are controlled, thereby increasing statistical power.\nSingle-N Designs: These designs focus on identifying causal effects within a single unit or participant, targeting the causal relationship at the individual level rather than averaging across a group.\n\nThreats to the validity of within-subject designs\n\nSequencing effects:\n\nOrder effects\nCarry-over effects\n\nMaturation effects\nHistory effects\n\nDesign elements, examples:\n\nTreatment orders counterbalanced (e.g., Latin square design)\nTreatment orders random within participants across sessions\nABAB designs, where treatment and control conditions are alternated (“light-switch design”).\nTraining of participants to minimize learning effects, experiment starts when participant performance has reached an asymptote.\nMany pauses between testing session, testing on several days\nFiller items to limit carry over effects (and mask purpose)\n\nIndividual level versus group level analysis\nDesigns for assessing single-unit causal effects\n\nRepeated measures of single unit(s).\nMeasures of equivalent units: “Identical twin” design\n\nCross-level bias. Examples:\n\n“No effect on group level imply no effects at the individual level”. Maybe\nsingle-unit causal effects in different directions cancel out\n“Small effect on group level imply small effects at the individual level”. Maybe large effect in some individuals, and no effect in most individuals.\n“Equal effect of two treatments at group level imply that it doesn’t matter what treatment I take”. Could still be that some treatments work better for some individuals and other treatments work better for other individuals.\n“A positive relationship at the group level imply positive relationships at the individual level”. No, it may even be the fact that all single-unit relationships are negative, whereas the group level relationship is positive, or vice versa (Simpson’s paradox)\n“Gradual slow increase in group performance with training imply that individuals learn slowly”. No, maybe individuals learn suddenly (“I go it”!), not gradually, but some need more time than others to get it.\n\n\nTheoretical articles to read:\n\nSmith & Little (2018) as an argument for targeting single-unit casual effects.\nChristensen et al. (2023), sections on within-participant and mixed designs\n\nOther sources: - Gelman et al. (2021), chapter 18, briefly discuss issues related to within subject designs, see section “Close substitutes”, p. 341.\n\n\n\n\n\nTo efficiently estimate average causal effects by removing variability due to individual differences, for instance differences in baseline performance. It is common that these experiments involve several (+10) participants, each of which is tested one or a few times per condition. Main results are presented as group average, although individual data may also be presented.\nTo estimate single-unit casual effects by extensive measurement in one (or a few) individuals. Below, I will call such experiments Single-N designs, but they are also known as Small-N designs, or Case-study designs. Extensive measurements, maybe several hundreds per condition, assure reliable measures at the individual level. Results are presented at the individual level, even if more than one participant is tested.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "07-method25.html#within-subject-experiments-targeting-average-causal-effects",
    "href": "07-method25.html#within-subject-experiments-targeting-average-causal-effects",
    "title": "7  M: Within-participant Experiments",
    "section": "7.1 Within-subject experiments targeting average causal effects",
    "text": "7.1 Within-subject experiments targeting average causal effects\nWithin-participant designs are a cost-effective method for estimating average causal effects because they allow researchers to separate out variation in overall performance during statistical analyses. This contrasts with between-participant experiments, where analyses must be conducted on averages from separate groups of individuals. Therefor, within-participant experiments typicality involve fewer participants than in between-participant designs.\n\nMixed within-between participant designs\nA mixed design incorporates both within-participant and between-participant variables, often structured as a factorial design where all possible combinations of the variable levels are included.\nFor instance, consider a study comparing two treatments, A and B, using a within-participant design. The order of administering these treatments varies across two groups if participants: one group receives treatment A followed by B, whereas the other group receives them in the reverse order (a setup often referred to as a cross-over design). This variation in treatment order across participants illustrates the between-participant element in the mixed design.\nIn this example, the between-participant variable is not the primary focus; it is included mainly to assess potential effects of treatment orders. The researcher typically hopes there will be no substantial order effects, allowing the between-participant variable to be safely disregarded. There are of course mixed designs where the between-participant variable is the main focus, and these will be discussed later when we explore between-participant experiments.\n\n\nIndiviual variation or random error or both?\nThe figure below is a nice example of a profile plot, illustrating the performance of a single individual tested across three conditions. This study aimed to compare groups of individuals based on their self-reported inner speech. The group averages reveal a clear trend: reaction times were faster in the ‘Orthographic rhyme’ condition compared to the other two conditions. However, was this trend consistent at the individual level? It’s difficult to say. The variability in individual profiles could be attributed to measurement errors, or it may reflect substantial individual differences. The profiles show average reaction times over 20 trials for each condition. This is far too few to obtain reliable reaction time estimates at the individual level, as typically, several hundred trials would be required.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "07-method25.html#single-n-design",
    "href": "07-method25.html#single-n-design",
    "title": "7  M: Within-participant Experiments",
    "section": "7.2 Single-N design",
    "text": "7.2 Single-N design\n\nOne basic fact of all psychological research is that subjects differ greatly in their abilities to perform almost any task (on almost any dependent variable).\n From a textbook on experimental psychology, (Kantowitz et al. (2014).\n\n\n\nIt is more useful to study one animal for 1000 hours than to study 1000 animals for one hour.\nB.F. Skinner, cited in Smith & Little (2018).\n\n\nA single-N design is a study aimed at estimating causal effects in a single individual. Although multiple participants are typically tested, the focus is always on presenting data at the individual level. Notably, Smith & Little (2018) refers to these as small-N designs, describing them as studies where ‘a large number of observations are made on a relatively small number of experimental participants’ (p. 2084). I prefer the term single-N design because ‘a relatively small number’ can often be just one.\nIn psychology, single-N studies are commonly used in fields such as psychophysics and behavioral intervention research, where they are often referred to as Single-Case Experimental Designs (SCED), see examples below.\n\nExample: Psychophysical experiment\nThis is data from a psychoacoustic experiment involving 13 listeners. The details are less important, but focus on comparing thresholds (y-axis, low values are better) for the tasks involving detection of sounds (filled circles) versus localization of sounds (open squares). Some listeners had similar localization as detection thresholds (e.g., IDs 4, 6, and, 7), suggesting that if they could detect the sound they could also localize it. For others, this was not the case, most notably for the listeners in the upper row of panels. For details, see Tirado et al. (2021).\n\n\n\nFig. 2 in Tirado et al. (2021)\n\n\n\nExample: Single-case Experimental Design (SCED)\nThis is data from an educational experiment involving measurement of four children’s disruptive behaviors in class (y-axis) over time (x-axis), while alternating between conditions A (SSR) and B (RC), in a ABAB-design. Details are not important, but it is clear that method B led to a decrease in disruptive behaviors compared to method A for all four children (each row refer to data from one child). For details, see Lambert et al. (2006).\n\n\n\nFig. 1 in Lambert et al. (2006)\n\n\n\nHere’s another SCED, this one using an ABA design. The figure serves as an excellent example of a design illustration, particularly highlighting data for manipulation checks.\n\n\n\nFig. 1 in Newbold et al. (2020)\n\n\n\n\nSingle-N design and still the single-unit causal effect is unobserved\nA single-unit is defined as a specific study-unit at a specific moment in time. For example, consider a participant, \\(P\\), in a psychophysical experiment at the moment \\(i\\), exposed to a stimulus \\(s_i\\). When the next stimulus, \\(s_{i+1}\\), is presented, technically, this constitutes a new study unit because time has passed and \\(P\\) is slightly older and have gained new experiences, specifically those related to exposure to \\(s_i\\). Consequently, the single-unit causal effect is always unobserved.\nHowever, in many cases, we may have strong reasons to assume that the two single units are sufficiently similar to be treated as the same unit measured twice. If this assumption holds, we can reasonably interpret the difference between \\(response(s_{i+1})\\) and \\(response(s_{i})\\) as an estimate of the causal effect. It is crucial, though, to justify the assumptions that allow us to treat repeated measures on a single individual as a proxy for the unobservable repeated testing of the same unit.\nStrategies for making such assumptions plausible include repeated measures on a single study unit (Single-N designs) and measures on equivalent units, which could be referred to as an “Identical-twin” design, assuming homogeneity of effects across these equivalent units.\nAs noted by Holland (1986), who coined the phrase The fundamental problem of causal inference, Single-N designs and Identical-twin designs are considered scientific solutions to this fundamental problem. In contrast, between-subject designs, such as randomized experiments, are referred to as statistical solutions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "07-method25.html#threats-and-tricks",
    "href": "07-method25.html#threats-and-tricks",
    "title": "7  M: Within-participant Experiments",
    "section": "7.3 Threats and tricks",
    "text": "7.3 Threats and tricks\nThree main categories of threats to the internal validity of within-participant experiments (also known as repeated measures design) are:\n\nSequencing effects can be categorized into two types (Christensen et al. (2023)):\n\nOrder effects. Factors like fatigue or boredom may affect performance in a within-participant experiment. For instance, responses to the final stimulus in a session may be more influenced by fatigue than responses to the first stimulus in a testing session.\nCarry-over effects. The influence of a previous stimulus may affect responses in subsequent conditions. For example, in a listening experiment, exposure to a loud sound might cause a following sound to be perceived as quieter than it would have been if it had been preceded by a softer sound.\n\nMaturation. The natural changes that occur in participants over time, independent of the experimental conditions. These changes can include physical, psychological, or cognitive developments that may confound the interpretation of experimental results.\nHistory. External events that occur between the measurements of different § conditions and that can influence participants’ responses. These events are unrelated to the experiment but can introduce confounding variables that affect the outcomes.\n\nAnd here a few examples of design elements to counteract these threats:\n\nCounterbalanced stimulus (or treatment) orders across participants.\n\nRandomized counterbalanzing. Unique order for each participant, hoping that order and carry-over effects average out across participants.\nComplete counterbalancing. Every possible order used.\nIncomplete counterbalancing, for example using a balanced Latin square design.\n\nRandom stimuli orders within participants across sessions. Different random orders may be used for each participnat (if focus is on average causal effects) or the same orders may be used for every participant (if single-unit causal effects\nare evaluated).\nBaseline condition to express each individual’s performance relative to their own performance on a baseline (reference) condition.\nIndividualized stimuli. To define stimuli at the individual level, so all participants have the same performance at specified reference conditions.\nPractice of participants to minimize learning effects, start experiment after many training trials, when the individual has reached his or her limit and no longer improves with repeated testing.\nMany pauses between testing session, and consider testing on several days, to minimize effects of fatigue and boredom. May also provide protection against short lived carry over effects (pauses providing a wash-out period)\nFiller items to limit carry over effects (and mask purpose)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "07-method25.html#are-group-level-analysis-meaningful",
    "href": "07-method25.html#are-group-level-analysis-meaningful",
    "title": "7  M: Within-participant Experiments",
    "section": "7.4 Are group-level analysis meaningful?",
    "text": "7.4 Are group-level analysis meaningful?\nMany psychology studies focus on truly intra-individual phenomena, such as an individual’s ability to perceive, remember, or attend to something. Perception, memory, and attention exist at the individual level – the group has no perception, memories, or attention span (at least not in the same way as individual’s have)\nBefore calculating group averages or conducting statistical analyses that imply a group average (e.g., regression analysis), it’s crucial to ask yourself:\nAre group-level analyses meaningful for my data?\nIn many cases, especially in psychology, the answer may be no. However, there are important exceptions where group-level analysis is appropriate and meaningful:\n\nThe Group Level Is the Focus: Sometimes, the primary interest is in understanding the effect of an intervention or phenomenon at the population level rather than the individual level. For instance, if you want to estimate the societal benefit of promoting a certain diet, you might express the outcome as the average weight loss in the target population. While some individuals may lose weight and others may gain, the overall effect is what matters for policy decisions. Similarly, if estimating the risk associated with an exposure, the focus is on relative risk across groups (e.g., exposed vs. unexposed individuals), which is inherently a group-level analysis. Risk, for instance, is calculated as the mean value of an indicator variable coded as 1 for diseased and 0 for healthy within a specified population.\nThe Group Average Represents a Typical Individual: If the group average accurately reflects the experience of a typical individual, then a group-level analysis may be meaningful. In such cases, interpreting the results at the individual level could be valid, assuming that individual differences are small or irrelevant.\nMinimal or Negligible Individual Differences:If variability among individuals is so small that it can be considered mere measurement error, which cancels out when averaged, then a group-level analysis is justified. This scenario assumes that all individuals react similarly to the treatment or condition. If this assumption holds true, in principle, you could obtain reliable results from testing just one person.\n\nIn summary, while psychology often focuses on individual-level phenomena, there are circumstances where group-level analyses are both meaningful and necessary. Always critically evaluate whether the assumptions underlying your analysis are appropriate for your data.\nRemember that group level analysis can not answer questions about single-unit causal effects:\n\nNo effect at group level, consistent with at least two incompatible hypothesis:\n\nNo or negligible single-unit causal effects, or\nSubstantial positive and negative single-unit causal effects, cancelling each other out at the group level.\n\nModerate effect at group level, consistent with:\n\nModerate single-unit causal effects in the same direction for all individuals, or\nLarge single-unit causal effects in some individuals, no or negligible or even reversed single-unit causal effects in other individuals\n\nLarge effect at group level, consistent with:\n\nLarge single-unit causal effects in the same direction for all individuals, or\nLarge single-unit causal effects in most individuals, moderate, no or negligible or even reversed single-unit causal effects in a few individuals.\n\n\n\n\nCross-level bias\nCross-level bias, also known as cross-level fallacy, aggregation bias or the ecological fallacy (terms I will use interchangeably), refers to the logical error of inferring that a trend observed at one level of analysis applies uniformly to all units at a lower level of analysis.\nFor example, if there is little or no difference between group averages in performance between a control group and a treatment group, this could indicate that:\n\nThe treatment had no or negligible effect on every individual.\nThe treatment had varying effects on individuals—some performed better, while others performed worse—resulting in the effects canceling out in the group average.\n\nIf scenario 2 is true, then it would be incorrect to conclude that the “treatment has no or negligible effect on individual performance”. This would be an example of aggregation bias.\nSimilarly, suppose the treatment group, on average, performed slightly better than the control group. This outcome could suggest that:\n\nThe treatment had a moderate effect on every individual.\nThe treatment had a significant effect on some individuals but no or negligible effect on others, leading to a moderate effect on average.\n\nIf scenario 2 is true, then it would be incorrect to conclude that the “treatment has a moderate effect on individual performance”. This would be an example of aggregation bias.\nIt is quite common for research papers in psychology to conclude, “Our results suggest that treatment A has a moderate effect on performance,” based solely on group-level analysis. However, if the study focuses on truly intra-individual phenomena, such as memory, attention, or perception, this conclusion assumes that the group-level result represents the treatment effects at the individual level—an assumption that is vulnerable to aggregation bias.\nThis risk is rarely discussed as an alternative explanation for the results. Authors often implicitly assume that the group-level result is representative of the treatment effects at the individual level. This assumption may or may not be accurate, and it should be explicitly addressed. Ideally, researchers should collect data that allows for individual-level analysis to avoid this bias.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "07-method25.html#practice",
    "href": "07-method25.html#practice",
    "title": "7  M: Within-participant Experiments",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\n\nEasy\n\n7E1. Threats to the validity of within-subject designs (also called repeated measure designs) include maturation, history effects, and carry-over effects. Explain each of these in your own words. Try to exemplify with reference to made-up scenarios.\n\n\n\n7E2. According to my notes above, single-N designs are within subject designs, but not all within-subject designs are single-N designs. The difference being in the targeted causal effect. Explain.\n\n\n\n7E3. Think of an experiment measuring responses under two treatments, A and B, and two groups of participants, one doing A before B, and the other doing B before A.\nFor each of the scenarios below, provide an example that would make it clear why the stated analytic strategy make sense.\n\nA scenario where you could safely ignore the between-subject factor and analyze this as a within-subject experiment.\nA scenario in which you would analyze this as a mixed design, with a focus on the interaction between treatment and treatment order.\nA scenario where you would prefer to analyze this as a between-participant experiment, ignoring the second measurement.\n\n\n\n\n7E4. Discuss the challenges in distinguishing between individual differences and measurement error in a within-participant experiment. Under what circumstances might it be difficult to determine whether observed variability is due to true individual differences or simply measurement error?\n\n\n\n7E5. Remember B.F. Skinner: “It is more useful to study one animal for 1000 hours than to study 1000 animals for one hour” (cited in Smith & Little (2018))\nIn what situations is Skinner’s perspective valid, and when might it not be? Discuss the advantages and limitations of intensive study of a single subject compared to brief studies of multiple subjects. Consider factors such as the nature of the research question, the goals of the study, and the generalizability of the findings. Provide examples to illustrate your points.\n\n\n\n7E6.\n\nWhat is a mixed between-within-participant design?\nDescribe a study where the primary focus of a mixed design is the within-participant variation, with between-participant variation serving as a control condition.\nDescribe a study where the primary focus of a mixed design is the between-participant variation, with within-participant variation serving as a control condition.\n\n\n\n\n7E7. Here a brief description of two emotion recognition experiments, both using a mixed design:\n\nEighty participants were randomly assigned to two different stimulus orders.\nParticipants were divided into two groups: 40 young adults (aged 20-35) and 40 older adults (aged 65-80), each group presented with the same set of stimuli.\n\nCompare the extent to which the between-participant results can be given a causal interpretation.\n\n\n\nMedium\n\n7M1. This is from Wikipedia: “For each of the 48 US states Robinson computed the illiteracy rate and the proportion of the population born outside the US. He showed that these two figures were associated with a negative correlation of -0.53; in other words, the greater the proportion of immigrants in a state, the lower its average illiteracy. However, when individuals are considered, the correlation was +0.12 (immigrants were on average more illiterate than native citizens).”\n\nHow is it possible to get both a positive and a negative correlation for the association between literacy and immigrant status? Draw a DAG and explain in words\nGuess the topic of the Wikipedia article from which the quote originates.\n\n\n\n\n7M2. In a within-subject experiment, you present 9 stimuli to each of 15 participants. The stimuli are repeated 11 times each, in total 99 stimuli presented in a random order. Discuss pros and cons of using:\n\nThe same random order for each participant,\nDifferent random orders for each participant.\n\n\n\n\n7M3. Consider the following three methods for generating random orders of the 99 stimuli described in 7M2:\n\nGenerate a completely random order of the 99 stimuli.\nGenerate a random order of the 99 stimuli, with the constraint that each stimulus type (A, B, …, I) can only be repeated after all other stimuli have been repeated an equal number of times.\nGenerate a random order of the 99 stimuli, with the constraint that no stimulus is repeated consecutively in the sequence.\n\nEvaluate the advantages and disadvantages of each method with respect to threats to validity of repeated-measure experiments.\n\n\n\n7M5. You are interested in exploring how people learn to recognize emotional expressions. You have access to a large data set of videos in which individuals display eight specific emotions through facial expressions and vocal tones. Your objective is to train participants to accurately identify these emotions by showing them the videos and having them select the correct emotion from a list of eight options.\nIn this experiment, you aim to investigate how feedback (indicating whether their response was correct or incorrect) influences the improvement of emotion recognition over time through repeated testing.\nTask: Design an experiment that examines the impact of feedback on the performance of individual participants using a single-subject (single-N) design. Assume you will be testing five participants. Your response should address the following components:\n\nSpecify the number of training sessions you will conduct, indicating which sessions will include feedback and which will not. Justify your choices and explain the reasoning behind the sequence and structure of these sessions.\nDescribe how you will distinguish the effects of feedback from other factors, such as practice effects or increased familiarity with the task, ensuring that any observed improvements can be attributed to feedback.\nDetail your plan for data analysis, including the methods you will use to assess the impact of feedback on emotion recognition performance at the individual level.\n\n\n\n\n7M6. A recent large and well-conducted randomized controlled trial compared weight loss in two groups: one following a low-calorie diet and the other a low-carbohydrate diet. Both groups lost an average of 3 kg over the 6-month study period (between-group difference = 0.0 kg, 95 CI [-0.4, 0.4]). Based on this study, I conclude:\n“Since both diets resulted in similar weight loss, it doesn’t matter which one I follow—either a low-calorie or low-carbohydrate diet will have the same effect for me.”\nCritically assess this conclusion.\n\n\n\nHard\n\n7H1. The figure below is data from a classical study on suicide rate and religious affiliation (data from 1880-1890). Suicide rate seem to increase with the proportion of protestants in four provinces (each data point refer to one province). The conclusion was that their religion make Protestants more likely to commit suicide than non-Protestants (primarily Catholics in this data). Find an alternative explanation.\nExample from Morgenstern et al. (2021), data originally presented by the sociologist Emile Durkheim.\n\n\n\nCode\nx &lt;- c(0.3, 0.45, 0.785, 0.95)\ny &lt;- c(9.56, 16.36, 22.0, 26.46)\nplot(x, y, xlim = c(0.2, 1), ylim = c(0, 30), xlab = \"Proportion protestant\",\n     ylab = \"Suicide rate (per 100,000)\")\ncf &lt;- coef(lm(y ~ x))\nxx &lt;- c(0.3, 0.95)\nyy &lt;- cf[1] + cf[2]*xx\nlines(xx, yy, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n7H2. Below is a schematic representation of a classic experimental design known as the Solomon four-group design. This design involves a treatment condition (X) with outcome (O) measured before and/or after the treatment. Participants are randomly assigned to one of the four groups, and the aim is to estimate the causal effect of treatment on outcome.\nDiscuss the purpose of each group in this design with respect to reducing potential threats to internal validity.\n\n\n\n\nGroup\nPre-treatment\nTreatment\nPost-treatment\n\n\n\n\n1\nO\nX\nO\n\n\n2\nO\n\nO\n\n\n3\n\nX\nO\n\n\n4\n\n\nO\n\n\n\n\n\n7H3. Here stimulus orders for four participants (1-4), each tested on four conditions (A, B, C, D). Is this a balanced Latin square design? Motivate why or why not.\n\n\n\n                 \n1 \"A\" \"B\" \"D\" \"C\"\n2 \"B\" \"C\" \"A\" \"D\"\n3 \"C\" \"D\" \"B\" \"A\"\n4 \"D\" \"A\" \"C\" \"B\"",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "07-method25.html#session-info",
    "href": "07-method25.html#session-info",
    "title": "7  M: Within-participant Experiments",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       yaml_2.3.10       rlang_1.1.6      \n[17] jsonlite_2.0.0    V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nChristensen, L., Turner, L. A., & Johnson, R. B. (2023). Randomized designs in psychological research.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945–960.\n\n\nKantowitz, B. H., Roediger III, H. L., & Elmes, D. G. (2014). Experimental psychology. Cengage Learning.\n\n\nLambert, M. C., Cartledge, G., Heward, W. L., & Lo, Y. (2006). Effects of response cards on disruptive behavior and academic responding during math lessons by fourth-grade urban students. Journal of Positive Behavior Interventions, 8(2), 88–99.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nMorgenstern, H., Wakefield, J., et al. (2021). Ecologic studies and analysis. Modern Epidemiology. 4th Ed. Wolters Kluwer.\n\n\nNewbold, D. J., Laumann, T. O., Hoyt, C. R., Hampton, J. M., Montez, D. F., Raut, R. V., Ortega, M., Mitra, A., Nielsen, A. N., Miller, D. B., et al. (2020). Plasticity and spontaneous activity pulses in disused human brain circuits. Neuron, 107(3), 580–589.\n\n\nSmith, P. L., & Little, D. R. (2018). Small is beautiful: In defense of the small-n design. Psychonomic Bulletin & Review, 25(6), 2083–2101.\n\n\nTirado, C., Gerdfeldter, B., & Nilsson, M. E. (2021). Individual differences in the ability to access spatial information in lag-clicks. The Journal of the Acoustical Society of America, 149(5), 2963–2975.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>M: Within-participant Experiments</span>"
    ]
  },
  {
    "objectID": "08-stat25.html",
    "href": "08-stat25.html",
    "title": "8  S: Linear Regression: Basics",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#topics",
    "href": "08-stat25.html#topics",
    "title": "8  S: Linear Regression: Basics",
    "section": "",
    "text": "Monotonic versus linear relationship\nCoefficient of correlation\n\nPearson, standardized measure of linear trend\nSpearman, standardized measure of monotonic trend\nAnscombe’s quartet\n\nNon-parametric smoothing functions (e.g., lowess())\n(Bootstrapping of bivariate data. Optional, for the interested reader!)\nLinear regression. Basic model: \\(y = b_0 + b_1x + \\epsilon\\).\n\nRegression coefficients, \\(b_0\\) intercept, \\(b_1\\) slope\n\\(\\epsilon\\) “error”, i.e., variability in the outcome not accounted for by the linear predictor:\n\nlm() calls it Residual standard error\nglm() calls it dispersion parameter and reports it squared (variance)\nstan_glm() calls it \\(\\sigma\\)\n\n\nstan_glm(): Understand output and how to obtain compatibility intervals using the function posterior_interval()\nRegression to the mean. Be aware of fallacies!\nGeneral Linear Model (GLM), basic model plus assumption of normal distribution:\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1x_i\\)\nEstimating regression models, three methods:\n\nOrdinary Least Squares (OLS)\nMaximum likelihood\nBayesian inference\n\n\nReadings.\n\nGelman et al. (2021), Chapter 6 (you may also read Chapter 5, section 5.4 on bootstrapping). Note: The book does not discuss Spearman’s coefficients of correlation and says little about Pearson’s. This probably reflects the author’s general preference for meaningful units and unstandardized effect size estimates. See my example below for how to derive compatibility intervals around a correlation coefficient, using bootstrapping.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#monotonic-versus-linear-relationship",
    "href": "08-stat25.html#monotonic-versus-linear-relationship",
    "title": "8  S: Linear Regression: Basics",
    "section": "8.1 Monotonic versus linear relationship",
    "text": "8.1 Monotonic versus linear relationship\n\nLinear relationship. X and Y are perfectly linearly related if data fall on a straight line in a scatter-gram, i.e., \\(Y = b_0 + b_1X\\).\nMonotonic relationship. (a) Y increases or stay the same for every increase in X, or (b) Y decreases or stay the same for every increase in X. All linear relationships are monotonic, but not all monotonic relationships are linear.\nPearson’s coefficient of correlation: \\(r_{x, y} = \\frac{Cov(x, y)}{\\sqrt{Var(x)Var(y)}}\\). \\(r_{x, y}\\) is a measure of the degree to which two variables are linearly related.\nSpearman’s coefficient of correlation is defined as the Pearson correlation coefficient between the ranked variables. It is a measure of the degree to which two variables are monotonically related.\n\n\n\nCode\n# These are just examples\nx &lt;-    c(1, 2, 3, 4, 5, 6, 7)\nylin &lt;- c(2, 3, 4, 5, 6, 7, 8)\nymon &lt;- c(2, 2.1, 4.7, 5.1, 5.2, 5.5, 8)\nplot(x, x, pch = '', ylim = c(0, 8), ylab = 'ylin (black), ymon (blue)')\npoints(x, ylin, type = 'b')\npoints(x, ymon, type = 'b', col = 'blue')\n\n\n\n\n\n\n\n\n\nCode\n# Correlation matrix\nrr &lt;- data.frame(x, ylin, ymon)  # Put variables in a data frame\nround(cor(rr, method = 'pearson'),2)\n\n\n        x ylin ymon\nx    1.00 1.00 0.94\nylin 1.00 1.00 0.94\nymon 0.94 0.94 1.00\n\n\nCode\ncor(rr, method = 'spearman')\n\n\n     x ylin ymon\nx    1    1    1\nylin 1    1    1\nymon 1    1    1\n\n\n\n\nAnscombe’s quartet\n\n\nCode\n# Get Anscombe's quartet data, it's in R\nd &lt;- anscombe\n\n# Plot the quartet using this function\nmyplot &lt;- function(x, y) {\n  plot(x, y, xlim = c(4, 20), ylim = c(3, 13), pch = 21, bg = 'darkgrey')\n  mfit &lt;- lm(y ~ x)\n  abline(mfit)\n  pear &lt;- cor(x, y, method = 'pearson')\n  spear &lt;- cor(x, y, method = 'spearman')\n  text(18, 5, round(pear, 2))\n  text(18, 4, round(spear, 2), col = 'lightblue')  #Spearman in blue\n}\n\n# Fix default settings of plotting: 2x2 panels and fix margins of figures\npar(mfrow = c(2,2), mar = c(5, 4, 1, 1))  \n\n# Use function to plot\nmyplot(d$x1, d$y1)\nmyplot(d$x2, d$y2)\nmyplot(d$x3, d$y3)\nmyplot(d$x4, d$y4)\n\n\n\n\n\n\n\n\n\n\n\n\nNon-parametric smooting lines, e.g., lowess()\nExample using data from Howell (2012) (Fig 9.2).\n\n\nCode\n# Data: Pace of life and Incidence of Heart Disease in 36 US cities\npace &lt;- c(27.67, 25.33,23.67, 26.33, 26.33, 25.00, 26.67, 26.33, 24.33, 25.67, \n          22.67, 25.00, 26.00, 24.00, 26.33, 20.00, 24.67, 24.00, 24.00, 20.67, \n          22.33, 22.00, 19.33, 19.67, 23.33, 22.33, 20.33, 23.33, 20.33, 22.67, \n          20.33, 22.00, 20.00, 18.00, 16.67, 15.00)\nheart &lt;- c(24, 29, 31, 26, 26, 20, 17, 19, 26, 24, 26, 25, 14, 11, 19, 24, 20, 13, \n           20, 18, 16, 19, 23, 11, 27, 18, 15, 20, 18, 21, 11, 14, 19, 15, 18, 16)\n\n\n\n\nCode\n# Plot data (Fig. 9.2)\nplot(pace, heart, xlab = \"Pace of life [a.u.]\", \n     ylab = \"Age-adjusted rate of heart disease\")\n\n# Draw smoothing curve\nlines(lowess(pace, heart), col = \"blue\", lty = 1)\n\n# Draw linear regression line\nabline(lm(heart ~ pace), lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nBootstrapped confidence interval of correlation coefficient (Optional!)\nThis section is optional (no exam questions on Bootstrapping!).\nBelow example how to do derive a confidence interval around Spearman’s rho with Bootstrapping. Data is on activity of cats and their preference for food-puzzles (this is data from a paper discussed at previous year’s Research methods 1.)\n\n\nCode\n# Function that takes random pairs of observations from the sample with \n# replacement and calculates a correlation. m = number of observations in the \n# sample = nrow(d)\ndo_stat &lt;- function(){\n  rownumber &lt;- sample(length(pace), size = length(pace), replace = TRUE)\n  pp &lt;- pace[rownumber]\n  hh &lt;- heart[rownumber]\n  cor(pp, hh, method = \"spearman\")  # Please try method = \"pearson\"\n}\n\n# Data\nactivity &lt;- c(4286.30, 5548.30, 10122.30, 6306.48, 3631.64, NA, 3745.55, 4143.21,\n                3895.82, 5147.45, 4656.12, 3843.03, 5655.86, 4820.61, 6723.03, \n                3447.50, 3710.67)\ncf_time &lt;- c(0.665540690, 0.457864800, 0.344189459, 0.774329406, 0.062175325, \n             0.438901412, 0.869390384, 0.064502761, 0.661828747, 0.123849917, \n             0.486835855, 0.016898694, 0.005084774, 0.210401442, 0.138330489, \n             0.001552313, 0.326076928)\ncats &lt;- data.frame(activity, cf_time)\ncats &lt;- cats[complete.cases(cats), ]  # Remove rows with missing data\n\n# Plot relationship\nplot(cats$activity, cats$cf_time, pch = 21, bg = \"grey\", ylim = c(0, 1),\n     xlab = \"Cat activity [activity measure from some device]\", \n     ylab = \"Proportion time spent on food puzzle (vs. tray)\")\n\n# Add smoothing curve to plot\nlines(lowess(cats$activity, cats$cf_time), col = \"blue\", lty = 2)\n\n# Add spearman coef all data, and with extreme x-value removed\nspear &lt;- cor(cats$activity, cats$cf_time, method = \"spearman\")\nspear2 &lt;- cor(cats$activity[cats$activity &lt; 9000], \n             cats$cf_time[cats$activity &lt; 9000], \n             method = \"spearman\", use = \"pairwise.complete.obs\")\ntext(x = 8500, y = 0.95,\n     sprintf(\"Spearman's rho = %0.2f \\n(with extreme x-value removed: %0.2f)\", \n             spear, spear2), cex = 0.9)\n\n\n\n\n\n\n\n\n\nBootstrap 95 % CI:\n\n\nCode\ndo_stat &lt;- function(){\n  rownumber &lt;- sample(nrow(cats), size = nrow(cats), replace = TRUE)\n  cc &lt;- cats[rownumber, ]  # Select rows from data frame\n  cor(cc$activity, cc$cf_time, method = \"spearman\") \n}\n\n# Do the bootstrap\nset.seed(123)\nboots &lt;- replicate(n = 1e4, do_stat())\n# Calculate confidence interval\nhist(boots, breaks = 100, main = \"Bootstrapped Spearman's rho\")\n\n\n\n\n\n\n\n\n\nCode\nbootci &lt;- quantile(boots, probs = c(0.025, 0.975))\nround(bootci, 2)\n\n\n 2.5% 97.5% \n-0.38  0.69 \n\n\n\nMy interpretation: The result was inconclusive with regard to a monotonic relationship between activity and relative time spent on the food puzzle. Spearman’s rho was positive (0.21), but the compatibility interval [-0.38, 0.69] was wide and included substantial negative as well as positive coefficients.\nCompare with the author’s (questionable) interpretation : “There was no correlation between CFtime and activity, Spearman correlation coefficient r(16) = 0.21, p = 0.44.”",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#linear-regression",
    "href": "08-stat25.html#linear-regression",
    "title": "8  S: Linear Regression: Basics",
    "section": "8.2 Linear regression",
    "text": "8.2 Linear regression\nBasic regression model (Gelman et al., 2021, p. 81):\n\\(y = b_0 + b_1x + \\epsilon\\),\nwhere \\(b_0\\) and \\(b_1\\) are regression coefficients, and \\(\\epsilon\\) is “error” or variability not related to the predictor \\(x\\). (See below, General Linear Model for additional assumptions and a more transparent notation of the model, where “error” will be called \\(\\sigma\\)).\n\nIn this course we will also consider four extensions of this basic model, namely\n\nSeveral predictors, that is, multiple regression: \\(y = b_0 + b_1x_1 + b_2x_2, + ... + b_kx_k + \\epsilon\\),\nNonlinear models, such as: \\(y = b_0 + b_1log(x) + \\epsilon\\),\n\nNonadditive models with an interaction term: \\(y = b_0 + bx_1 + bx_2 + bx_1bx_2 + \\epsilon\\),\n\nLogistic regression: A generalized linear model of binary outcomes.\n\n\n\nSimple example: Regression fake data\nWe will simulate some data. I am using the code from Gelman et al.  (2020; Section 6.2, p. 82).\n\n\nCode\nset.seed(123)  # set.seed() not given in the book, so we cannot reproduce exactly!\nx &lt;- 1:20\nn &lt;- length(x)\na &lt;- 0.2  # Intercept\nb &lt;- 0.3  # Slope \nsigma &lt;- 0.5  # This is the \"error\" component (epsilon)\ny &lt;- a + b*x + rnorm(n, mean = 0, sd = sigma)  # Note: Gelman et al. \n                                               # coded  the last term \n                                               # +  sigma*rnorm(n); same thing.\nfake &lt;- data.frame(x, y)\n\n# Scatter plot \nplot(fake$x, fake$y, pch = 21, bg = \"grey\")\n\n\n\n\n\n\n\n\n\n\nBelow I use the lm() function to fit a line to the data.\n\n\nCode\nfit_1 &lt;- lm(y ~ x)\nsummary(fit_1)  # Summary stat\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99395 -0.30140 -0.01884  0.25971  0.86677 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.35505    0.23100   1.537    0.142    \nx            0.29198    0.01928  15.141  1.1e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4973 on 18 degrees of freedom\nMultiple R-squared:  0.9272,    Adjusted R-squared:  0.9232 \nF-statistic: 229.3 on 1 and 18 DF,  p-value: 1.102e-11\n\n\nCode\nconfint(fit_1)  # Confidence intervals (we call them \"compatibility intervals\")\n\n\n                 2.5 %    97.5 %\n(Intercept) -0.1302658 0.8403583\nx            0.2514646 0.3324908\n\n\nHere a plot corresponding to Gelman et al.’s Fig. 6.1.\n\n\nCode\nplot(fake$x, fake$y, main = \"Data and fitted regression line\", \n     pch = 21, bg = \"grey\")  # Plot data\nabline(fit_1)  # Add regression line\n\n# Add text with regression equation\na_hat &lt;- fit_1$coefficients[1]  # Intercept\nb_hat &lt;- fit_1$coefficients[2]  # Slope\nequation &lt;- sprintf(\"y = %.2f + %.2fx\", a_hat, b_hat)  # As text\ntext(mean(fake$x) + 0.5, mean(fake$y), equation, adj = 0, cex = 0.8)\n\n\n\n\n\n\n\n\n\n\nHere I fit the same model as above using stan_glm(). See, Gelman et al. p. 82, but note that I added the option refresh = 0 in stan_glm() to suppress the Stan’s sampling-progress output.\n\n\nCode\nfit_1 &lt;- stan_glm(y ~ x, data = fake, refresh = 0)  # library(rstanarm)\nprint(fit_1, digits = 2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 20\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 0.36   0.24  \nx           0.29   0.02  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.52   0.09  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\n# \"Posterior uncertainty intervals\"\", often called \"credible interval\", but we \n# call them \"Compatibility intervals\"!\nci95 &lt;- posterior_interval(fit_1, prob = 0.95)\nround(ci95, 3)\n\n\n              2.5% 97.5%\n(Intercept) -0.117 0.870\nx            0.249 0.332\nsigma        0.383 0.768\n\n\n\nHere is a table corresponding to Gelman et al.’s Fig. 6.2, but I added a column for compatibility intervals\n\n\n\n\n\n\n\n\n\n\nParameter\nAssumed value\nEstimate\nUncertainty\n95 % CI\n\n\n\n\n\\(a\\)\n0.2\n0.36\n0.24\n[-0.12, 0.87]\n\n\n\\(b\\)\n0.3\n0.29\n0.02\n[0.25, 0.33]\n\n\n\\(\\sigma\\)\n0.5\n0.52\n0.09\n[0.38, 0.77]\n\n\n\nNote.\n\nParameter, is the intercept, \\(a\\), slope, \\(b\\), and unaccounted variability (“error”), \\(\\sigma\\) of the fitted linear model.\nAssumed value is the “true” population value according to our simulated scenario.\nEstimate is the point estimate of the population parameter.\nUncertainty is the “standard error” of the estimate (i.e, “standard error” in the general interpretation of the term, here calculated as the MAD-SD of the posterior distribution).\n95 % CI is the 95 % Compatibility interval around the estimate (this interval is also known as the Posterior uncertainty interval or credible interval).\n\n\n\n\nInterpretation of regression coefficients\nThe intercept is the predicted mean value of the outcome when x = 0. The regression coefficient (“slope”) represents the difference in the outcome for each one-unit difference in the predictor. Example: If the outcome is weight (measured in kg) and the predictor is height (measured in cm), the slope’s unit would be kg/cm.\nNote that the slope can always be interpreted as a comparison: It is a difference between the average outcomes of observations with predictor values of \\(x+1\\) vs. \\(x\\).\nSometimes the slope parameter can also be interpreted as a causal effect, for example, the between-group difference in a randomized experiment. If the treatment variable, T, is an indicator with treatment group coded 1 and control group coded 0, the regression coefficient for T can be interpreted as the average causal effect of the treatment on the outcome. This may be viewed as an estimate of the average difference between potential outcomes: one where everyone was tested under the treatment condition and another where everyone was tested under the control condition. Interpreting the observed coefficient as a causal-effect estimate requires strong assumptions. Therefore, it is generally safer to interpret a regression parameter as a difference between groups rather than as a causal effect. See section 6.3 and elsewhere in Gelman et al. (2021).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#regression-to-the-mean",
    "href": "08-stat25.html#regression-to-the-mean",
    "title": "8  S: Linear Regression: Basics",
    "section": "8.3 Regression to the mean",
    "text": "8.3 Regression to the mean\n\nSuccess: Talent + luck; Great success: A little more talent + much more luck.\nSports Illustrated Jinx.\nA flurry of deaths by natural causes in a village led to speculation about some new and unusual threat. A group of priests attributed the problem to the sacrilege of allowing women to attend funerals, formerly a forbidden practice. The remedy was a decree that barred women from funerals in the area. The decree was quickly enforced, and the rash of unusual deaths subsided. This proves that the priests were correct.\nOld exam question: “Children of chess world-champions are likely to become proficient chess players, but so far none have reached the top 100, presumably because chess world champions tend to be selfish persons unwilling to share their knowledge with others, including their children”. Find an alternative explanation\n\n\nSimulating fake data on students who take two tests, see Gelman et al., p.89.\n\n\nCode\nset.seed(123)\nn &lt;- 1000\ntrue_ability &lt;- rnorm(n, mean = 50, sd = 10)\nnoise_1 &lt;- rnorm(n, mean = 0, sd = 10)\nnoise_2 &lt;- rnorm(n, mean = 0, sd = 10)\nmidterm &lt;- true_ability + noise_1\nfinal &lt;-   true_ability + noise_2\nexams &lt;- data.frame(midterm, final)\nhead(exams)\n\n\n   midterm    final\n1 34.43726 39.27921\n2 37.29867 50.06760\n3 65.40728 60.17119\n4 49.38333 62.89736\n5 25.79945 53.03424\n6 77.55638 60.99797\n\n\n\nPlot data and add a regression line\n\n\nCode\n# Scatter plot\nplot(exams$midterm, exams$final, pch = 21, bg = \"white\")\n\n# Regression analysis to fit line to data\nfit_1 &lt;- stan_glm(final ~ midterm, data = exams, refresh = 0)\n\n# Add lines to plot\nabline(coef(fit_1))\nxyline &lt;- lines(0:100, 0:100, lty = 3, col = \"blue\")  # Line x = y\n\n\n\n\n\n\n\n\n\nCode\n# Show regression estimates\nprint(fit_1, digits = 2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      final ~ midterm\n observations: 1000\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 24.91   1.37 \nmidterm      0.50   0.03 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 11.72   0.27 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\nround(posterior_interval(fit_1, prob = 0.95), 2)\n\n\n             2.5% 97.5%\n(Intercept) 22.27 27.63\nmidterm      0.44  0.54\nsigma       11.22 12.26\n\n\n\nNote: Regression analysis actually assume that there is no measurement error in the predictor. So this assumption was violated by the analysis above. However, for prediction of final exam scores from mid-term exam scores, it make sense to treat the latter as fixed, simply because they are fixed (as exam scores tend to be).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#general-linear-model-glm",
    "href": "08-stat25.html#general-linear-model-glm",
    "title": "8  S: Linear Regression: Basics",
    "section": "8.4 General linear model (glm)",
    "text": "8.4 General linear model (glm)\n\n\n\nAll models are wrong, but some are useful(Box (1979))\n\n\n\n\nThe basic regression model (see Gelman et al., 2021, p. 81):\n\\(y_i = b_0 + b_1x_i + \\epsilon_i\\), for \\(i = 1, 2, ..., n\\) observations.\nWe typically assume that the “errors”, \\(\\epsilon_i\\), are independent and normally distributed with mean = 0 and a fixed standard deviation \\(\\sigma\\):\n\\(\\epsilon_i \\sim Normal(0, \\sigma)\\).\nGiven these assumptions, we can write the linear regression model as:\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1x_i\\)\nThe first line is the probabilistic part of the model, sometimes called the likelihood, that relates to irreducible “error”, that is, variation around predicted values (implied by \\(\\sigma)\\)).\nThe second line is the deterministic part, we will call it the linear predictor of the model. It gives the predicted mean \\(\\mu_i\\) of observations for given value of the predictor, \\(x_i\\) (or values of several predictors when we move to multiple regression),\nThe model imply infinitely many normal distributions, one for each possible value of x, and each with the same standard deviation (an assumption known as homoscedasticity).\nHere these model implications are illustrated for a few x-values: 10, 20, …, 90.\n(Code only for nerds.)\n\n\nCode\n# Simulate data\nset.seed(123)\nx &lt;- rep(seq(1, 100, 2), 10)\nx &lt;- x + rnorm(length(x), mean = 0, sd = 1)\ny &lt;- rnorm(length(x), mean = 50, sd = 20) + 0.8*x\n\n# Draw scattergram with regression line\nmfit &lt;- lm(y ~ x)\nsd_res &lt;- sd(mfit$residuals)\nylim = c(0, 200)\nplot(x, y, ylim = ylim)\nabline(mfit, lty = 1, col = \"blue\", lwd = 2)\n\n\n# Add normal curves\ndrawnormal &lt;- function(x_input) {\n  y_pred &lt;- mfit$coefficients[1] + mfit$coefficients[2]*x_input\n  sd_res &lt;- sd(mfit$residuals)\n  xx &lt;- seq(y_pred - 3 * sd_res, y_pred + 3 * sd_res, 1)\n  dx &lt;- x_input - dnorm(xx, mean = y_pred, sd = sd_res) * 200\n  lines(dx, xx, col = \"blue\", lwd = 2)\n  points(x_input, y_pred, pch = 21, bg = \"blue\", cex = 1.5)\n  lines(c(x_input, x_input), ylim, lty = 3, col = \"blue\" )\n}\n\n# Invisible() supresses unwanted output from sapply()\ninvisible(sapply(seq(10, 90, 10), drawnormal))\n\n# Add regression equation\nrect(0, 170, 29, 200, col = \"white\", border = NA)  # White box\nb0 &lt;- round(mfit$coefficients[1], 1)\nb1 &lt;- round(mfit$coefficients[2], 1)\nsres &lt;- round(sd_res, 1)\nequation1 = bquote(y[i] ~ \"~\" ~ Normal(mu[i], sigma == ~ .(sres)))\nequation2 = bquote(mu[i] == ~ .(b0) ~ \"+\" ~ .(b1) ~ x[i])\ntext(1, 190, equation1, adj = 0, cex = 0.8, col = \"blue\")\ntext(1, 180, equation2, adj = 0, cex = 0.8, col = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#methods-for-estimating-regression-models",
    "href": "08-stat25.html#methods-for-estimating-regression-models",
    "title": "8  S: Linear Regression: Basics",
    "section": "8.5 Methods for estimating regression models",
    "text": "8.5 Methods for estimating regression models\nThree methods for estimating parameters of a regression model:\n\nOrdinary Least squares  Minimizing the residual sum of squares. This is the classical method, in R we use lm() to obtain least square estimates.\nMaximum likelihood  Maximizing the likelihood function (more below). In R, the most common choice is to use glm(). However, maximum likelihood estimation is the same as Bayesian inference without priors, and can be implemented in stan_glm() with non-default arguments, see Gelman et al. (2021), p. 110.\nBayesian inference  Estimates the posterior distribution of parameters. The posterior is a compromise between the data (likelihood function) and prior beliefs (prior distribution), as described in more detail below. In R we can use stan_glm() to make Bayesian inference (using a Markov Chain Monte Carlo (MCMC) method). If the user does not specify priors, stan_glm() uses default priors.\n\nFor reasonably large data sets, the three methods give very similar estimates, as illustrated below.\n\n\nNote on missing data and complete-case analysis\nWhen faced with missing data, most software automatically applies a complete-case analysis—also known as list-wise deletion—by excluding all study units with missing values on any variables included in the regression model. This process is typically performed without warning, so it’s important to be mindful that it can result in a significant reduction in sample size. Additionally, it may introduce bias if the missingness is related to other variables, including those not measured, that could confound the relationship between the outcome and predictors.\nIf there is a substantial amount of missing data, it’s advisable to also perform analyses using a simple data imputation method (see Gelman et al. (2021), section 17.4; summarized in these notes, 11.4). Ideally, the interpretation of results will remain consistent. However, if the results change, careful consideration is required to determine the best course of action. In such cases, more advanced imputation methods may be necessary—refer to Gelman et al. (2021), section 17.5 for further guidance.\n\n\n\nlm(), glm() and stan_glm()\nData in figure above\n\n\nCode\n# Here data again, so you don't have to search for it in the code above!\nset.seed(123)\nx &lt;- rep(seq(1, 100, 2), 10)\nx &lt;- x + rnorm(length(x), mean = 0, sd = 1)\ny &lt;- rnorm(length(x), mean = 50, sd = 20) + 0.8*x\n\nplot(x, y)\n\n\n\n\n\n\n\n\n\n\nModel to be fitted to data:\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1x_i\\)\n\nMain task is to estimate intercept (\\(b_0\\)) and slope (\\(b_1\\)) of our linear model, \\(\\mu_i = b_0 + b_1x_i\\). stan_glm() will also give us an estimate of the standard deviation around the line (sigma, \\(\\sigma\\)).\n\nModel fit, using lm()\n\n\nCode\n# Data in data frame\ndd &lt;- data.frame(x, y)\n\n# Model fit using lm(), i.e. Ordinary Least Squares\nfit_lm &lt;- lm(y ~ x, data = dd) \nsummary(fit_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = dd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.775 -13.776   0.144  12.880  53.079 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.97230    1.81081   27.04   &lt;2e-16 ***\nx            0.81961    0.03135   26.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.23 on 498 degrees of freedom\nMultiple R-squared:  0.5785,    Adjusted R-squared:  0.5776 \nF-statistic: 683.5 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(fit_lm)\n\n\n                 2.5 %     97.5 %\n(Intercept) 45.4145258 52.5300794\nx            0.7580113  0.8812017\n\n\n\nModel above, using glm()\n\n\nCode\nfit_glm &lt;- glm(y ~ x, data = dd)\nsummary(fit_glm)\n\n\n\nCall:\nglm(formula = y ~ x, data = dd)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 48.97230    1.81081   27.04   &lt;2e-16 ***\nx            0.81961    0.03135   26.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 409.2695)\n\n    Null deviance: 483544  on 499  degrees of freedom\nResidual deviance: 203816  on 498  degrees of freedom\nAIC: 4430.1\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nconfint(fit_glm)\n\n\n                2.5 %    97.5 %\n(Intercept) 45.423172 52.521433\nx            0.758161  0.881052\n\n\n\nModel above, using stan_glm()\n\n\nCode\ndd &lt;- data.frame(x, y)  # stan_glm prefer data to be stored in a data frame\nfit_stan_glm &lt;- stan_glm(y ~ x, data = dd, seed = 123, refresh = 0)\nprint(fit_stan_glm, digits = 2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 500\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 49.01   1.83 \nx            0.82   0.03 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 20.26   0.63 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\nround(posterior_interval(fit_stan_glm, prob = 0.95), 3)\n\n\n              2.5%  97.5%\n(Intercept) 45.458 52.519\nx            0.759  0.881\nsigma       19.060 21.622\n\n\n\nBelow is for advanced users\nstan_glm() does Bayesian estimation of the model parameters. In our simple example above, we have three parameters: Intercept (\\(b_0\\)), slope (\\(b_1\\)), and standard deviation around the regression line (\\(\\sigma\\)). The stan_glm()-algorithm (called “Hamiltonian Monte Carlo”, HMC) samples many sets of parameters, each set with three values &lt;\\(b_0\\), \\(b_1\\), \\(\\sigma\\)&gt;. HMC does this this in a clever way that assure that parameter-sets that fit the data better are more likely to be sampled. The default is to collect 4000 sets of parameters. The sampled parameters are stored and can be used for further data analysis.\nHere an example, where I draw many regression lines, each a reasonable line given the data and default priors used by stan_glm() (default is weak priors, meaning that they will not yield estimates different from approaches like glm() that does not account for prior beliefs in the estimation).\n\n\nCode\n# Data frame with sampled parameter values\nss &lt;- data.frame(fit_stan_glm)\n# Rename parameters according to model notation above\nnames(ss) &lt;- c(\"b0\", \"b1\", \"sigma\")  \nhead(ss)\n\n\n        b0        b1    sigma\n1 46.38761 0.8591003 21.16674\n2 50.51860 0.8152445 20.50395\n3 50.27949 0.8287463 20.34503\n4 50.32700 0.7915958 20.69936\n5 52.26407 0.7799136 21.18584\n6 52.26827 0.7778340 20.67591\n\n\nCode\nplot(dd$x, dd$y)\n\n# Add a random sample of other lines. Use transparent color to see where most fall\nn &lt;- 50  # Number of lines to show\nrrows &lt;- sample(1:nrow(ss), size = n, replace = FALSE)\nrlines &lt;- ss[rrows, -3]  # n lines\nxx &lt;- 1:100\n\n# Add lines using for loop\nfor (j in 1:n) {\n  yy &lt;- rlines[j, 1] + rlines[j, 2] * xx\n  lines(xx, yy, col = rgb(0, 1, 0, 0.2))\n}\n\n# Add \"best\" line, defined by point estimates of b0 and b1\nabline(fit_stan_glm, lwd = 1.5)  \n\n\n\n\n\n\n\n\n\n\nThis code show the full posterior for the slope parameter, \\(b_1\\):\n\n\nCode\n# This extract the parameter values that stan sampled from the posterior \n# distribution using a specific Markow Chain Monte Carlo (MCMC) method called \n# Hamiltonian Monte Carlo\nss &lt;- data.frame(fit_stan_glm)\nbx &lt;- ss$x  # Subset slope parameter\n\n# Here a density plot of the sampled x coefficients. This is our estimate \n# of the slope parameter.\n# Plot density curve for sigma\nplot(density(bx), xlab = \"Slope parameter\", \n     main = \"Posterior probability: Slope parameter, with 95 and 50 % CI\")\n\n# Derive point estimate and compatibility interval\npoint_estimate &lt;- median(bx)  # This is summary value preferred by stan_glm\nci95 &lt;- quantile(bx, prob = c(0.025, 0.975))\nci50 &lt;- quantile(bx, prob = c(0.25, 0.75))\n\n# Plot point estimate and compatibility interval\narrows(x0 = ci95[1], x1 = ci95[2], y0 = 0.01, y1 = 0.01, \n       length = 0, col = \"blue\")\narrows(x0 = ci50[1], x1 = ci50[2], y0 = 0.01, y1 = 0.01, \n       length = 0, col = \"darkblue\", lwd = 3)\npoints(point_estimate, 0.01, pch = 21, bg = 'lightblue')  # Add point\"\n\n# Point estimate and MAD SD to confirm that the values are the same \n# as stan_glm reported in print(fit_stan_glm)\npost_mad &lt;- mad(bx)\nposterior_summary &lt;- c(point_est = point_estimate, MAD_SD = post_mad)\n\n# Add to plot\ntext2plot &lt;- sprintf(\"Median  = %.2f, MAD_SD =  %.2f\", \n                     round(point_estimate, 2), round(post_mad, 2))\ntext(0.85, 10, text2plot, adj = 0, cex = 0.75)\n\n\n\n\n\n\n\n\n\nCode\n# Print to console\nround(posterior_summary, 3)\n\n\npoint_est    MAD_SD \n    0.820     0.031",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#practice",
    "href": "08-stat25.html#practice",
    "title": "8  S: Linear Regression: Basics",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n8E1. Pearson’s coefficient of correlation, \\(r_{x, y}\\) is sometimes misleading. Illustrate this graphically in scattergrams for each of the examples below:\n\n\\(r_{x, y}\\) is too low because of a restriction in range.\n\\(r_{x, y}\\) is too high because of an outlier.\n\\(r_{x, y}\\) is close to zero because of a strong non-linear relationship\n\\(r_{x, y}\\) is high but less than 1.0 because of a perfect non-linear relationship.\n\n(Old exam question)\n\n\n\n8E2.\n\nWould you expect the relationship between weight [kg] and age [years] in children (0-18 years) to be linear, monotonic or non-monotonic?\nHow about adults (18-100 years)?\n\n\n\n\n8E3. Estimate by eye the intercept and slope of each of the four lines in the plot.\n\n\n\nCode\nplot(0, 0, pch= \"\", xlab = \"x\", ylab = \"y\", xlim = c(2, 11), ylim =c(-10, 10))\nabline(0, 1, col = \"red\", lty = 1)\nabline(-2, -0.5, col = \"blue\", lty = 2)\nabline(5, 0, col = \"darkgreen\", lty = 3)\nabline(-10, 2, col = \"black\", lty = 4)\n\n\n\n\n\n\n\n\n\n\n\n8E4. Draw lines illustrating these linear functions, assuming x is a continuous variable, and estimate by eye the values of \\(b_0\\), and \\(b_1\\) from your drawings.\n\n\\(y_i = b_0 + b_1 x_i\\)\n\\(y_i = b_1 x_i\\)\n\\(y_i = b_0\\)\n\n\n\n\n8E5. Draw by hand scatter grams illustrating (made up) data and regression lines for these linear models. Assume x to be continuous and \\(\\epsilon_i  \\neq  0\\). For each drawing, estimate by eye the values of \\(b_0\\), and \\(b_1\\).\n\n\\(y_i = b_0 + b_1 x_i + \\epsilon_i\\)\n\\(y_i = b_1 x_i + \\epsilon_i\\)\n\\(y_i = b_0 + \\epsilon_i\\)\n\n\n\n\n8E6. Below are two scattergrams with a non-parametric smoothing function (lowess()). For each scatterplot, do you think a linear regression model would be appropriate? Please motivate.\n\n\n\nCode\nset.seed(124)\n\npar(mfrow = c(1, 2))\n# Left\nn &lt;- 30\nx &lt;- runif(n, 0, 400)\ny &lt;- 200 - 0.4*x + rnorm(n, 0, 40)\nplot(x, y, ylim = c(0, 300), xlim = c(0, 400))\nlines(lowess(y ~ x), lty = 2, col = \"red\")\n\n# Right\nn &lt;- 30\nx &lt;- runif(n, 0, 400)\ny &lt;- 200 - 0.4*x + 0.002*(x - mean(x))^2 + rnorm(n, 0, 50)\nplot(x, y, ylim = c(0, 300), xlim = c(0, 400))\nlines(lowess(y ~ x), lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nMedium\n\n8M1. Below is the output from a regression analysis in R, estimating the relationship between reaction time (rt, in milliseconds) on a test and hours of sleep last night (ranging from 3 to 10 hours).\n\nHow many observations were there in this data set?\nWhat function in R was used to do the regression analysis?\nDraw a scattergram with the fitted line and some hypothetical data that appears consistent with the analysis output.\n\n\n\n\nCode\nset.seed(123)\nn &lt;- 15\nsleep &lt;- runif(n, 3, 9)\nrt &lt;- 400 - 20*sleep + rnorm(n, 0, 30)\nfit8m1 &lt;- lm(rt ~ sleep)\nsummary(fit8m1)\n\n\n\nCall:\nlm(formula = rt ~ sleep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.925 -26.438  -1.428  20.938  72.771 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   422.28      35.96   11.74  2.7e-08 ***\nsleep         -22.22       5.42   -4.10  0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.26 on 13 degrees of freedom\nMultiple R-squared:  0.5639,    Adjusted R-squared:  0.5304 \nF-statistic: 16.81 on 1 and 13 DF,  p-value: 0.001253\n\n\n\n\n8M2. Below are the 95% confidence intervals for the model in 8M1. In one sentence, describe the effect of sleep on reaction time (assuming a causal relationship), referencing both the point estimate (given above) and the interval estimate (below), and rounding the estimate to a reasonable level of precision. Ensure your phrasing aligns with the recommendations of Amrhein et al. (2019).\n\n\n\nCode\nconfint(fit8m1)\n\n\n                2.5 %   97.5 %\n(Intercept) 344.59947 499.9547\nsleep       -33.92954 -10.5131\n\n\n\n\n8M3. Draw by hand scatter grams illustrating (made up) data and regression lines for these linear models. Assume \\(D\\) to be an indicator variable coded {0, 1} and \\(\\epsilon_i  \\neq  0\\). For each drawing, estimate by eye the values of \\(b_0\\), and \\(b_1\\).\n\n\\(y_i = b_0 + b_1 D_i + \\epsilon_i\\)\n\\(y_i = b_1 D_i + \\epsilon_i\\)\n\\(y_i = b_0 + \\epsilon_i\\)\n\n\n\n\n8M4. Assume that you recoded \\(D\\) in 8M3, to have values {-1, 1}. What estimates would you then have for \\(b_0\\) and \\(b_1\\) in each of your drawings.\n\n\n\n8M5. Gelman et al. talks about descriptive, predictive and causal interpretation of regression coefficients (page 85). Explain in your own words with reference to a model of well-being as a function of physical activity.\n\n\n\n8M6. “Your outcome variable does not seem to be normally distributed, so linear regression would be inappropriate.” What’s wrong with this argument?\n\n\n::: 8M7. Revisit the simulation of mid-term and final exam scores.\n\nFind out what happens with the regression equation if we assume error-free midterm scores.\nFind out what happens with the regression equation as you make the errors of midterm and final scores smaller or larger. :::\n\n\n\n8M8. Rerun the fake-data simulation above from Gelman et al. (p.82), but increase the number of observations for each of the 20 x-values. Approximately how many observations per x-value would you need to correctly estimate (within 2 decimals) the true slope parameter most of the time?\n\n\n\n\nHard\n\n8H1.\nRelate the regression assumptions of linearity, normality and homoscedasticity to the model notation:\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1x_i\\).\n\n\n\nFollow up questions for the advanced student (this will not be on the exam!)\n\nSometimes, \\(ind\\) is added to the first line: \\(y_i \\overset{\\text{ind}}{\\sim} Normal(\\mu_i, \\sigma)\\),  why?\nAnd why not: \\(y_i \\overset{\\text{iid}}{\\sim} Normal(\\mu_i, \\sigma)\\)?\n\n\n\n\n\n8H2. Go back to your simulation in 8M8.\n\nDid the number of observations needed to recover the slope parameter suffice for the intercept? If not, why?\nWhat happen if you “restrict the range” of x to between 10 and 20? Try it out, and discuss the result.\nWould it help to rerun the simulation in (b) but with \\(X\\) centered around its mean? Try it out, and discuss the result.\n\n\n\n\n8H3. Run a bivariate regression on some data (simulated or real) using stan_glm(), plot the data in a scattergram with a set of regression lines from the posterior distribution (see examples above and in Gelman et al, e.g., their Fig. 9.2)\n\n\n\n8H4.\n\nFrom 8H3,plot the posterior for the “standardized” slope \\(\\frac{b_1}{\\sigma}\\), and report median posterior with 90 % compatibility interval.\nHow does \\(\\frac{b_1}{\\sigma}\\) relate to Cohen’s d?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "08-stat25.html#session-info",
    "href": "08-stat25.html#session-info",
    "title": "8  S: Linear Regression: Basics",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] rstanarm_2.32.1 Rcpp_1.0.14    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     dplyr_1.1.4          farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] shinystan_2.6.0      promises_1.3.3       shinyjs_2.1.0       \n[10] digest_0.6.37        mime_0.13            lifecycle_1.0.4     \n[13] StanHeaders_2.32.10  survival_3.7-0       magrittr_2.0.3      \n[16] posterior_1.6.1      compiler_4.4.2       rlang_1.1.6         \n[19] tools_4.4.2          igraph_2.1.4         yaml_2.3.10         \n[22] knitr_1.50           htmlwidgets_1.6.4    pkgbuild_1.4.8      \n[25] curl_6.4.0           plyr_1.8.9           RColorBrewer_1.1-3  \n[28] dygraphs_1.1.1.6     abind_1.4-8          miniUI_0.1.2        \n[31] grid_4.4.2           stats4_4.4.2         xts_0.14.1          \n[34] xtable_1.8-4         inline_0.3.21        ggplot2_3.5.2       \n[37] scales_1.4.0         gtools_3.9.5         MASS_7.3-61         \n[40] cli_3.6.5            rmarkdown_2.29       reformulas_0.4.1    \n[43] generics_0.1.4       RcppParallel_5.1.10  rstudioapi_0.17.1   \n[46] reshape2_1.4.4       minqa_1.2.8          rstan_2.32.7        \n[49] stringr_1.5.1        shinythemes_1.2.0    splines_4.4.2       \n[52] bayesplot_1.13.0     parallel_4.4.2       matrixStats_1.5.0   \n[55] base64enc_0.1-3      vctrs_0.6.5          V8_6.0.4            \n[58] boot_1.3-31          Matrix_1.7-1         jsonlite_2.0.0      \n[61] crosstalk_1.2.1      glue_1.8.0           nloptr_2.2.1        \n[64] codetools_0.2-20     distributional_0.5.0 DT_0.33             \n[67] stringi_1.8.7        gtable_0.3.6         later_1.4.2         \n[70] QuickJSR_1.8.0       lme4_1.1-37          tibble_3.3.0        \n[73] colourpicker_1.3.0   pillar_1.10.2        htmltools_0.5.8.1   \n[76] R6_2.6.1             Rdpack_2.6.4         evaluate_1.0.3      \n[79] shiny_1.11.1         lattice_0.22-6       markdown_2.0        \n[82] rbibutils_2.3        backports_1.5.0      threejs_0.3.4       \n[85] httpuv_1.6.16        rstantools_2.4.0     gridExtra_2.3       \n[88] nlme_3.1-166         checkmate_2.3.2      xfun_0.52           \n[91] zoo_1.8-14           pkgconfig_2.0.3     \n\n\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Retire statisticial significance. Nature, 567, 305–307.\n\n\nBox, G. E. (1979). Robustness in the strategy of scientific model building. In Robustness in statistics (pp. 201–236). Elsevier.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHowell, D. C. (2012). Statistical methods for psychology. Cengage Learning.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>S: Linear Regression: Basics</span>"
    ]
  },
  {
    "objectID": "09-method25.html",
    "href": "09-method25.html",
    "title": "9  M: Randomized Experiments",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "09-method25.html#topics",
    "href": "09-method25.html#topics",
    "title": "9  M: Randomized Experiments",
    "section": "",
    "text": "Between-participant experiments, and why they cannot target single-unit causal effects.\nPrima facie causal effect, i.e., observed contrast between group-averages.\nRandom assignment\n\nSimple randomization (a.k.a. completely randomized experiment)\nStratified randomization\n\nIn groups of participants (to adjust for categorical covariate)\nIn pairs of participants (to adjust for continous covariate)\n\n\nIgnorable assignment mechanism. Random assignment is an ignorable assignment mechanisms because it implies that groups will be balanced on all covariates, measured and unmeasured (for large n), which in turn implies that group assignment is independent of potential outcomes\nSample average causal effect unobserved true average in the sample\nPopulation average causal effect unobserved true average in the population\nSampling error. Relates to the sampling distribution and its dispersion\nRandomization error. Relates to the randomization distribution and its dispersion\nMatch first, then randomize may reduce randomization error.\n\nTheoretical articles to read:\n\nChristensen et al. (2023), sections on between-participant designs and randomization\nGelman et al. (2021), chapter 18, sections 18.3 - 18.6; tables 18.1-5.\n\nOther sources:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "09-method25.html#between-participant-designs",
    "href": "09-method25.html#between-participant-designs",
    "title": "9  M: Randomized Experiments",
    "section": "9.1 Between-participant designs",
    "text": "9.1 Between-participant designs\nRemember that the single-unit causal effect \\(\\tau_i\\) for an individual \\(i\\) involve the contrast between two potential outcomes, for example, their difference:\n\\(\\tau_i = y_i^1 - y_i^0\\), where\n\\(y_i^1\\) is individual \\(i\\)’s outcome had he or she been in the treatment group and \\(y_i^0\\) is \\(i\\)’s outcome had he or she been in the control group (cf. Gelman et al. (2021), p.343).\nThe Fundamental Problem of Causality (Holland (1986)) implies that \\(\\tau_i\\) always is unobserved, because we can never test a single individual twice at the same time. We may however estimate \\(\\tau_i\\) if repeated measures is an option. But in many situations, repeated measures are not practically or ethically possible or it is not scientifically meaningful to repeat measurements due to carry-over effects. In such situations, we cannot estimate single-unit causal effects (they are “unidentified”), and have to settle with estimates of average causal effects. In experimental research this is done by comparing outcomes of groups in between-subject experiments.\n\nThe way individuals are assigned to experimental groups in between-participants experiments is crucial for the causal interpretation of between-group differences. If the participants are free to chose experimental group or if we don’t know how they were assigned to treatments (both often called “self-selection”), then the difference between group means is probably a biased estimate of the average causal effect. The observed group difference is sometimes called the prima-facie causal effect estimate.\n\n\n\n\nTable 18.1 Gelman et al. (2021)\n\n\nIn the table above \\(z = 1\\) refer to treatment group (fish oil supplements) and \\(z = 0\\) refer to control group (no fish oil supplements). If interpreted causally, \\(\\tau_{pf} = 12.5\\) suggest that fish oil supplements on average increased systolic blood pressure with 12.5 units. This interpretation rests on the assumption that \\(\\tau_{pf}\\) is an unbiased estimate of what we really want to know, namely the average single-unit causal effect, SATE. \\(\\tau_{pf}\\) would be an unbiased estimate of SATE if it would be the same as SATE for very large sample sizes, or on average if the study was conducted over and over again.\nIf participants are self-selected to groups, then the prima-facie causal effect estimate may not equal the true average causal effect in the sample even for large N or repeated studies, because of systematic errors (bias) due to group differences at baseline, or group differences in treatment effects, or both.\n\n\n\n\nTable 18.2 Gelman et al. (2021)\n\n\nIn this table we see the unobserved potential outcomes (also known as the counterfactual outcomes). The true average causal effect was -7.5 units, that is, the treatment would on average decrease the systolic blood pressure, not increase it as suggested by the prima-facie estimate. Maybe the participants were free to select group, and the older, being more concerned with their health, chose fish oil supplements whereas the younger did not. If so, age may be a confounder causing both treatment and outcome.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "09-method25.html#randomized-experiment",
    "href": "09-method25.html#randomized-experiment",
    "title": "9  M: Randomized Experiments",
    "section": "9.2 Randomized experiment",
    "text": "9.2 Randomized experiment\nThe randomized experiment is generally agreed to be the “Gold Standard” for estimating average causal effects. A randomized experiment, in medicine often called randomized clinical trial (RCT), is a between-subject experiment in which participants are randomly assigned to experimental conditions, for instance to a control condition and a treatment condition. The main idea is that the contrast between the average outcome of the treatment group and the average outcome of control group is an unbiased estimate of the (unobservable) average causal effect of the treatment.\n\n\nRandom assignment methods\nRandom assignment means that study participants are assigned to different treatment conditions using a random process, for example, using a random number generator. Each participant has a non-zero probability to be assigned to all conditions, typically, but not always, with the same probability to each condition.\nThere are various ways to do this, below examples for an experiment with two groups (control and treatment groups):\n\nCompletely randomized experiment. Each participant have the same probability to be assigned to treatment. For example:\n\nFor each participant, we just toss a coin, or use R: z = sample(c(0, 1), size = 1), to decided whether he or she should be assigned to the the control (z = 0) or treatment group (z = 1). This may lead to different numbers of participants in the two groups.\nTo make sure that the both groups will have the same number of participants, all coin tosses can be made in advanced. For an experiment with eight participants:\nz = sample(c(0, 0, 0, 0, 1, 1, 1, 1), size = 8, replace = FALSE) will yield a random order of treatment conditions, and then you assign your first participant to the first treatment, your second to the second, etc. (there are of course equivalent alternatives, such as randomizing your list of participants, and assign the upper half to treatment z = 0 and the lower half to treatment z = 1.)\n\nStratified randomization\n\nRandomized blocks experiment. You do the randomization separately for specified blocks (strata) of participants, for example, separately for men and women. One may view this as several randomized experiments, one per stratum. Blocking will make sure that the groups are perfectly balanced on the stratified variable. For small samples, this may be a good idea if the stratified variable is strongly related to the outcome. Note that the probability of being assigned to treatment may differ between strata.\nMatched pairs experiment. Randomization done in blocks of only two participants (“twins”), may be very effective to create groups balanced on continuous variables. For example, if the outcome is performance on a logical test, with two groups, one doing physical exercises before the test and the other relaxing before the test, then intelligence may be an important variable to control. One way would be to measure IQ before the test, and then rank order the participants on their IQ scores, and randomly assign the two best to either conditions, the third and forth, to either conditions, etc. This would lead to approximately balanced groups with regard to IQ scores. See also Gelman et al.  Table 18.3.\n\n\n\nIn general:\nMatch or block as far as possible on measured variables that you consider causally relevant, then randomize.\nThis will reduce randomization errors (discussed further below), and thereby increase the precision of estimates.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "09-method25.html#why-random-assignment-works",
    "href": "09-method25.html#why-random-assignment-works",
    "title": "9  M: Randomized Experiments",
    "section": "9.3 Why random assignment works",
    "text": "9.3 Why random assignment works\n\nSimple answer: Makes the groups balanced an all covariates, measured and unmeasured. So random assignment works not by eliminating individual differences, but by making the composition of individual differences equal across treatment groups (for large n).\nTechnical answer: Because it implies that treatment assignment is independent of the potential outcomes. If true, the assignment mechanism may be considered ignorable (“worry-free”), i.e., the average causal effect estimates would only be prone to random errors.\n\nFor small sample sizes, randomization may not make group assignment independent of potential outcomes, and the groups they may still differ in expected outcomes at baseline or in the way they react to treatment. This is called a randomization error. Here is an illustration for potential outcomes of eight participants (from Gelman et al. (2021), Table 18.2):\n\n\nCode\n# Gelman background and potential uutcomes, Table 18.2\n\nid &lt;- c(\"Audry\", \"Anna\", \"Bob\", \"Bill\", \"Caitlin\", \"Cara\", \"Dave\", \"Doug\")\nfemale &lt;- c(1, 1, 0, 0, 1, 1, 0, 0)\nage &lt;- c(40, 40, 50, 50, 60, 60, 70, 70)\ny0 &lt;- c(140, 140, 150, 150, 160, 160, 170, 170)\ny1 &lt;- c(135, 135, 140, 140, 155, 155, 160, 160)\nsate &lt;- mean(y1) - mean(y0)\nd &lt;- data.frame(id = id, female = female, age = age, y0 = y0, y1 = y1)\n\n# Print data frame, using knitr::kable()\nknitr::kable(d, align = \"c\", \n             caption = \"Potential outcomes eight participants\")\n\n\n\nPotential outcomes eight participants\n\n\nid\nfemale\nage\ny0\ny1\n\n\n\n\nAudry\n1\n40\n140\n135\n\n\nAnna\n1\n40\n140\n135\n\n\nBob\n0\n50\n150\n140\n\n\nBill\n0\n50\n150\n140\n\n\nCaitlin\n1\n60\n160\n155\n\n\nCara\n1\n60\n160\n155\n\n\nDave\n0\n70\n170\n160\n\n\nDoug\n0\n70\n170\n160\n\n\n\n\n\n\nWith 8 participants, there are 70 ways in which half can be assigned to the treatment group and the rest to the control condition. Figure below show prima-facie estimates for all 70 splits.\n\n\nCode\n# All possible assignments of half to treatment group, rest to control group\ngg &lt;- combn(8, 4)\nall_pf &lt;- numeric(choose(8, 4))  # Empty vector to be filled\nidn &lt;- 1:8  # ID as 1 to 8\n\nfor (j in 1:choose(8, 4)){\n  treat &lt;- gg[, j]  # Treatment group\n  control &lt;- idn[!is.element(idn, gg[, j])]  # Remaining to control group\n  pf &lt;- mean(y1[treat]) - mean(y0[control])  # Calculate PF\n  all_pf[j] &lt;- pf\n}\n\nhh &lt;- hist(all_pf, breaks = seq(-27.75, 12.75, by = 0.5), \n     xlim = c(-30, 20), ylim = c(0, 20), \n     main = \"\", xlab = \"Prima-facie causal effect estimate\")\nlines(c(sate, sate), c(0, 20), col = \"red\", lwd = 0.4)\n\n\n\n\n\n\n\n\n\nThe red line show the average causal effect in this sample = -7.5 units, prima-facie estimates range from -27.5 to +12.5: Deviations from the average causal effect are randomization errors that only are due to how the participants happened to be assigned to groups. The particular assignments illustrated in Gelman et al’s Table 18.3 and 18.4 yield prima-facie estimates of -7.5 (= SATE) and -17.5 units, respectively.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "09-method25.html#simulating-a-randomized-experiment",
    "href": "09-method25.html#simulating-a-randomized-experiment",
    "title": "9  M: Randomized Experiments",
    "section": "9.4 Simulating a randomized experiment",
    "text": "9.4 Simulating a randomized experiment\nHere a simulation with a more realistic scenario of 30 + 30 participants, with an average causal effect in the population of 2 units (0.2 sd units) and average causal effect in the sample (n = 30 + 30) of about 3 units.\n\n\nCode\n# Simulate potential outcomes for one sample of n participants\nset.seed(999)\nn &lt;- 60\npate &lt;- 2  # Population average causal effect\ny0 &lt;- rnorm(n, mean = 50, sd = 10)\ny1 &lt;- rnorm(n, mean = 50 + pate, sd = 10)\nsate &lt;- mean(y1) - mean(y0)  # Sample average causal effect\n\n# Function that randomly assigns participants, and out-put the \n# prima-facie causal effect estimate\n\nrandexp &lt;- function() {\n  n &lt;- length(y0)\n  z &lt;- sample(c(rep(0, n/2), rep(1, n/2)), size = n, replace = FALSE)\n  y &lt;- (1 - z)*y0 + z*y1  # Observed value\n  pf &lt;- mean(y[z == 1]) -mean(y[z == 0])\n  pf\n} \n\n# Repeat randomization many times\nprima_facie &lt;- replicate(1e4, randexp())\n\n# Display result\nhist(prima_facie, breaks = seq(from = -10.5, to = 10.5, by = 1), main = \"\")\nmtext(side = 3, \n  text = \"Red line: Sample average causal effect\\nBlue line: Population average causal effect\",\n      cex = 0.8)\nlines(c(pate, pate), c(0, 5e3), col = \"blue\", lty = 1 )\nlines(c(sate, sate), c(0, 5e3), col = \"red\", lty = 2 )\n\n\n\n\n\n\n\n\n\nCode\nprint(round(c(sate = sate, pate = pate), 3))\n\n\n sate  pate \n2.974 2.000",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "09-method25.html#practice",
    "href": "09-method25.html#practice",
    "title": "9  M: Randomized Experiments",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n9E1. Random selection of study participants from a population increases the external validity of a study, whereas random assignment of participants to experimental conditions increases its internal validity. Explain.\n\n\n\n9E2. Why may it be beneficial to first match or block on measured variables before randomly assign to treatment conditions?\n\n\n\n9E3. The table below similar to those in chapter 18 of Gelman et al. (2021), with\n\nUnit: Name of participant\ny0: Potential outcome if in control group (group = 0)\ny1: Potential outcome if treated (group = 1)\nGroup: Treatment group (1 = treated, 0 = control)\nObserved: Observed data\n\n\nCalculate the sample average treatment effect (SATE)\nCalculate the prima-facie (or naive) causal effect estimate, does it agree with SATE?\nFrom the perspective of the potential outcome model of causality, causal inference may be viewed as a missing data problem, explain with reference to the table (what data is missing?)\n\n\n\n\nCode\nid &lt;- sample(LETTERS[1:10], size = 10, replace = FALSE)  # Participant number\nCpot &lt;- c(10, 11, 11, 12, 9, 12, 12, 13, 13, 14)  # Potential outcome treatment\nTpot &lt;- c(10, 10,  9, 12, 8, 13, 13, 14, 13, 15)  # Potential outcome control\ngroup &lt;- c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1)  # 1 = treatment, 0 = control group\nY &lt;- ifelse(group == 1, Tpot, Cpot)  # Observed outcome\nsingle_unit &lt;- Tpot - Cpot  # Single unit causal effects\n\n# Collect data in data frame\ntable_data &lt;- data.frame(Unit = id, \"y0\" = Cpot, \"y1\" = Tpot, \n                         Group = group, \n                         Observed = Y)\n# Print data frame, using knitr::kable()\nknitr::kable(table_data, align = \"l\", \n    caption = \"Hypothetical causal inference data from randomized experiment\")\n\n\n\nHypothetical causal inference data from randomized experiment\n\n\nUnit\ny0\ny1\nGroup\nObserved\n\n\n\n\nB\n10\n10\n0\n10\n\n\nJ\n11\n10\n0\n11\n\n\nI\n11\n9\n0\n11\n\n\nA\n12\n12\n0\n12\n\n\nH\n9\n8\n0\n9\n\n\nE\n12\n13\n1\n13\n\n\nC\n12\n13\n1\n13\n\n\nF\n13\n14\n1\n14\n\n\nG\n13\n13\n1\n13\n\n\nD\n14\n15\n1\n15\n\n\n\n\n\n\n\n9E4. Below is another table similar to those in Chapter 18 of Gelman et al. (2021).\nCalculate:\n\nObserved values (eight values listed in order from Audrey to Doug)\nSingle-unit causal effects (eight values listed in order from Audrey to Doug)\nThe Sample average treatment effect (SATE)\nThe Prima-facie causal-effect estimate (Gelman et al. call it “naive estimated treatment effect”)\n\n\n\n\nCode\nunit &lt;- c(\"Audrey\", \"Anna\", \"Bob\", \"Bill\", \"Caitlin\", \"Cara\", \"Dave\", \"Doug\")\nfemale &lt;- c(1, 1, 0, 0, 1, 1, 0, 0)\nage &lt;- c(40, 40, 50, 50, 60, 60, 70, 70)\ntreat &lt;- c(1, 1, 1, 0, 0, 0, 0 , 1)\ny0 &lt;- c(125, 125, 130, 130, 150, 150, 160, 160)\ny1 &lt;- c(125, 125, 120, 120, 150, 150, 150, 150)\n# Collect data in data frame\ntable_data &lt;- data.frame(Unit = unit, Female = female, Age = age, Treatment = treat, \n                               y0 = y0, y1 = y1)\n# Print data frame, using knitr::kable()\nknitr::kable(table_data, align = \"l\", \n    caption = \"Hypothetical causal inference data for the effect of a fish oil supplements on systolic blood pressure.\")\n\n\n\nHypothetical causal inference data for the effect of a fish oil supplements on systolic blood pressure.\n\n\nUnit\nFemale\nAge\nTreatment\ny0\ny1\n\n\n\n\nAudrey\n1\n40\n1\n125\n125\n\n\nAnna\n1\n40\n1\n125\n125\n\n\nBob\n0\n50\n1\n130\n120\n\n\nBill\n0\n50\n0\n130\n120\n\n\nCaitlin\n1\n60\n0\n150\n150\n\n\nCara\n1\n60\n0\n150\n150\n\n\nDave\n0\n70\n0\n160\n150\n\n\nDoug\n0\n70\n1\n160\n150\n\n\n\n\n\n\n\n9E5. The assignment to treatments in the table of 9E3 was random. Make a new random assignment where you first match the participants on sex and age and then randomly assign them to treatments (matched pair experiment).\n\nCalculate the prima-facie causal-effect estimate based on your new random assignment.\nBriefly comment on this estimate in relation to SATE and the single unit causal effects.\n\n\n\n\n\nMedium\n\n9M1. Gelman et al. (2021) define the sample average treatment effect (SATE) and the population average treatment effect (PATE). These are typically estimated by the prima facie treatment effect, i.e., the contrast between the average of the treatment group and the average of the control group.\n\nExplain why SATE and PATE are unobservable.\nRelate SATE and PATE to the concepts “internal validity” and “external validity”.\nIs it possible for the prima facie estimate to be close to SATE but far from PATE? Motivate your answer with an example.\nIs it possible for the prima facie estimate to be close to PATE but far from SATE? Motivate your answer with an example.\n\n\n\n\n9M2. What is a randomization distribution? Refer to SATE and prima-facie causal-effect estimates in your explanation.\n\n\n\n9M3. In education research, random assignment sometimes refer to schools or classes rather than pupils, and the unit of analyzes is schools (or classes). There might be different arguments for this, list as many as you can think of.\n\n\n\n\nHard\n\n9H1.\n(a) The the table in 9E3 show one random assignment of participants (column Group). How many different such randomization could you do with with the constraint of equal group sizes? (b) Do them all, and illustrate the corresponding prima-facie effect sizes in a histogram.\n\n\n\n9H2. Think of an experiment with 300 men randomly assigned to one of two groups. You have background information on age (continuous from 18 to 60 years), which is believed to influence the outcome variable (continuous variable).\nCompare these two design strategies:\n\nA completely randomized design, where you control for age in the statistical analyses using multiple linear regression (syntax: outcome ~ group + age).\nA matched pair design (pairs of equal age).\n\n\nDiscuss circumstances under which these design strategies would lead to similar causal effect estimates and when they may differ.\nSimulate data where the two designs would yield about the same point estimates, but the matched pair design would give higher precision (tighter confidence intervals)\n\n\n\n\n9H3. Think of a scenario where the first design in 9H2 may lead to a biased estimate of the causal effect. Explain in terms of the potential outcome model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "09-method25.html#session-info",
    "href": "09-method25.html#session-info",
    "title": "9  M: Randomized Experiments",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       yaml_2.3.10       rlang_1.1.6      \n[17] jsonlite_2.0.0    V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nChristensen, L., Turner, L. A., & Johnson, R. B. (2023). Randomized designs in psychological research.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945–960.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>M: Randomized Experiments</span>"
    ]
  },
  {
    "objectID": "10-stat25.html",
    "href": "10-stat25.html",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "",
    "text": "Topics\nToday we will fit linear models with no or one predictor:\nWe will illustrate this using data the data set kidiq described below, using kid_score as outcome variable.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#topics",
    "href": "10-stat25.html#topics",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "",
    "text": "Estimating the mean is the same as regressing on a constant term (Gelman et al., 2021, p. 99)\nIndicator variables\nEstimating a difference between two means is the same as regressing on an indicator variable (Gelman et al., 2021, p. 100)\nCategorical variables\nEstimating differences between several means is the same as regressing on a categorical variable\nOne continuous predictor\n\ninterpretation of intercept and slope\ncentering of predictor to make intercept meaningful\n\n(Parameterization)\n\nReadings.\n\nGelman et al. (2021), Chapter 7-8, Chapter 9, section 9.1\n\n\n\n\n\n\nNo predictor. Estimating the mean and CI of a variable.\nOne indicator-variable predictor. Estimating the mean difference and CI between two groups of observations.\nOne categorical-variable predictor. Estimating the mean difference and CI between several groups of observations.\nOne continuous predictors. Estimating the slope of a linear relationship, i.e.  the difference in means between groups differing with one unit on the predictor.\n\n\n\n\nData set kidiq.txt\nWe will use the data set in kidiq.txt to illustrate several points below. The data is from a survey of adult American women and their children. Gelman et al. (2021) describe the data set and use it at several places, e.g., pp. 130-136, 156-158, 161, 185-187, and 197.\nCode book:\n\nkid_score  Test score children, on IQ-like measure (in the sample with mean and standard deviation of 87 and 20 points, respectively).\nmom_hs  Indicator variable: mother did complete high-school (1) or not (0)\nmom_iq  Test score mothers, transformed to IQ score: mean = 100, sd = 15.\nmom_work  Working status of mother, coded 1-4:\n\n1: mother did not work in first three years of child’s life,\n2: mother worked in second or third year of child’s life,\n3: mother worked part-time in first year of child’s life,\n4: mother worked full-time in first year of child’s life.\n\nmom_age Age of mothers, years\n\n\n\nCode\nd &lt;- read.table(\"./datasets/kidiq.txt\", sep = \",\", header = TRUE)\nstr(d)\n\n\n'data.frame':   434 obs. of  5 variables:\n $ kid_score: int  65 98 85 83 115 98 69 106 102 95 ...\n $ mom_hs   : int  1 1 1 1 1 0 1 1 1 1 ...\n $ mom_iq   : num  121.1 89.4 115.4 99.4 92.7 ...\n $ mom_work : int  4 4 4 3 4 1 4 3 1 1 ...\n $ mom_age  : int  27 25 27 25 27 18 20 23 24 19 ...",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#intercept-only-model",
    "href": "10-stat25.html#intercept-only-model",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "10.1 Intercept-only model",
    "text": "10.1 Intercept-only model\n\nEstimating the mean is the same as regressing on a constant term\nRegression analysis can be used to estimate a population mean and standard deviation from a sample, suing an intercept-only model:\n\\(y_i \\sim N(\\mu, \\sigma)\\)\n\\(\\mu = b_0\\), where \\(b_0\\) is a constant (intercept).\nSample size, mean and standard deviation for the kid_score variable:\n\n\nCode\nsample_stats &lt;- c(n = length(d$kid_score), mean = mean(d$kid_score), \n            sd = sd(d$kid_score))\nround(sample_stats, 1)\n\n\n    n  mean    sd \n434.0  86.8  20.4 \n\n\nHere is estimates of population \\(\\mu\\) and \\(\\sigma\\), using stan_glm():\n\n\nCode\nm0 &lt;- stan_glm(kid_score ~ 1, data = d, refresh = 0)\nprint(m0)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ 1\n observations: 434\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 86.8    1.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 20.4    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\nposterior_interval(m0, prob = 0.95)\n\n\n                2.5%    97.5%\n(Intercept) 84.81816 88.74237\nsigma       19.13409 21.85210\n\n\nTry lm(), it gives very similar estimates, but it does not provide a compatibility interval around \\(\\sigma\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#indicator-variables",
    "href": "10-stat25.html#indicator-variables",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "10.2 Indicator variables",
    "text": "10.2 Indicator variables\nIndicator variables are coded 1 (presence of something) and 0 (absence). In the kidIQ data, moms_hs is an indicator variable coded 1 if the mother completed high school and 0 otherwise. The mean of an indicator variable is the proportion of 1’s, so the mean of mom_hs is the proportion of mothers who completed high-school.\n\n\nCode\npar(mfrow = c(1, 2))\n# Plot frequncy table\nplot(table(d$mom_hs), \n     xlab = \"Indicator: mom_hs\\n0 = high-school not completed\\n1 = completed\", \n     ylab = \"Frequencies: mom_hs\", ylim = c(0, 434))\n# Plot proprtion table\nplot(table(d$mom_hs)/length(d$mom_hs),  # Proportions\n     xlab = \"Indicator: mom_hs\\n0 = high-school not completed\\n1 = completed\", \n     ylab = \"Proprtions: mom_hs\",\n     ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\ntable(d$mom_hs) # Frequency table\n\n\n\n  0   1 \n 93 341 \n\n\nCode\nround(table(d$mom_hs)/length(d$mom_hs), 3)  # Proportions\n\n\n\n    0     1 \n0.214 0.786 \n\n\nCode\nround(mean(d$mom_hs), 3)  # Mean indicator should be the same as proportion 1's\n\n\n[1] 0.786\n\n\n\n\nEstimating a difference between two means is the same as regressing on an indicator variable\nConsider this model:\n\\(y_i \\sim N(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1D_i\\), where \\(D_i\\) is an indicator variable coded 0 or 1.\nThe interpretation of the coefficients is simple:\n\n\\(b_0\\) is the mean of \\(y_i\\) for observations for which \\(D = 0\\).\n\nThat is, \\(b_0 = Mean(y_i | D = 0)\\)\n\n\\(b_1\\) is the difference in mean \\(y_i\\) between observations for which \\(D = 1\\) and \\(D = 0\\).\n\nThat is, \\(b_1 = Mean(y_i | D = 1) - Mean(y_i | D = 0)\\),\nso, \\(Mean(y_i | D = 1) = b_0 + b_1\\)\n\n\\(\\sigma\\) is the standard deviation of observations, assumed to be equal for observations in each group (homoscedasticity)\n\n\nHere applied to outcome variable kid_score (\\(y_i\\)) and indicator variable moms_hs (\\(D_i\\)):\n\n\nCode\n# Note: stan_glm() is a method based on data simulation, the seed argument set \n# the random number generator\nm1 &lt;- stan_glm(kid_score ~ mom_hs, data = d, refresh = 0, seed = 123)\nprint(m1)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs\n observations: 434\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 77.5    2.0  \nmom_hs      11.8    2.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 19.9    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\nposterior_interval(m1, prob = 0.95)\n\n\n                2.5%    97.5%\n(Intercept) 73.40918 81.54022\nmom_hs       7.26764 16.36398\nsigma       18.64532 21.28927\n\n\nInterpretation: On average, kids to mothers who completed high-school had 12 points higher scores than kids to mothers who did not complete high-school. Our data is compatible with a difference in the population between about 7 - 16 points, given the assumptions of our statistical model (independent observations, normal distributions with equal standard deviations).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#categorical-variables",
    "href": "10-stat25.html#categorical-variables",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "10.3 Categorical variables",
    "text": "10.3 Categorical variables\nCategorical variables code membership in categories for nominal scale variables. In the kidIQ data, the variable mom_work is a nominal variable coding the amount of work of mothers during their kid’s first three years. It is coded 1, 2, 3, and 4 for the four categories described above in the code book. This is a nominal scale value, although it can be discussed whether it also can be interpreted as an ordinal scale, from the least to the most amount of work during the first three years of child’s life (more information would be needed on the definition of the variable to decide this).\nThe categorical variable mom_work could be coded as three indicator variables, for example:\n\n\\(D_2\\) coded 1 if mom_work = 2 and 0 otherwise,\n\\(D_3\\) coded 1 if mom_work = 3 and 0 otherwise, and\n\\(D_4\\) coded 1 if mom_work = 4 and 0 otherwise.\n\nThe first category, mom_work = 1, would be implied for observations for which \\(D_2 = D_3 = D_4 = 0\\). In general, a categorical variable with \\(k\\) categories can be coded with \\(k-1\\) indicator variables. However, in R it is possible to use a categorical variable (factor) to obtain the same thing. Here a boxplot of kid_score for each category of mom_work\n\n\nCode\n# Make mom_work a factor, and add labels (labels not needed, but helpful)\nd$mom_work_f &lt;- factor(d$mom_work, \n                  levels = c(1, 2, 3, 4),\n                  labels = c(\"1_no\", \"2_only_y2_y3\",\"3_part_y1\", \"4_full_y1\"))\nboxplot(d$kid_score ~ d$mom_work_f, xlab = \"Categorical variable: mom_work\",\n        ylab = \"Kids score on intelligence test\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\nEstimating differences between several means is the same as regressing on a categorical variable\nR will analyse a categorical variable (factor) with \\(k\\) categories as an analysis with \\(k-1\\) indicator variables. The analysis will estimate one regression coefficient for each category except the first category (reference). The regression coefficients refer to the mean difference between the group defined by the category and the group defined by the reference category. The intercept estimates the mean of the reference category.\nHere an example predicting kid_score from mom_work using stan_glm():\n\n\nCode\n# Use factor, defined previous code block\nm2 &lt;- stan_glm(kid_score ~ mom_work_f, data = d, refresh = 0, seed = 123)\nprint(m2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_work_f\n observations: 434\n predictors:   4\n------\n                       Median MAD_SD\n(Intercept)            82.1    2.3  \nmom_work_f2_only_y2_y3  3.7    3.1  \nmom_work_f3_part_y1    11.4    3.5  \nmom_work_f4_full_y1     5.2    2.6  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 20.3    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\nposterior_interval(m2, prob = 0.95)\n\n\n                             2.5%     97.5%\n(Intercept)            77.4093836 86.486748\nmom_work_f2_only_y2_y3 -2.3954141  9.963632\nmom_work_f3_part_y1     4.3567994 18.604737\nmom_work_f4_full_y1    -0.1811025 10.592623\nsigma                  18.9977610 21.716944\n\n\nThe reference category (1: no work) had a mean of around 82 points. Categories 2, 3, and, 4 had higher scores the first category, about 4, 11, and 5 points, respectively.\nYou can change the reference category by redefining the factor:\n\n\nCode\n# Make 4: full-time work the reference category. \nd$mom_work_f &lt;- factor(d$mom_work, \n                  levels = c(4, 1, 2, 3),\n                  labels = c(\"4_full_y1\", \"1_no\", \"2_only_y2_y3\",\"3_part_y1\"))\nm2 &lt;- stan_glm(kid_score ~ mom_work_f, data = d, refresh = 0, seed = 123)\nprint(m2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_work_f\n observations: 434\n predictors:   4\n------\n                       Median MAD_SD\n(Intercept)            87.2    1.4  \nmom_work_f1_no         -5.2    2.8  \nmom_work_f2_only_y2_y3 -1.4    2.5  \nmom_work_f3_part_y1     6.3    3.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 20.2    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThe reference category (4: full time work) had a mean of around 87 points. Categories 1 and 2 had lower scores than category 4, whereas category 3 had higher scores.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#one-continous-predictor",
    "href": "10-stat25.html#one-continous-predictor",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "10.4 One continous predictor",
    "text": "10.4 One continous predictor\nEstimating the relationship between Mothers IQ-score (mom_iq) and kid_score\n\n\nCode\nm3 &lt;- stan_glm(kid_score ~ mom_iq, data = d, refresh = 0, seed = 123)\nprint(m3)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_iq\n observations: 434\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 25.8    5.9  \nmom_iq       0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.3    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nPlot data and model prediction\n\n\nCode\n# Scatter plot\nplot(d$mom_iq, d$kid_score, \n     xlab = \"Mother's IQ-score\", \n     ylab = \"Kid's score on IQ-related measure\")\nabline(m3$coefficients)\n\n\n\n\n\n\n\n\n\n\n\nCentering of predictor to make intercept meaningful\nIn the model above, the intercept = 25.81 is not meaningful as it refer to the kid_score of a kid to a mother with an IQ-score = 0 (an impossible value). Centering the predictor:\nd$mom_iq_centered &lt;- d$mom_iq - mean(d$mom_iq)\nmake the intercept meaningful: It is the kid_score of a kid to a mother with a mean IQ-score. Note that the slope remains the same.\n\n\nCode\n# Center the predictor variable\nd$mom_iq_centered &lt;- d$mom_iq - mean(d$mom_iq)\n\n# Fit model\nm3center &lt;- stan_glm(kid_score ~ d$mom_iq_centered, \n                     data = d, refresh = 0, seed = 123)\nprint(m3center)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ d$mom_iq_centered\n observations: 434\n predictors:   2\n------\n                  Median MAD_SD\n(Intercept)       86.8    0.9  \nd$mom_iq_centered  0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.3    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\n# Scatter plot\nplot(d$mom_iq_centered, d$kid_score, \n     xlab = \"Mother's IQ-score, centered around mean(IQ-score)\", \n     ylab = \"Kid's score on IQ-related measure\")\nabline(m3center$coefficients)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#advanced-parameterization",
    "href": "10-stat25.html#advanced-parameterization",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "10.5 Advanced: Parameterization",
    "text": "10.5 Advanced: Parameterization\nA model can often be written in several ways, with different sets of parameters. The models, or parameterizations, are mathematically equivalent, but one parameterization may be more useful than another.\nAs an example, consider these two parameterizations of the same model:\n\nm1:\n\n\\(y_i \\sim N(\\mu_i, \\sigma)\\)\n\n\\(\\mu_i = b_0 + b_1D_i\\), where \\(D_i\\) is an indicator variable coded 0 or 1.\n\nm1b:\n\n\\(y_i \\sim N(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0D_{1i} + b_1D_{2i}\\), where\n\n\\(D_{1}\\) is an indicator variable where 1 means that the mother did not complete high school,\n\\(D_{2}\\) is an indicator variable where 1 means that the mother did complete high school.\n\n\n\nIn parameterization m1, \\(b_1\\) refer to a difference between means, whereas in m1b, \\(b_1\\) refer to the mean of observations for which \\(D = 1\\). In both parameterizations, \\(b_0\\) refer to the mean of observations for which \\(D = 0\\).\nIf your goal is to estimate group means rather than differences between means,\nparameterization m1b may be your choice. Note that m1b still assume equal standard deviations. (If you don’t like this assumption, then just fit an intercept-only model separately to each group of kids.)\nHere the two versions of the model are fitted using glm():\n\n\nCode\nd$mom_hs0 &lt;- 1 * (d$mom_hs == 0)  # New indicator variable\n\n# Model with intercept and one indicators\nm1_lm &lt;- glm(kid_score ~ mom_hs, data = d)\n\n# Model with no intercept and two indicators\nm1b_lm &lt;- glm(kid_score ~ 0 + mom_hs0 + mom_hs, data = d)\n\n# Model outputs\nsummary(m1_lm)\n\n\n\nCall:\nglm(formula = kid_score ~ mom_hs, data = d)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 394.1231)\n\n    Null deviance: 180386  on 433  degrees of freedom\nResidual deviance: 170261  on 432  degrees of freedom\nAIC: 3829.5\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nsummary(m1b_lm)\n\n\n\nCall:\nglm(formula = kid_score ~ 0 + mom_hs0 + mom_hs, data = d)\n\nCoefficients:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nmom_hs0   77.548      2.059   37.67   &lt;2e-16 ***\nmom_hs    89.320      1.075   83.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 394.1231)\n\n    Null deviance: 3450038  on 434  degrees of freedom\nResidual deviance:  170261  on 432  degrees of freedom\nAIC: 3829.5\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nYou may of course do the same thing with stan_glm(). But it may not be necessary, as you can use it’s parameter estimates to derive new parameters. Here an illustration, using parameter estimates from model m1:\n\\(y_i \\sim N(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1D_i\\),\nto derive point estimate and 95 % confidence interval for the mean of the group \\(D = 1\\), that is, \\(b_0 + b_1\\), for the kidIQ data.\nSamples from the posterior:\n\n\nCode\n# Fit model using glm_stan()\nm1 &lt;- stan_glm(kid_score ~ mom_hs, data = d, refresh = 0, seed = 123)\n\n# Save samples from the posterior distribution\nsamples &lt;- as.data.frame(m1)\nnames(samples) &lt;- c(\"b0\", \"b1\", \"sigma\")  # rename column names\n\n# Derive new estimate: b0 + b1\nsamples$mean_g1 &lt;- samples$b0 + samples$b1\nhead(samples)\n\n\n        b0        b1    sigma  mean_g1\n1 79.26794 10.223293 19.09675 89.49123\n2 80.57249 10.290838 19.24931 90.86332\n3 79.01779  9.521749 19.85170 88.53954\n4 76.58362 13.120297 20.32644 89.70392\n5 77.75065 12.179655 19.70027 89.93030\n6 77.85677 12.065890 19.95251 89.92266\n\n\nEstimates from glm_stan():\n\n\nCode\n# median and 95 % CI for group 1 (mom_hs = 1)\ng1posterior &lt;- quantile(samples$mean_g1, probs = c(0.025, 0.5, 0.975))\ng1posterior\n\n\n    2.5%      50%    97.5% \n87.21719 89.30087 91.43292 \n\n\nCode\n# Plot posterior, with point estimate and 95 % CI\nplot(density(samples$mean_g), main = \"\", \n     xlab = \"Estimated mean IQ-score for kids to \\nmothers with completed high-school\", \n     ylab = \"Posterior probabbility\")\nlines(x = c(g1posterior[1], g1posterior[3]), y = c(0.01, 0.01),\n      col = \"blue\")\npoints(g1posterior[2], 0.01, pch = 21, bg = \"lightblue\")\n\n\n\n\n\n\n\n\n\nThe estimated interval is almost identical to the estimates of parameterization m1b using glm() (or stan_glm())\nEstimates from glm(kid_score ~ 0 + mom_hs0 + mom_hs, data = d)\n\n\nCode\nm1b &lt;- glm(glm(kid_score ~ 0 + mom_hs0 + mom_hs, data = d))\npoint_est &lt;- m1b$coefficients[2]\nnames(point_est) &lt;- \"\"\nci95lo &lt;- confint(m1b)[2, 1]\nci95hi &lt;- confint(m1b)[2, 2]\nm1b_estimates &lt;- c(ci95lo = ci95lo, point_est = point_est, ci95hi = ci95hi)\nround(m1b_estimates, 3)\n\n\n   ci95lo point_est    ci95hi \n   87.213    89.320    91.427 \n\n\n\nA similar strategy can be used with categorical variables: by excluding the intercept, regression coefficients refer to mean values for each category. Here illustrated using the four-category variable mom_work as predictor and kid_score as outcome variable, using stan_glm() and lm():\n\n\nCode\n# Model with no intercept yields estimates of means for each category\nm2b &lt;- stan_glm(kid_score ~ 0 + mom_work_f, data = d, refresh = 0, seed = 123)\nprint(m2b)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ 0 + mom_work_f\n observations: 434\n predictors:   4\n------\n                       Median MAD_SD\nmom_work_f4_full_y1    87.2    1.5  \nmom_work_f1_no         81.9    2.2  \nmom_work_f2_only_y2_y3 85.9    2.0  \nmom_work_f3_part_y1    93.5    2.6  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 20.2    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\nm2b_lm &lt;- lm(kid_score ~ 0 + mom_work_f, data = d)\nsummary(m2b_lm)\n\n\n\nCall:\nlm(formula = kid_score ~ 0 + mom_work_f, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-65.85 -12.85   2.79  14.15  50.50 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \nmom_work_f4_full_y1      87.210      1.413   61.72   &lt;2e-16 ***\nmom_work_f1_no           82.000      2.305   35.57   &lt;2e-16 ***\nmom_work_f2_only_y2_y3   85.854      2.065   41.58   &lt;2e-16 ***\nmom_work_f3_part_y1      93.500      2.703   34.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.23 on 430 degrees of freedom\nMultiple R-squared:  0.949, Adjusted R-squared:  0.9485 \nF-statistic:  2000 on 4 and 430 DF,  p-value: &lt; 2.2e-16\n\n\nCompare these estimates to the the sample means for each category:\n\n\nCode\naggregate(list(kid_score = d$kid_score), list(mom_work = d$mom_work_f), mean)\n\n\n      mom_work kid_score\n1    4_full_y1  87.20976\n2         1_no  82.00000\n3 2_only_y2_y3  85.85417\n4    3_part_y1  93.50000",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#practice",
    "href": "10-stat25.html#practice",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n10E1. Load the data in the file earnings.txt. It is data from a survey conducted 1990. This data set is used by Gelman et al. (2021) at several places. Please find a code book for this data set below the Practice section.\n\nWeight is given in pounds. Transform it to kilograms, and estimate average weight in the population using regression (intercept-only regression), report point estimate and 90 % compatibility interval.\nMake a histogram overlaid with a kernel density estimate of the observed weight [kg], and discuss in relation to assumptions of your model.\n\n\n\n\n\n10E2. Continue on 10E1, but now estimate the difference in weight between males and females by regressing on an indicator variable. Report point estimate and 90 % compatibility interval.\n\n\n\n10E3. Continue on 10E1, but now estimate the difference in weight between groups defined by ethnicity (Blacks, Whites, Hispanics, Other):\n\nWith Blacks s reference category.\nWith Whites as reference category.\n\n\n\n\n10E4. Below is estimated coefficients from glm(). What does the coefficients refer to?\n\n\n\nCode\nm &lt;- read.table(\"datasets/earnings.txt\", header = TRUE, sep = \",\")\nm$weight_kg &lt;- m$weight * 0.45359237\nm$ethnicity_f &lt;- factor(m$ethnicity)\nfit &lt;- glm(weight_kg ~ 0 + ethnicity_f, data = m)\nfit\n\n\n\nCall:  glm(formula = weight_kg ~ 0 + ethnicity_f, data = m)\n\nCoefficients:\n   ethnicity_fBlack  ethnicity_fHispanic     ethnicity_fOther  \n              72.63                68.96                64.91  \n   ethnicity_fWhite  \n              70.97  \n\nDegrees of Freedom: 1789 Total (i.e. Null);  1785 Residual\n  (27 observations deleted due to missingness)\nNull Deviance:      9434000 \nResidual Deviance: 438600   AIC: 14930\n\n\n\n\n\nMedium\n\n10M1. Below a scatter plot of the relationship between weight [kg] and age [years] from the earnings.txt data (used above in 10E1). The smooth function was added using the loess() function in R.\n\nBased on the plot and the red line, would linear regression seem useful to estimate the relationship?\nIf you could add one additional predictor to reduce the variability around the regression line, which one would you suggest?\n\n\n\n\nCode\nm &lt;- read.table(\"datasets/earnings.txt\", header = TRUE, sep = \",\")\nm$weight_kg &lt;- m$weight * 0.45359237\nplot(m$age, m$weight_kg, xlab = \"Age [years]\", ylab = \"Weight [kg]\")\n\n# Fit loess() line // lowess() did not work well here for some reason\nloess_fit &lt;- loess(weight_kg ~ age, data = m)\ndd &lt;- data.frame(age = seq(18, 90, length.out = 100))\npredicted &lt;- predict(loess_fit, newdata = dd)\nlines(dd$age, predicted, col = \"red\", lwd = 3, lty = 3)\n\n\n\n\n\n\n\n\n\n\n\n10M2. Below a scatter plot of the relationship between weight [kg] and age [years] for males &lt; 50 years old. The line was fitted using linear regression (glm(weight_kg ~ age)).\n\nEstimate by eye the intercept and slope of regression line.\nThe intercept makes little sense (why?), how would you fix that?\n\n\n\n\nCode\nmm &lt;- m[m$male == 1 & (m$age &lt; 50), c(\"weight_kg\", \"age\")]\nplot(mm$age, mm$weight_kg, xlab = \"Age [years]\", ylab = \"Weight [kg]\")\nmmfit &lt;- glm(weight_kg ~ age, data = mm)\nabline(mmfit, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n10M3. Below is a summary of a regression model for the data in 10M2, i.e., for men &lt; 50 years old. This time with the predictor centered at 20 years and expressed in decades: \\(age20dec = (age - 20)/10\\).\n\nGiven an approximate 95 % confidence around the regression coefficient for the age variable\nIn one sentence, describe how weight and age are related according to the regression model in terms of weight differences per decade, with reference to both point and interval estimate.\nReexpress (b), but now assume a causal relationship between age and weight.\nWould you assess the size of the age-effect on weight to be reasonable? Discuss.\n\n\n\n\nCode\nmm$age20dec &lt;- (mm$age - 20)/10\nmmfit &lt;- glm(weight_kg ~ age20dec, data = mm)\nsummary(mmfit)\n\n\n\nCall:\nglm(formula = weight_kg ~ age20dec, data = mm)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  73.9696     1.0516   70.34  &lt; 2e-16 ***\nage20dec      5.2100     0.7127    7.31 1.15e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 173.4478)\n\n    Null deviance: 91309  on 474  degrees of freedom\nResidual deviance: 82041  on 473  degrees of freedom\nAIC: 3801\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\n10M4. I wanted to explore whether smokers have lower weight than non-smokers. I used the data from above (earnings.txt, full sample) and used linear regression to fit this model:\n\\[weight_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i= b_0 + b_1 smoke_i\n\\]\nwhere \\(weight\\) is in kg and \\(smoke\\) is an indicator variable coded 1 if current smoker and 0 if not.\nBelow is a summary of the regression result. This time I used stan_glm() from the rstanarm package.\n\nExplain the meaning of the three estimated parameter values: \\(b_0, b_1, \\sigma\\).\nSummarize the result in a single sentence with respect to the hypothesis that smokers have lower weight than non-smokers, refer to both point and interval estimates of the coefficient for smoke. Provide two versions: (1) In terms of differences between groups, and (2) in terms of causal effect.\nThe result was in the expected direction: smokers had lower weight than non-smokers in this sample. Still, the causal interpretation (b) may still be wrong. Discuss potential confounders (some available in the data set, see code book below).\n\n\n\n\nCode\n# Indicator-coded smoking variable: 1 if smoker (&lt;7 cigs a week), 0 if not.  \n# smokenow in the original data set was defined as 1 for more than 7 cigs per \n# week, 2 if not.\nm$smoke &lt;- ifelse(m$smokenow == 1, 1, 0)\n\n# Fit model, note: refresh = 0 suppresses output to the console, not needed\nsmokefit &lt;- rstanarm::stan_glm(weight_kg ~ smoke, data = m, refresh = 0)\nprint(smokefit)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      weight_kg ~ smoke\n observations: 1788\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 71.4    0.4  \nsmoke       -2.0    0.9  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 15.7    0.3  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\n\nHard\n\n10H1. This exercise is a version of Exercise 7.7 from Gelman et al. (2021):\nDraw 100 values of the predictor x from a uniform distribution between 0 and 50. Then simulate 100 observations (\\(y_i\\), where \\(i = 1, 2, ..., 100\\)) from this model:\n\\(y_i \\sim Normal(\\mu_i = 2 + 5x_i, \\sigma = 10)\\)\nSave x and y in a data frame called fake and fit the model\nstan_glm(y ~ x, data = fake) (or use glm() instead).\n\nPlot data and regression line. Add regression equation to the plot.\nAre the estimated coefficients close to the assumed true values? Discuss a way of defining “close” in this context.\n\n\n\n\n10H2. This exercise is a version of Exercise 7.8 from Gelman et al. (2021):\nRepeat the simulation in 10H1 a thousand times (omit plotting). Check that the coefficient estimates (\\(b_0, b_1, \\sigma\\)) are centered around the true parameter values.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#data-set-earnings.txt",
    "href": "10-stat25.html#data-set-earnings.txt",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "Data set earnings.txt",
    "text": "Data set earnings.txt\nThe data set is from a survey on “Work, family, and well-being in the United States” conducted in 1990. Gelman et al. use it as several places, for example, pp. 84-85 and 189-195. The data set provided by Gelman et al. is a selection of variables from the original study, and they have excluded respondents with missing data on height or earnings (see footnote 4, page 84). I managed to reconstruct a code book of the variables in the data set from the surveys code book, available at the Gelman et al’s web site.\n\nCode book\n\nheight – Height [inches]\nweight – Wight [pounds]\nmale – Sex: male = 1, female = 0\nearn – Personal income in 1989 before taxes [USD]\nearnk – Personal income in 1989 before taxes [kUSD]\nethnicity – Black, Hispanic, White, or Other\neducation – Highest grade or year of school:\n\n0 None\n1-8 Elementary school\n9-12 High school\n14-16 College\n17 Some graduate school\n18 Graduate or professional degree\n99 Don’t know\n\nmother_education – Highest grade or year of school mother. Code as for education.\nfather_education – Highest grade or year of school father. Code as for education.\nwalk – Frequency of talking a walk, including walking to work, train station, etc.\n\n1 Never\n2 Once a month or less\n3 About twice a week\n4 About once a week\n5 Twice a week\n6 Three times a week\n7 More than 3 times a week\n8 Every day\n\nexercise – Frequency of strenuous exercising. A bit unclear how this was measured, but I guess it was a categorical scale from rarely (1) to often (7).\nsmokenow – Smoke 7 or more cigarettes a week. 1 = yes, 2 = no.\ntense – On how many of the past 7 days have you felt tense or anxious? 0-7.\nangry – On how many of the past 7 days have you felt angry?\nage – Age [years]\n\n\nFollow this link to find data Save (Ctrl+S on a PC) to download as text file",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "10-stat25.html#session-info",
    "href": "10-stat25.html#session-info",
    "title": "10  S: Linear Regression: Single Predictor",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] rstanarm_2.32.1 Rcpp_1.0.14    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     dplyr_1.1.4          farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] shinystan_2.6.0      promises_1.3.3       shinyjs_2.1.0       \n[10] digest_0.6.37        mime_0.13            lifecycle_1.0.4     \n[13] StanHeaders_2.32.10  survival_3.7-0       magrittr_2.0.3      \n[16] posterior_1.6.1      compiler_4.4.2       rlang_1.1.6         \n[19] tools_4.4.2          igraph_2.1.4         yaml_2.3.10         \n[22] knitr_1.50           htmlwidgets_1.6.4    pkgbuild_1.4.8      \n[25] curl_6.4.0           plyr_1.8.9           RColorBrewer_1.1-3  \n[28] dygraphs_1.1.1.6     abind_1.4-8          miniUI_0.1.2        \n[31] grid_4.4.2           stats4_4.4.2         xts_0.14.1          \n[34] xtable_1.8-4         inline_0.3.21        ggplot2_3.5.2       \n[37] scales_1.4.0         gtools_3.9.5         MASS_7.3-61         \n[40] cli_3.6.5            rmarkdown_2.29       reformulas_0.4.1    \n[43] generics_0.1.4       RcppParallel_5.1.10  rstudioapi_0.17.1   \n[46] reshape2_1.4.4       minqa_1.2.8          rstan_2.32.7        \n[49] stringr_1.5.1        shinythemes_1.2.0    splines_4.4.2       \n[52] bayesplot_1.13.0     parallel_4.4.2       matrixStats_1.5.0   \n[55] base64enc_0.1-3      vctrs_0.6.5          V8_6.0.4            \n[58] boot_1.3-31          Matrix_1.7-1         jsonlite_2.0.0      \n[61] crosstalk_1.2.1      glue_1.8.0           nloptr_2.2.1        \n[64] codetools_0.2-20     distributional_0.5.0 DT_0.33             \n[67] stringi_1.8.7        gtable_0.3.6         later_1.4.2         \n[70] QuickJSR_1.8.0       lme4_1.1-37          tibble_3.3.0        \n[73] colourpicker_1.3.0   pillar_1.10.2        htmltools_0.5.8.1   \n[76] R6_2.6.1             Rdpack_2.6.4         evaluate_1.0.3      \n[79] shiny_1.11.1         lattice_0.22-6       markdown_2.0        \n[82] rbibutils_2.3        backports_1.5.0      threejs_0.3.4       \n[85] httpuv_1.6.16        rstantools_2.4.0     gridExtra_2.3       \n[88] nlme_3.1-166         checkmate_2.3.2      xfun_0.52           \n[91] zoo_1.8-14           pkgconfig_2.0.3     \n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>S: Linear Regression: Single Predictor</span>"
    ]
  },
  {
    "objectID": "11-method25.html",
    "href": "11-method25.html",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "11-method25.html#topics",
    "href": "11-method25.html#topics",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "",
    "text": "Non-compliance and missing data: Why they may introduce bias in randomized experiments (DAGs are helpful to illustrate)\nComplier types\n\nCompliers\nAlways takers\nNever takers\nDefiers\n\nBroken experiments: Analytic approaches.\n\nNon-compliance\n\nAs treated (AT) analysis\nPer protocol (PP) analysis\nIntention to treat (ITT) analysis\nTreatment effect bounding\n\nMissing data\n\nComplete cases analysis. Understand risk of bias.\nLast observation carried forward. Understand risk of bias.\nMean or regression based imputation of single values. Understand risk of bias.\n(Multiple imputation: a better alternative but not covered in this course)\n\n\n\nTheoretical articles to read:\n\nGelman et al. (2021), chapter 18, sections 18.3 - 18.6. Understand SATE and PATE, and tables 18.3-5. (For the interested reader, Chapter 17 of Gelman et al’s book has a nice discussion of missing data, however, this chapter is not part of the course.)\nSagarin et al. (2014), Focus on: ITT, AT and PP analysis (pp. 317 320, 327 328), and the four patterns of compliance (p. 321).\nRohrer (2018) is an excellent introduction to DAGs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "11-method25.html#randomized-experiment-how-can-i-loose",
    "href": "11-method25.html#randomized-experiment-how-can-i-loose",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "11.1 Randomized experiment: How can I loose?",
    "text": "11.1 Randomized experiment: How can I loose?\nThe randomized experiment is often labeled the “Gold standard” of causal inference. A bit undeservedly if you ask me:\n\nSingle-unit causal effects still unknown. Remember: SATE (or PATE) close to zero does not imply absence of single-unit causal effects; moderate or small SATE (or PATE) consistent with substantial single-unit causal effect. Single-unit causal effects can only be inferred given strong assumptions. For example, assumptions about normally distributed single-unit causal effects would imply that the average causal effect is representative of the typical individual, or the assumption of constant treatment effect (West & Thoemmes (2010), p. 23) would imply that the average causal effect is representative of every individual (unrealistic for most behavioral science phenomena, people react differently to stimuli and treatments).\nNon-compliance may introduce systematic error (as discussed below), and internal validity issues, as well as issues with external validity (results generalizes only to compliers).\nDrop out may introduce systematic error (as discussed below), and internal validity issues, as well as issues with external validity (results generalizes only to non-drop outs).\nConstruct validity issues: What was the mechanism of the observed effect? Threats include Placebo and Hawthorne effects (e.g., Gelman et al. (2021), p. 354).\nRandomization error (remedy: large n). May lead to discrepancy between SATE and prima-facie estimate, due to randomization errors and thereby issues with internal validity.\nSampling error (remedy: large n). May lead to discrepancy between SATE and PATE, due to sampling error and thereby issues with external validity.\n\nTo be fair, some of the above applies also to within-subject experiments, including\nSingle-N designs (my “Gold standard”).\n\nBelow I will use Directed Acyclical Graphs (DAGs) to illustrate how confounding may cause problems even in large randomized experiments with non-compliance or attrition.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "11-method25.html#non-compliance",
    "href": "11-method25.html#non-compliance",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "11.2 Non-compliance",
    "text": "11.2 Non-compliance\nProblem: Confounding by unmeasured variable causing both treatment compliance and outcome.\n\nRandomized experiment\n\n\nCode\nlibrary(dagitty) # R version of http://www.dagitty.net\n\nrandomized_exp &lt;- dagitty( \"dag {\n   Random -&gt; Treatment\n   Treatment -&gt; Outcome\n   Z -&gt; Outcome\n}\")\n\ncoordinates(randomized_exp) &lt;- list(\n  x = c(Random = 1, Treatment = 2, Z = 2.5, Outcome = 3),\n  y = c(Random = 2, Treatment = 2, Z = 1.5, Outcome = 2))\n\nplot(randomized_exp)\n\n\n\n\n\n\n\n\n\n\nBroken randomized experiment (treatment has a causal effect)\n\n\nCode\nbroken_exp &lt;- dagitty( \"dag {\n   Random -&gt; Treatment\n   Treatment -&gt; Outcome\n   Z -&gt; Treatment -&gt; Outcome\n   Z -&gt; Outcome\n}\")\n\ncoordinates(broken_exp) &lt;- list(\n  x = c(Random = 1, Treatment = 2, Z = 2.5, Outcome = 3),\n  y = c(Random = 2, Treatment = 2, Z = 1.5, Outcome = 2))\n\nplot(broken_exp)\n\n\n\n\n\n\n\n\n\n\nBroken randomized experiment (treatment has no causal effect)\n\n\nCode\nbroken_exp2 &lt;- dagitty( \"dag {\n   Random -&gt; Treatment\n   Z -&gt; Treatment\n   Z -&gt; Outcome\n}\")\n\ncoordinates(broken_exp2) &lt;- list(\n  x = c(Random = 1, Treatment = 2, Z = 2.5, Outcome = 3),\n  y = c(Random = 2, Treatment = 2, Z = 1.5, Outcome = 2))\n\nplot(broken_exp2)\n\n\n\n\n\n\n\n\n\n\nThe problem is that \\(Z\\) in the broken experiment scenarios may introduce bias. One way to think about this is offered by the potential outcome perspective, from which we may define four “causal types”. Think of an experiment in which kids are randomly assigned to “music lesson” (treatment) or “no music lessons” (control), cf. Schellenberg (2004).\n\nCompliers will do as we say: if assigned to “music lesson” they will take music lessons, if assigned to “no music lessons” they will not take music lessons.\nAlways-takers will always take the treatment: if assigned to “music lesson” they will take music lessons, if assigned to “no music lessons” they will nevertheless take music lessons.\nNever-takers will never take the treatment: if assigned to “music lesson” they will not take music lessons, and same if they were assigned to “no music lessons”.\nDefiers will never do as we say: if assigned to “music lesson” they will not take music lessons, if assigned to “no music lessons” they will take music lessons.\n\nWe usually assume that there are no defiers. Sometimes it is also possible to assume no always takers, if, for instance, we are administrating the music lessons we may make sure to only let in those who were assigned to treatment (but you never know, maybe an always-taker would buy similar music lessons from somewhere else).\nIt might be that the possible confounder \\(Z\\) may lead to unbalanced groups, for instance, \\(Z\\) might be socioeconomic status (SES), and maybe there are more never-takers among people with low compared to high SES. This may lead to lower SES among the non-treated than the treated, and this may look as an effect if treatment on the outcome. In Schellenberg (2004), the outcome was IQ scores, a variable that probably is related to SES.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "11-method25.html#analytic-approaches-to-non-compliance",
    "href": "11-method25.html#analytic-approaches-to-non-compliance",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "11.3 Analytic approaches to non-compliance",
    "text": "11.3 Analytic approaches to non-compliance\nSagarin et al. (2014)\n\nIntention-to-treat\nAs-treated\nPer-protocol\nInstrumental variable (not covered in this course)\nDose-response estimation (not covered in this course)\nPropensity score analysis (not covered in this course)\nTreatment effect bounding (briefly exemplified below)\n\n\nSimple example:\nRandomized experiment, 500 to Control group (do nothing), 500 to Experimental group (participate in training program).\nNon-compliance: 100 participants in the treatment group did not take the treatment, and those were at high risk of mortality for other reasons (confounder Z).\nData\n\n\n\n\n\n\n\n\n\n\nSurvived\nDead\nRow sum\n\n\n\n\nControl\n450\n50\n500\n\n\nTreatment\n380\n20\n400\n\n\nControl non-complier\n80\n20\n100 (100 non-compliers, assigned to treatment)\n\n\nColumn sum\n910\n90\n1000\n\n\n\n\nIntention-to-treat\n\n\n\n\nSurvived\nDead\nRow sum\n\n\n\n\nControl\n450\n50\n500\n\n\nTreatment\n380 + 80\n20 + 20\n500 (400 + 100 non-compliers)\n\n\nColumn sum\n910\n90\n1000\n\n\n\n\n\nCode\n# Relative risk and risk difference Control vs. Treated\nRD &lt;- c(RD = (50/500) - (40/500))\nRR &lt;- c(RR = (50/500) / (40/500))\nround(c(RR, RD), 3)\n\n\n  RR   RD \n1.25 0.02 \n\n\n\nAs-treated analysis: Risk mortality in Control group vs. Treatment group\n\n\n\n\nSurvived\nDead\nRow sum\n\n\n\n\nControl\n450 + 80\n50 + 20\n600 (500 + 100 non-compliers)\n\n\nTreatment\n380\n20\n400\n\n\nColumn sum\n910\n90\n1000\n\n\n\n\n\nCode\n# Relative risk and risk difference Control vs. Treated\nRD &lt;- c(RD = (70/600) - (20/400))\nRR &lt;- c(RR = (70/600) / (20/400))\nround(c(RR, RD), 3)\n\n\n   RR    RD \n2.333 0.067 \n\n\nPer-protocol analysis: Risk mortality in Control group vs. Treatment group\n\n\n\n\nSurvived\nDead\nRow sum\n\n\n\n\nControl\n450\n50\n500\n\n\nTreatment\n380\n20\n400\n\n\nColumn sum\n830\n70\n900\n\n\n\n\n\nCode\n# Relative risk and risk difference Control vs. Treated\nRD &lt;- c(RD = (50/500) - (20/400))\nRR &lt;- c(RR = (50/500) / (20/400))\nround(c(RR, RD), 3)\n\n\n  RR   RD \n2.00 0.05 \n\n\n\nHypotetical Data 1 (best case)\nAssuming that all non-compliers (never-takers) would have survived had they taken the treatment\n\n\n\n\n\n\n\n\n\n\nSurvived\nDead\nRow sum\n\n\n\n\nControl\n450\n50\n500\n\n\nTreatment\n380 + 100\n20\n500 (400 + 100 surviving never-takers)\n\n\nColumn sum\n930\n70\n1000\n\n\n\nUpper bound: Risk mortality in Control group vs. Treatment group\n\n\nCode\n# Relative risk and risk difference Control vs. Treated\nRDhi &lt;- c(RDhi = (50/500) - (20/500))\nRRhi &lt;- c(RRhi = (50/500) / (20/500))\nround(c(RRhi, RDhi), 3)\n\n\nRRhi RDhi \n2.50 0.06 \n\n\nHypotetical Data 2 (worst case)\nAssuming that all never-takers would have died had they taken the treatment\n\n\n\n\n\n\n\n\n\n\nSurvived\nDead\nRow sum\n\n\n\n\nControl\n450\n50\n500\n\n\nTreatment\n380\n20 + 100\n500 (400 + 100 non-surviving never takers)\n\n\nColumn sum\n830\n170\n1000\n\n\n\nLower bound: Risk mortality in Control group vs. Treatment group\n\n\nCode\n# Relative risk and risk difference Control vs. Treated\nRDlo &lt;- c(RDlo = (50/500) - (120/500))\nRRlo &lt;- c(RRlo = (50/500) / (120/500))\nround(c(RRlo, RDlo), 3)\n\n\n  RRlo   RDlo \n 0.417 -0.140",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "11-method25.html#missing-data",
    "href": "11-method25.html#missing-data",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "11.4 Missing data",
    "text": "11.4 Missing data\nMissing data may lead to similar problem as non-compliance, through adjusting for a collider variable and thereby introducing bias (often called attrition bias).\n\n\n\n\n\n\n\n\n\n\nClassification of missing data:\n\nMissingness completely at random (MCAR). The best scenario, probability of missingness is the same for all study units regardless of background variables. No bias related to missingness.\nMissingness at random (MAR). The second best scenario: Probability of missingness depend only on observed variables, missing cases can be excluded if these observed variables are adjusted for.\nMissingness not at random (MNAR). The worst scenario. Missingness related to unobserved variables, potentially inducing bias in causal effect estimates.\n\n\nCommon methods for dealing with missing data in randomized experiments (and elsewhere):\n\nComplete cases. Analyse only cases for which there is no missing data on the outcome variable. If the experiment involve measures before and after treatment, this would require data on both. May lead to bias if missingness is not random, but is related to unmeasured confounders and treatment.\nLast value carried forward. A common method, where missing data after treatment (“lost to follow up”) is replaced (imputed) with the before treatment measure. May lead to bias if missingness is not random, but related to unmeasured confounders or to treatment.\nMean imputation. Imputation of the mean of of the observed values for variable with missing data. May lead to an underestimation of the variable’s variance.\nPredicted value Impute predicted value based on a regression analysis of participants with full data sets. Similar problem as mean imputation.\n(Advanced methods, e.g., multiple imputation. not covered by this course. Generally viewed as superior to the above methods)\n\nFor more on missing data, see Gelman et al. (2021) Chapter 17, sections 3-6 (discussed further in the Statistics course).\n\n\nSimple simulation of how missing data may introduce bias\nThis DAG show a scenario with no treatment effect and drop out related to a covariate Z, leading to attrition bias.\n\n\n\n\n\n\n\n\n\n\nHere a simulation of this scenario.\n\n\nCode\nset.seed(999)\n\n## Generate treatment and covariate variables\nn &lt;- 400  # Total sample size\nrand &lt;- sample(c(0, 1), size = n, replace = TRUE)  # Randomization\ntreat &lt;- rand  # In this simulation there are only compliers\nz &lt;- rnorm(n)  # Standardized covariate, normal distribution, mean = 0, sd = 1\n\n## Generate drop out and dependent variable (y) \n# Everyone in treatment group low on z drops out\ndropout &lt;- 1 * (treat == 1 & z &lt; -1) \n# True data: including dropouts. Outcome related to z, but not to treatment\nytrue &lt;- rnorm(n) + 3 * z   \n# Observed data: Dropouts = NA\nyobs &lt;- ytrue\nyobs[dropout == 1] &lt;- NA\n\n## Put data in data frame\nd &lt;- data.frame(rand, treat, z, dropout, ytrue, yobs)  # Make data frame\nsummary(d)\n\n\n      rand           treat             z                dropout     \n Min.   :0.000   Min.   :0.000   Min.   :-2.588446   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:-0.711258   1st Qu.:0.000  \n Median :1.000   Median :1.000   Median : 0.003622   Median :0.000  \n Mean   :0.515   Mean   :0.515   Mean   :-0.023033   Mean   :0.085  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.: 0.655788   3rd Qu.:0.000  \n Max.   :1.000   Max.   :1.000   Max.   : 3.061137   Max.   :1.000  \n                                                                    \n     ytrue               yobs       \n Min.   :-9.12489   Min.   :-7.010  \n 1st Qu.:-2.13428   1st Qu.:-1.401  \n Median : 0.03864   Median : 0.388  \n Mean   :-0.08352   Mean   : 0.359  \n 3rd Qu.: 2.00790   3rd Qu.: 2.235  \n Max.   :10.13607   Max.   :10.136  \n                    NA's   :34      \n\n\nCode\ntable(treatment = d$treat, dropout = d$dropout)\n\n\n         dropout\ntreatment   0   1\n        0 194   0\n        1 172  34\n\n\n\nAnalyze “true data”, that is, assuming we know the values for the drop outs. No suggested treatment effect, as expected given our simulation.\n\n\nCode\n## Analyze \"true\" data, i.e. assuming we know values for drop outs\nm_true &lt;- lm(ytrue ~ treat, data = d)  # or t.test(g$y ~ g$treat, var.equal = TRUE)\nm_true\n\n\n\nCall:\nlm(formula = ytrue ~ treat, data = d)\n\nCoefficients:\n(Intercept)        treat  \n   -0.07720     -0.01226  \n\n\nCode\nconfint(m_true)\n\n\n                 2.5 %    97.5 %\n(Intercept) -0.5144118 0.3600022\ntreat       -0.6214896 0.5969766\n\n\n\nAnalyze observed data, drop outs not included. Strong treatment effect suggested, this because we unintentionally controlled for “dropout” (analyses conditional on dropout = 0) and thereby introduced bias through covariate Z (so called “collider bias”)\n\n\nCode\n## Analyze observed data, drop outs excluded\ntable(treatment = d$treat[d$dropout == 0], dropout = d$dropout[d$dropout == 0])\n\n\n         dropout\ntreatment   0\n        0 194\n        1 172\n\n\nCode\nm_observed &lt;- lm(yobs ~ treat, data = d)  # lm() excludes rows with NA\nm_observed\n\n\n\nCall:\nlm(formula = yobs ~ treat, data = d)\n\nCoefficients:\n(Intercept)        treat  \n    -0.0772       0.9283  \n\n\nCode\nconfint(m_observed)\n\n\n                 2.5 %    97.5 %\n(Intercept) -0.4705239 0.3161144\ntreat        0.3545671 1.5020637\n\n\n\nIn this simple scenario, the bias is removed if we control for the covariate Z, for example, by adding it as a covariate in the regression analyses\n\n\nCode\n## Adjusting for the covariate eliminates bias \nm_observed2 &lt;- lm(yobs ~ treat + z, data = d)\nm_observed2\n\n\n\nCall:\nlm(formula = yobs ~ treat + z, data = d)\n\nCoefficients:\n(Intercept)        treat            z  \n   0.008107    -0.007058     2.944561  \n\n\nCode\nconfint(m_observed2)\n\n\n                 2.5 %    97.5 %\n(Intercept) -0.1349844 0.1511988\ntreat       -0.2190974 0.2049811\nz            2.8260857 3.0630357\n\n\n\n\n\nSurvivorship bias\nThe attrition bias discussed above is an example of survivorship bias, a specific form of selection bias that occurs when only the “survivors” or successful outcomes are considered, while failures or non-survivors are excluded from analysis.\nA classic example involves the analysis of aircraft damage during World War II. Initially, it was suggested to reinforce the areas of returning planes that showed the most damage. However, statistician Abraham Wald proposed reinforcing the vital areas that showed little to no damage. He recognized that planes with damage to these critical areas had not returned.\nAnother example is the analysis of successful musicians. Suppose a study shows that all of them practiced for more than 10,000 hours. It would be incorrect to conclude that “practicing for 10,000+ hours will make you a professional musician,” because the sample excludes those who practiced extensively but did not achieve success. By ignoring the “non-survivors” (those who did not make it despite the effort), the conclusion falsely attributes success solely to the number of practice hours, when other factors may play a role.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "11-method25.html#practice",
    "href": "11-method25.html#practice",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n11E1.\na. Explain with an example and a DAG how missing data may lead to bias of causal effect estimates in randomized experiments.\nb. Explain with an example and a DAG how non-compliance may lead to bias of causal effect estimates in randomized experiments.\n\n\n\n11E2.\nExplain why as-treated and per-protocol analysis of data from randomized experiments may be biased in situations with non-compliance, and explain in what way, if any, this problem is avoided using intention-to-treat analysis.\n\n\n\n11E3.\nTreatment effect bounding is an attempt to estimate an interval of possible effect sizes in cases of drop outs or non-compliance in randomized experiments. The general idea can be applied in other contexts: Assume that a course was given a mean rating of 8 on a scale from 1 (disaster) to 10 (excellent); 100 students took the course, but only 70 provided a rating. Calculate an interval of possible mean ratings had all students answered.\n\n\n\n11E4. Why might replacing missing data in a variable with its mean value seem like a reasonable approach, and what are the significant drawbacks of this method?\n\n\n\n11E5. Fooled by the Winners is a book by David Lockwood (2021, Austin, Texas: Greenleaf Book Group Press).\n\nBased on the title, what do you think the book might be about?\nConsider the following excerpt from the book: “In every town I have lived in, many restaurants seem to have been around for a long time, in some cases decades. This could lead one to believe that restaurants are a relatively reliable, stable business.” (p.45) Why might this conclusion be flawed?\n\n\n\n\n\nMedium\n\n11M1. Last-value-carried-forward is a common method for imputation of data lost to follow up.\n\nExplain how this method may lead to underestimation of the true effect.\n\nCan it also lead to overestimation?\n\n\n\n\n11M2. Answer a. and b. from 11M1 but now with regard to intention-to-treat (ITT) analyses.\n\n\n\n11M3. Above we defined four types of compliance types. A somewhat similar typology relates to outcomes in a scenario with a binary exposure (e.g., smoking yes/no) and a binary outcome (e.g., lung cancer yes/no): 1. Doomed, 2. Causal, 3. Preventive, 4. Immune.\n\nTry to figure out how these types are defined.\n\nThey are always unobserved. Explain.\n\n\n\n\n11M4. Explain the concepts of missing data mechanisms:\n\nMissing Completely at Random (MCAR),\nMissing at Random (MAR),\nMissing Not at Random (MNAR),\n\nusing an illustrative example based on an imagined randomized experiment? Include how each mechanism would impact data analysis and interpretation.\n\n\n\n\nHard\n\n11H1. Illustrate with a simulation how non-compliance may bias effect estimates, assume:\n\nA substantial proportion always-takers, no never-takers\nA substantial proportion never-takers, no always-takers\nSubstantial proportions always-takers and never-takers\n\n\n\n\n11H2. Illustrate with a simulation how missing data may or may not bias effect estimates in situations with complete cases analysis and:\n\nMissingness completely at random.\nMissingness at random.\nMissingness not at random.\n\n\n\n\n11H3. Rerun your simulations from 11H2, but now using one or several methods for data imputation.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "11-method25.html#session-info",
    "href": "11-method25.html#session-info",
    "title": "11  M: Broken Experiments and Quasi-experiments",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       yaml_2.3.10       rlang_1.1.6      \n[17] jsonlite_2.0.0    V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42.\n\n\nSagarin, B. J., West, S. G., Ratnikov, A., Homan, W. K., Ritchie, T. D., & Hansen, E. J. (2014). Treatment noncompliance in randomized experiments: Statistical approaches and design issues. Psychological Methods, 19(3), 317.\n\n\nSchellenberg, E. G. (2004). Music lessons enhance IQ. Psychological Science, 15(8), 511–514.\n\n\nWest, S. G., & Thoemmes, F. (2010). Campbell’s and rubin’s perspectives on causal inference. Psychological Methods, 15(1), 18.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>M: Broken Experiments and Quasi-experiments</span>"
    ]
  },
  {
    "objectID": "12-stat25.html",
    "href": "12-stat25.html",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#topics",
    "href": "12-stat25.html#topics",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "",
    "text": "Linear transformations. Discussed earlier, here some new examples\nMultiple regression\n\nCombining indicator and continuous predictors, see example with weight and age.\nAdd quadratic term to model non-linear trend\nSeveral continuous predictors.\n\nRegression diagnostics\nAssumptions behind multiple regression\n\nReadings:\n\nGelman et al. (2021), Chapter 10 on multiple predictors, Chapter 11 on assumptions and regression diagnostics, Chapter 12 on linear transformations (Sections 12.1 - 12.3).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#multiple-regression",
    "href": "12-stat25.html#multiple-regression",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "12.1 Multiple regression",
    "text": "12.1 Multiple regression\nBivariate regression involves a single predictor. Adding more than one predictor to the model makes it a multiple regression model. Multiple predictors may be included for at least two reasons:\n\nPrediction. Several predictors may improve the model’s ability to predict future observations\nCasual inference. If we are estimating the causal effect of an exposure variable on the outcome variable, adding additional predictors (“covariates”, i.e., potential confounders) may justify a causal interpretation of the regression coefficient for the exposure variable.\n\n\n\nData: sat_howell.txt\nHere an example of a simple multiple regression model with surprising reversal of trend compared to simple bivariate model. Data from Table 15.1 in Howell’s “Statistical Methods for Psychology” (8th ed).\nThe data is based on a paper by Guber (1999) regarding the relationship between state spendings on education and students performance on two standardized tests (SAT and ACT), data is from 1994-95. Data is from the 50 U.S. states (so unit of observation is state). SAT scores range from 200 to 800, the ACT range between 1 and 36. SAT has been characterized as mainly a test of ability, whereas the ACT is more of a test of material covered in school.\nCode book:\n\nid – id number of state\nState – Name of state\nExpend – Average annual expenditures per student (unit: 1000 USD)\nPTratio – Pupil/teacher ratio (average in state)\nSalary – Average annual salary for teachers (unit: 1000 USD)\nPctSAT – Percentage of students in the state that takes the SAT test\nVerbal – Average SAT score on the verbal part of tests\nMath – Average SAT score on the math part of tests\nSAT – Average combined SAT score\nPctACT – Percentage of students in the state that takes the ACT test\nACT – Average combined ACT score\n\n\nFollow this link to find data Save (Ctrl+S on a PC) to download as text file\n\n\n\nCode\nh &lt;- read.table(\"./datasets/sat_howell.txt\", header = TRUE, sep = ',')\n\n\n\nResearch question: Estimate the causal effect of State school spendings (expend) on SAT-scores (sat)\n\nAnalyses below uses the following variables:\n\nSAT (sat): Outcome variable. This is the mean performance on ability test\nExpend (expend): Independent variable. How much the state spends on education\nlogPctSAT (psat) as covariate. This is the log of the percentage of students in the state taking the SAT. Reasons for taking the log: often done with proportions, makes it less truncated.\nState (state) Name of State\nSalary (salary) Average teacher salary\nPTratio (ptratio) Average pupil-to-teacher ratio\n\n\nBoxplot of percentage (left) and log percentage (right) of students in the state taking the SAT.\n\n\nCode\npar(mfrow = c(1, 2))\nboxplot(h$PctSAT, main = \"Per cent SAT takers\", ylim = c(0, 100))\nboxplot(log(h$PctSAT), main = \"log(Per cent SAT takers)\")\n\n\n\n\n\n\n\n\n\n\nTo simplify the following presentation, I put the key variables in a new data frame.\n\n\nCode\n# Rename selected variables to simplify the following presentation\nexpend &lt;- h$Expend\nsat &lt;- h$SAT\npsat &lt;- h$PctSAT  # log-transform, log() in R is the natural log ln\npsatlog &lt;- log(h$PctSAT)  # log-transform, log() in R is the natural log ln\nstate &lt;- h$State\nptratio &lt;- h$PTratio\nsalary &lt;- h$Salary\n\n# Put in new data frame called d\nd &lt;- data.frame(state, expend, psat, psatlog, ptratio, salary, sat)\nstr(d)\n\n\n'data.frame':   50 obs. of  7 variables:\n $ state  : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Ark\" ...\n $ expend : num  4.41 8.96 4.78 4.46 4.99 ...\n $ psat   : int  8 47 27 6 45 29 81 68 48 65 ...\n $ psatlog: num  2.08 3.85 3.3 1.79 3.81 ...\n $ ptratio: num  17.2 17.6 19.3 17.1 24 18.4 14.4 16.6 19.1 16.3 ...\n $ salary : num  31.1 48 32.2 28.9 41.1 ...\n $ sat    : int  1029 934 944 1005 902 980 908 897 889 854 ...\n\n\n\nTake a look at the outcome variable SAT:\n\n\nCode\npar(mfrow = c(1, 2))\n# Histogram\nhist(d$sat, breaks = seq(775, 1225, 50), freq = TRUE,\n     main = \"\")  # Bins of 50 SAT-points\n\n# Histogram with density overlay\nhist(d$sat, breaks = seq(775, 1225, 50), freq = FALSE, main = \"\")  # Bins of 50 SAT-points\nlines(density(d$sat))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Correlation matrix three variables of interest\nround(cor(d[, 2:6]), 3)\n\n\n        expend   psat psatlog ptratio salary\nexpend   1.000  0.593   0.561  -0.371  0.870\npsat     0.593  1.000   0.961  -0.213  0.617\npsatlog  0.561  0.961   1.000  -0.132  0.613\nptratio -0.371 -0.213  -0.132   1.000 -0.001\nsalary   0.870  0.617   0.613  -0.001  1.000\n\n\nCode\npairs(~ sat + expend + psat + psatlog, data = d)\n\n\n\n\n\n\n\n\n\n\n\n\nCrude model\n\n\nCode\n# Bivariate regression model\nplot(d$expend, d$sat)            # Scatter plot to inspect data\ncrude &lt;- glm(sat ~ expend, data = d)    # Simple (crude) regression model of SAT\nsummary(crude)               # Look at model output\n\n\n\nCall:\nglm(formula = sat ~ expend, data = d)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1089.294     44.390  24.539  &lt; 2e-16 ***\nexpend       -20.892      7.328  -2.851  0.00641 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 4887.2)\n\n    Null deviance: 274308  on 49  degrees of freedom\nResidual deviance: 234586  on 48  degrees of freedom\nAIC: 570.57\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nabline(crude)                # Draw regression model\n\n\n\n\n\n\n\n\n\nCode\n# b of expend, with 95 % confidence intervals\ncrude_expend &lt;- c(crude$coefficients[2], confint(crude)[2, ])\nnames(crude_expend) &lt;- c('b', 'ci95lo', 'ci95hi')\nround(crude_expend, 3)\n\n\n      b  ci95lo  ci95hi \n-20.892 -35.255  -6.529 \n\n\n\n\n\nAdjusted model\n\n\nCode\n# Multiple regression model\nadjusted = glm(sat ~ expend + psatlog, data = d) # multiple regression model\nsummary(adjusted)\n\n\n\nCall:\nglm(formula = sat ~ expend + psatlog, data = d)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1147.113     16.700   68.69  &lt; 2e-16 ***\nexpend        11.130      3.264    3.41  0.00134 ** \npsatlog      -78.205      4.471  -17.49  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 664.6464)\n\n    Null deviance: 274308  on 49  degrees of freedom\nResidual deviance:  31238  on 47  degrees of freedom\nAIC: 471.76\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nconfint(adjusted)\n\n\n                  2.5 %     97.5 %\n(Intercept) 1114.380296 1179.84488\nexpend         4.731932   17.52733\npsatlog      -86.968108  -69.44187\n\n\nCode\n# 95 % confidence intervals adjusted model about b of expend\nadjusted_expend &lt;- c(adjusted$coefficients[2], confint(adjusted)[2, ])\nnames(adjusted_expend) &lt;- c('b', 'ci95lo', 'ci95hi')\nround(adjusted_expend, 3)\n\n\n     b ci95lo ci95hi \n11.130  4.732 17.527 \n\n\n\nScatterplot with symbol size related to percentage SAT-takers (psat). Lines from crude model (left) and from the adjusted model (right) for states with psat at the 25th (dashed line), 50th (solid line) and the 75th (dotted line) percentile respectively.\n\n\nCode\npar(mfrow = c(1, 2))\nplot(d$expend, d$sat, cex = d$psatlog, pch = 21, bg = rgb(0, 1, 0, 0.1),\n     xlab = \"Expend\", ylab = \"SAT-score\")\nabline(crude, lty = 2)\n\n# Lines for psat = Q25 and Q75\nplot(d$expend, d$sat, cex = d$psatlog, pch = 21, bg = rgb(0, 1, 0, 0.1),\n     xlab = \"Expend\", ylab = \"SAT-score\")\nxlin &lt;- c(3, 10)\nylin25 &lt;- adjusted$coefficients[1] + \n        adjusted$coefficients[2]*xlin + \n        adjusted$coefficients[3]*quantile(d$psatlog, prob = 0.25)\nlines(xlin, ylin25, col = \"blue\", lty = 2 )\nylin50 &lt;- adjusted$coefficients[1] + \n        adjusted$coefficients[2]*xlin + \n        adjusted$coefficients[3]*quantile(d$psatlog, prob = 0.5)\nlines(xlin, ylin50, col = \"blue\", lty = 1 )\nylin75 &lt;- adjusted$coefficients[1] + \n        adjusted$coefficients[2]*xlin + \n        adjusted$coefficients[3]*quantile(d$psatlog, prob = 0.75)\nlines(xlin, ylin75, col = \"blue\", lty = 3 )\n\n\n\n\n\n\n\n\n\n\n\n\nPlot effect estimates for crude and adjusted model\n\n\nCode\n# create empty plot \nplot(0, xaxt = \"n\", xlim = c(0.5,2.5), ylim = c(-50, 50), \n     pch = \"\", xlab = \"Model\", \n     ylab = \"Linear regression coefficient (SAT / Expend)\")\n# Add x-axis labels\naxis(1, at = c(1, 2), labels = c('Crude', 'Adjusted'))\n\n\n# Add dotted line (lty = 2) at no effect: regression coefficient = 0\nlines(c(-0.2,3.2), c(0,0), lty = 2)\n\n# Add CI for crude and adjusted model\narrows(x0 = 1, x1 = 1, y0 = crude_expend[2], y1 = crude_expend[3], \n       angle = 90, code = 3, length = 0.1)\npoints(1, crude_expend[1], pch = 21, bg = \"grey\")\n\narrows(x0 = 2, x1 = 2, y0 = adjusted_expend[2], y1 = adjusted_expend[3], \n       angle = 90, code = 3, length = 0.1)\npoints(2, adjusted_expend[1], pch = 21, bg = \"grey\")\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted against observed data\nThis plot shows model predictions (adjusted model) versus observed data. Note that predictions (y-axis) refer to predicted mean values for given values of the predictors.\n\n\nCode\nsat_pred &lt;- adjusted$fitted.values\nplot(sat, sat_pred, xlim = c(800, 1200), ylim = c(800, 1200))\nlines(c(800, 1200), c(800, 1200))\n\n\n\n\n\n\n\n\n\nCode\nR &lt;- cor(sat, sat_pred)\nR2 &lt;- R^2\nrms_error &lt;- sqrt(mean((sat_pred - sat)^2))\nround(c(R = R, R2 = R2, rms_error = rms_error), 3)\n\n\n        R        R2 rms_error \n    0.941     0.886    24.995 \n\n\n\n\n\nCheck of residuals\nOur regression model implies that data are normally distributed around predicted values. This means that residuals (observed value - predicted mean value) should be evenly spaced around zero. A way to check this is to plot residuals versus fitted values.\n\n\nCode\nadj_fitted &lt;- adjusted$fitted\nadj_res &lt;- adjusted$residuals\nplot(adj_fitted, adj_res, xlab = \"SAT score (fitted values)\", \n     ylab = \"Residuals (Adjusted model)\")\nabline(h = 0, lty = 2)\n\n# Smoothing estimated, should be approx flat around y = 0\nlines(lowess(adj_res ~ adj_fitted), col = \"red\", lty = 3) \n\n\n\n\n\n\n\n\n\n\nWith small data sets it may be useful to check each residual. Here I just plot residuals for each state, to see for which state the model fitted best and worst.\n\n\nCode\nadj_res &lt;- adjusted$residuals\nadj_res_sorted &lt;- adj_res[order(adj_res)]\nplot(adj_res_sorted, xaxt='n', xlab = '', ylab = \"Residual\", ylim = c(-70, 70))\nlines(c(0, 51), c(0, 0), lty = 2)\n\nstate_sorted &lt;- state[order(adj_res)]\naxis(1, at = 1:length(adj_res_sorted), labels = state_sorted, las = 2, cex = 0.5)\n\n\n\n\n\n\n\n\n\n\nMore on diagnostic plots below (section: Regression diagnostics).\n\n\n\nMore models\nHere modeling as above but with additional variables. Always keep risk of over-adjustment bias in mind! Another issue is the number of data points, the largest model below has 5 parameters for the 50 data points; this is probably OK. An old rule-of-thumb is at least 10 data points per estimated parameter (Note:\nWith Bayesian estimation you can have fewer, and compensate for lack of data with stricter priors.)\n\n\nCode\n# Recalculating and renaming the two models from above \nm1 &lt;- glm(sat ~ expend)  # Crude above\nm2 &lt;- glm(sat ~ expend + psatlog) # Adjusted above\nm3 &lt;- glm(sat ~ expend + psatlog + ptratio)\nm4 &lt;- glm(sat ~ expend + psatlog + salary)\nm5 &lt;- glm(sat ~ expend + psatlog + ptratio + salary)\n\nm1b &lt;- c(m1$coefficients[2], confint(m1)[2, ])\nm2b &lt;- c(m2$coefficients[2], confint(m2)[2, ])\nm3b &lt;- c(m3$coefficients[2], confint(m3)[2, ])\nm4b &lt;- c(m4$coefficients[2], confint(m4)[2, ])\nm5b &lt;- c(m5$coefficients[2], confint(m5)[2, ])\n\n# create empty plot \nplot(0, xaxt = \"n\", xlim = c(0.5,5.5), ylim = c(-50, 50), \n     pch = \"\", xlab = \"\", \n     ylab = \"Linear regression coefficient (SAT / Expend)\")\n# Add x-axis labels\naxis(1, at = 1:5, las = 2, cex = 0.7, \n     labels = c('Expend', 'Expend+\\nlogpSAT', 'Expend+\\nlogpSAT+\\nPTratio', \n                'Expend+\\nlogpSAT+\\nSalary', \n                'Expend+\\nlogpSAT+\\nPTratio+\\nSalary'))\n\n\n# Add dotted line (lty = 2) at no effect: regression coefficient = 0\nlines(c(-0.2,5.7), c(0,0), lty = 2)\n\n# Add CI for models\nadd_ci &lt;- function(m, xpos = 1){\n  arrows(x0 = xpos, x1 = xpos, y0 = m[2], y1 = m[3], \n       angle = 90, code = 3, length = 0.1)\n  points(xpos, m[1], pch = 21, bg = \"grey\")\n}\nadd_ci(m1b, xpos = 1)\nadd_ci(m2b, xpos = 2)\nadd_ci(m3b, xpos = 3)\nadd_ci(m4b, xpos = 4)\nadd_ci(m5b, xpos = 5)\n\n\n\n\n\n\n\n\n\n\nWhich of these two is your model?\n\n\nCode\nlibrary(dagitty)\n\n\nWarning: package 'dagitty' was built under R version 4.4.3\n\n\nCode\ndag1 &lt;- dagitty( \"dag {\n   expend -&gt; sat\n   sat &lt;- pratio &lt;- U -&gt; expend\n   expend -&gt; salary -&gt; sat\n   expend -&gt; ptratio -&gt; sat\n}\")\n\ndag2 &lt;- dagitty( \"dag {\n   expend -&gt; sat\n   sat &lt;- pratio &lt;- U -&gt; expend\n   expend &lt;- salary -&gt; sat\n   expend &lt;- ptratio -&gt; sat\n}\")\n\ncoordinates(dag1) &lt;- list(\n  x = c(expend = 1, U = 1.5, pratio = 2.5, salary = 2.5, ptratio = 2.5, sat = 4),\n  y = c(expend = 3, U = 1.5, pratio = 2, salary = 1, ptratio = 2.5, sat = 3))\n\ncoordinates(dag2) &lt;- list(\n  x = c(expend = 1, U = 1.5, pratio = 2.5, salary = 2.5, ptratio = 2.5, sat = 4),\n  y = c(expend = 3, U = 1.5, pratio = 2, salary = 1, ptratio = 2.5, sat = 3))\n\npar(mfrow = c(1, 2))\nplot(dag1)\nplot(dag2)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#assumptions",
    "href": "12-stat25.html#assumptions",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "12.2 Assumptions",
    "text": "12.2 Assumptions\nSee Gelman et al. (2021), Chapter 11, Section 11.1 for an excellent review of regression assumptions. They list a number of assumptions, in order of importance. Here a brief version:\n\nValidity. Make sure your data maps the research question you are trying to answer. If your data contain test-scores, consider the validity of these (are they really measuring what they purport to do?). If your goal is causal inference, then the selection of covariates is crucial, both included and omitted variables may induce bias. Select your covariates from a causal theory (e.g., a Directed Acyclical Graph). Be aware of the causal nature of the interpretation of a regression coefficient as “the effect of a variable with all else held constant”. This causal interpretation often make no sense.\nRepresentativeness. If you want to generalize to the population, your sample should be representative of the population. Unrepresentative sample is not only a threat to external validity, but may also be a threat to internal validity, e.g., collider bias (a form a selection bias). Often the population is thought of as an infinite hypothetical superpopulation, so the question arises also when you conduct analysis on all members of a population (maybe you want to be able to predict future members of the population, etc.)\nAdditivity and linearity These are the most important mathematical assumptions of the general linear model. Note that many non-linear relationships can be made linear by suitable transformations\nIndependence of errors. We assume that observations are sampled independent of each other.\nEqual variance of errors This is \\(\\sigma\\) in our linear model, it is assumed to be constant for all levels of predictors. Non-linear transformations may help, for example, the log transform if the variability around the line is proportional to the size of the predicted value.\nNormality of errors This is the assumption that the outcome is normally distributed given our predictors, that is, the distribution of points around a given point of the regression line is normally distributed. For the prediction of the regression line (i.e., mean values for a given value of x), this is not a very important assumption.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#regression-diagnostics",
    "href": "12-stat25.html#regression-diagnostics",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "12.3 Regression diagnostics",
    "text": "12.3 Regression diagnostics\nThere are several ways to check your model. In the SAT-data example above, these three plots were used:\n\nDisplay regression line(s) as a function of one predictor. This is the best way to understand the implications of our model. Consider several plots, each with a different value of one or several other key variables. This is crucial for understanding interaction models. See examples above, for age-weight data and for the school data.\nResidual plots. For small data sets, it may be a good idea to rank order residuals to check them one one by one (see school-data example above). For large data sets, a plot of residuals versus predicted values can indicate problems with your model. We want outcomes to be evenly distributed along the regression line, resulting in an even residual plot (see Gelman et al. (2021), figs. 11.6-8).\nPredicted value against observed data. Gives an idea of how well the overall model fits the data, see example above from the school data.\n\n\n\nAdvanced - Regression diagnostics\nWith Bayesian estimation, several other diagnostic plots are available. Bayesian models are data-generative, that is, they can be used to generate simulated data in accordance with your model. Comparing these simulated data (your model implications) with your actual outcome data is a fundamental way to check your. A good model should generate data that looks a lot like the data you observed (the data it was supposed to model). Such plots may guide you to aspects of the model that seem inconsistent with your model.\nWe have already discussed one such plot above (3. Predicted against observed data), but that one was limited to point-estimates of predicted means. A more advanced plot will simulate new data, taking into account uncertainty in predicted means (\\(\\mu_i\\)) and uncertainty in the predicted variation around means (\\(\\sigma\\)). See, Gelman et al., Section 11.4 for a simple illustration.\nBelow I illustrate using models of the SAT-data discussed above. To do this, I need first to fit the models in stan_glm() from the rstanarm packages.\n\n\nCode\n# Crude model\nm1 &lt;- stan_glm(sat ~ expend, data = d, refresh = 0)  \nprint(m1)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      sat ~ expend\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 1088.9   44.1\nexpend       -20.8    7.3\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 70.5    7.2  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\n# Adjusting for psat\nm2 &lt;- stan_glm(sat ~ expend + psatlog, data = d, refresh = 0) \nprint(m2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      sat ~ expend + psatlog\n observations: 50\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 1147.1   16.3\nexpend        11.1    3.3\npsatlog      -78.1    4.7\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 26.1    2.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nI then extract samples from the posterior distribution of model parameters.\n\n\nCode\n# Extract samples, you may also use as.matrix() as Gelman et al (e.g., p. 163)\n \n# Crude model\nss1 &lt;- as.data.frame(m1) \nhead(ss1)\n\n\n  (Intercept)    expend    sigma\n1    1115.356 -27.21964 57.91113\n2    1052.209 -13.09154 83.51162\n3    1137.713 -30.30710 62.34097\n4    1116.702 -23.27341 71.25001\n5    1133.792 -26.80801 79.69628\n6    1051.870 -15.75319 57.69220\n\n\nCode\n# Adjusted model\nss2 &lt;- as.data.frame(m2)\nhead(ss2)\n\n\n  (Intercept)    expend   psatlog    sigma\n1    1125.964 16.346956 -82.72654 27.01791\n2    1159.490  8.475829 -75.83672 23.89718\n3    1127.625 12.157837 -74.88251 27.75706\n4    1133.542 13.470384 -78.22500 28.97512\n5    1146.142 16.611083 -87.31651 27.44121\n6    1146.331  7.548495 -73.11641 25.40585\n\n\n\nThen I create simulated data sets, one for each row of the data frame with sampled parameter values. Each row is one plausible model.\n\n\nCode\nndraws &lt;- nrow(ss1)\nn &lt;- length(d$sat)\n\n# Model m1\nyrep1 &lt;- array(NA, dim = c(ndraws, n))\nx &lt;- d$expend\n\nfor (j in 1:ndraws){\n  yrep1[j, ] &lt;- rnorm(n, \n                     mean = ss1$`(Intercept)` + ss1$expend*x,\n                     sd = ss1$sigma)\n}\n  \n# Model m2\nyrep2 &lt;- array(NA, dim = c(ndraws, n))\nx1 &lt;- d$expend\nx2 &lt;- d$psatlog\n\nfor (j in 1:ndraws){\n  yrep2[j, ] &lt;- rnorm(n, \n                     mean = ss2$`(Intercept)` + ss2$expend*x1 + ss2$psatlog*x2,\n                     sd = ss2$sigma)\n}\n\n# Look at yrep arrays\nstr(yrep1)\n\n\n num [1:4000, 1:50] 997 932 1055 1134 1045 ...\n\n\nCode\nstr(yrep2)\n\n\n num [1:4000, 1:50] 1053 1004 1034 1056 1055 ...\n\n\nThe objects yrep1 and yrep2 contain 4000 simulated data sets, each with 50 observations (because our data set contained 50 observations)\n\nFinally, I may plot observed data (y) and simulations (yrep), e.g., using kernel density plots as below.\n\n\nCode\nset.seed(123)\npar(mfrow = c(1, 2))\n\n# Model 1: Crude\nplot(density(d$sat), xlim = c(700, 1300),\n     ylim = c(0, 0.008), main = \"Crude model\", xlab = \"SAT score\")\nfor (j in sample(1:ndraws, 30)) {\n  lines(density(yrep1[j, ]), col = rgb(1, 0, 0, 0.2))\n}\n\n\n# Model 2: Adjusted\nplot(density(d$sat), xlim = c(700, 1300),\n     ylim = c(0, 0.008), main = \"Adjusted model\", xlab = \"SAT score\")\nfor (j in sample(1:ndraws, 30)) {\n  lines(density(yrep2[j, ]), col = rgb(1, 0, 0, 0.2))\n}\n\n# Legend\nlines(c(1100, 1150), c(0.008, 0.008))\nlines(c(1100, 1150), c(0.0075, 0.0075), col = \"red\")\ntext(1160, 0.008, labels = \"y\",    pos = 4, cex = 0.7)\ntext(1160, 0.0075, labels = \"yrep\", pos = 4, cex = 0.7)\n\n\n\n\n\n\n\n\n\n\nA faster way to simualte data sets (yrep) is to use the poster_predict() function in rstanarm. Here a plot of distribution of IQRs over 4000 sets of yrep (black line), compared to the IQR of the observed data, y (blue line).\n\n\nCode\nyrep1 &lt;- posterior_predict(m1)\nyrep2 &lt;- posterior_predict(m2)\n\npar(mfrow = c(1, 2))\niqr1 &lt;- apply(yrep1, 1, IQR)\nplot(density(iqr1), main = \"Crude model IQR\", xlim = c(20, 200), xlab = \"IQR\")\nabline(v = IQR(d$sat), col = \"blue\")\n\niqr2 &lt;- apply(yrep2, 1, IQR)\nplot(density(iqr2), main = \"Adjusted model IQR\", xlim = c(20, 200), xlab = \"IQR\")\nabline(v = IQR(d$sat), col = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#model-fit",
    "href": "12-stat25.html#model-fit",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "12.4 Model fit",
    "text": "12.4 Model fit\nModels are often evaluated based on how well they fit data. For linear models, common measures of fit are derived from residuals. A residual, \\(r_i\\), is defined as\n\\(r_i = y_i - \\hat{y_i}\\), where \\(\\hat{y_i}\\) is the model’s prediction for an observation \\(y_i\\).\nWhen the model is evaluated using the same data on which it was fitted (or “trained”), this may be referred to as internal or in-sample model evaluation. In this approach, the observed data is used twice—first to fit the model and then to assess its performance based on residuals. Two common measures for in-sample fit are the residual standard deviation (\\(\\hat{\\sigma}\\)) and the proportion of explained variance, \\(R^2\\).\nOne key limitation of in-sample fit is the risk of overfitting, where the model captures random patterns specific to a particular data set. As a result, an overfitted model may perform poorly when applied to new, unseen data that lacks those specific random errors.\nWhen the model is evaluated using a new data set, this may be referred to as external or out-of-sample model evaluation. A general term for this is cross-validation as briefly discussed below.\n\n\nIn-sample: Residual standard deviation and explained variance\nThe residual standard deviation, \\(\\hat{\\sigma}\\), is an estimate of the \\(\\sigma\\) of the general linear model (see these notes, Section 8.4). \\(\\sigma\\) is the dispersion of values around the population mean associated with specific values of the predictors. We may use \\(\\hat{\\sigma}\\) as a measure of the unexplained variation in the data.\nThe amount of variance “explained” by the model, \\(R^2\\). This is a standardized measure that relates \\(\\hat{\\sigma}^2\\) to the sample variance of the outcome, \\(s^2_y\\): \\(R^2 = 1  - (\\hat{\\sigma}^2/s^2_y)\\). If equal, then \\(R^2 = 0\\), if \\(\\hat{\\sigma}^2 = 0\\), all data points fall on the regression line, then \\(R^2 = 1\\).\nFor more on this, see Gelman et al. (2021), section 11.6.\n\n\n\nOut-of-sample: Cross-validation\nNot covered in this course, but important, so the ambitious student should consult Gelman et al. (2021), section 11.7-11.8.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#ten-quick-tips-ro-regression-modelling",
    "href": "12-stat25.html#ten-quick-tips-ro-regression-modelling",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "12.5 Ten quick tips ro regression modelling",
    "text": "12.5 Ten quick tips ro regression modelling\nAppendix B of Gelman et al. (2021) give 10 tips to improve your regression modeling. Excellent suggestions, read carefully! And remember where you read it, so you can revisit as your modelling skills guest more advanced and your models more complex. Here is a short version of my interpretation of their tips:\n\nThink about variation and replication. If possible, fit your model to different data sets to get a sense of variation across studies and problems. One study is seldom enough to seriously answer your research question.\nForget about statistical significance. “Forget about p-values, and forget about whether your confidence intervals exclude zero” (p. 493). Dichotomous thinking is throwing away useful information about uncertainty. So, resist dichotomous thinking and embrace uncertainty (cf. Amrhein et al. (2019))\nGraph the relevant and not the irrelevant. See the book for many useful ways of visualizing your model and data. Do not forget to plot model implications to understand your model. And be prepared to explain any graph you show (if you can’t, it is probably not a useful plot).\nInterpret regression coefficients as comparisons. Avoid the term “effect” of regression coefficients unless you really mean causal effect. But you can always think of a predictor coefficient as a difference in the outcome for one unit difference in the predictor (keeping other predictors constant).\nUnderstand statistical methods using fake-data simulation. Nowadays, data simulation is indispensable tool for the data analyst. Simulation is a way of understanding our models before we start fitting them to data, and it is a way of evaluating how well they fit data. For complex models, you should use simulations to test your code and make sure that you can recovery true parameters.\nFit many models. Fitting a set of models, from simple to complex is a way to understand our models and our data. Always be aware of the risk of over-fitting and under-fitting, and, if your goal is causal inference, of introducing bias with incorrect choice of predictors.\nSet up a computational work flow. For complex models and large data sets, you will encounter computational issues. It may just take too long time to fit your model, or the software will refuse to do as you say (resulting in error messages). Consider simple models, maybe separate models for subsets of your data, and always check you model against fake-data simulations.\nUse transformations. “Consider transforming just about every variable in sight” (p. 496). In addition to linear and non-linear transformations of single variables, also consider interactions and combinations of variables. As long as you model make sense it is worth testing and comparing to other models.\nDo causal inference in a targeted way, not as a byproduct of a large regression. Select predictors carefully if your goal is causal inference. Use Directed Acyclical Graphs to hep you think clearly about causation with many variables, especially, of course, for non-experimental data.\nLearn methods through live examples. Learn modelling by applying to problems that you found interesting. “Know your data, your measurements, and your data-collection procedure. … Understand the magnitudes of your regression coefficients, not just their signs. You will need this understanding to interpret your findings and catch things that go wrong” (p. 496).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#practice",
    "href": "12-stat25.html#practice",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n12E1. In a linear regression model predicting weight [kg] as a function of age [y], the age variable used in the linear regression model was transformed to:\n\\(age_{transformed} = 10(age - 40)\\)\nWhy? (two reasons)\n\n\n\n12E2. Centering of a predictor in a multiple regression model will change the intercept of the model. What happens to the regression coefficients (slopes)?\n\n\n\n\nMedium\n\n12M1. Go back to the kidiq data discussed in chapter 10 of these notes. Do diagnostic plots. Specifically:\n\nPlot model prediction versus observed values, separately for mom_hs = 0 and mom_hs = 1.(cf. Gelman et al, Fig. 11.4)\nResiduals versus fitted values (cf. Gelman et al, Fig. 11.7 left panel), with separate symbols for mom_hs = 0 and mom_hs = 1.\n\nComment on what you see.\n\n\n\n\nHard\n\n12H1. Go back to the model in 12M1. Plot kernel densities for observed kidiq (y) overlaid with densities for simulated data sets (yrep). Do for the whole data set, and separately for mom_hs = 0 and mom_hs = 1.\nUse the function posterior_predict() to generate simulated data (yrep). Compare your plot with the corresponding plot generate using the function pp_check()\n\n\n\n12H2. Redo the density plot from 12H1, but now from scratch. That is, don’t use posterior_predict() to generate samples, do it “by hand” from extracted samples (as.data.frame(modelfit) or as.matrix(modelfit)). Se example above, and in Gelman et al (p. 163)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "12-stat25.html#session-info",
    "href": "12-stat25.html#session-info",
    "title": "12  S: Linear Regression: Multiple Predictors",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4   rstanarm_2.32.1 Rcpp_1.0.14    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     dplyr_1.1.4          farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] shinystan_2.6.0      promises_1.3.3       shinyjs_2.1.0       \n[10] digest_0.6.37        mime_0.13            lifecycle_1.0.4     \n[13] StanHeaders_2.32.10  survival_3.7-0       magrittr_2.0.3      \n[16] posterior_1.6.1      compiler_4.4.2       rlang_1.1.6         \n[19] tools_4.4.2          igraph_2.1.4         yaml_2.3.10         \n[22] knitr_1.50           htmlwidgets_1.6.4    pkgbuild_1.4.8      \n[25] curl_6.4.0           plyr_1.8.9           RColorBrewer_1.1-3  \n[28] dygraphs_1.1.1.6     abind_1.4-8          miniUI_0.1.2        \n[31] grid_4.4.2           stats4_4.4.2         xts_0.14.1          \n[34] xtable_1.8-4         inline_0.3.21        ggplot2_3.5.2       \n[37] scales_1.4.0         gtools_3.9.5         MASS_7.3-61         \n[40] cli_3.6.5            rmarkdown_2.29       reformulas_0.4.1    \n[43] generics_0.1.4       RcppParallel_5.1.10  rstudioapi_0.17.1   \n[46] reshape2_1.4.4       minqa_1.2.8          rstan_2.32.7        \n[49] stringr_1.5.1        shinythemes_1.2.0    splines_4.4.2       \n[52] bayesplot_1.13.0     parallel_4.4.2       matrixStats_1.5.0   \n[55] base64enc_0.1-3      vctrs_0.6.5          V8_6.0.4            \n[58] boot_1.3-31          Matrix_1.7-1         jsonlite_2.0.0      \n[61] crosstalk_1.2.1      glue_1.8.0           nloptr_2.2.1        \n[64] codetools_0.2-20     distributional_0.5.0 DT_0.33             \n[67] stringi_1.8.7        gtable_0.3.6         later_1.4.2         \n[70] QuickJSR_1.8.0       lme4_1.1-37          tibble_3.3.0        \n[73] colourpicker_1.3.0   pillar_1.10.2        htmltools_0.5.8.1   \n[76] R6_2.6.1             Rdpack_2.6.4         evaluate_1.0.3      \n[79] shiny_1.11.1         lattice_0.22-6       markdown_2.0        \n[82] rbibutils_2.3        backports_1.5.0      threejs_0.3.4       \n[85] httpuv_1.6.16        rstantools_2.4.0     gridExtra_2.3       \n[88] nlme_3.1-166         checkmate_2.3.2      xfun_0.52           \n[91] zoo_1.8-14           pkgconfig_2.0.3     \n\n\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Retire statisticial significance. Nature, 567, 305–307.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nGuber, D. (1999). Getting what you pay for. Journal of Statistics Education, 7(2).\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>S: Linear Regression: Multiple Predictors</span>"
    ]
  },
  {
    "objectID": "13-method25.html",
    "href": "13-method25.html",
    "title": "13  M: Observational studies",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#topics",
    "href": "13-method25.html#topics",
    "title": "13  M: Observational studies",
    "section": "",
    "text": "Experiment versus observation\nAssumptions needed: Different DAGs may be consistent with the same data\nControl for, adjust for, conditioning on (same thing) to achieve other things equal (ceteris paribus)\n\nRestriction, Stratification\nMatching\nRandom assignment\nStatistical control\n\nTypes of observational studies\n\nNatural experiment\nCohort study\nCase-control study\nCross-sectional study\n\nBias in observational studies\n\nConditioning on a confounder may reduce bias (“confounding bias”, also known as “omitted variable bias”): \\(X \\leftarrow \\boxed{Z} \\rightarrow Y\\)\n\nNegative confounding\nResidual confounding\n\nConditioning on a mediator may introduce bias (“overadjustment bias”): \\(X \\rightarrow \\boxed{Z} \\rightarrow Y\\)\nConditioning on a collider may introduce bias (“collider bias”, also known as “selection bias”, or “Berkson error”): \\(X \\rightarrow \\boxed{Z} \\leftarrow Y\\)\n\n\nTheoretical articles to read:\n\nRohrer (2018) on causal inference in observational studies, several points made using directed acyclical graphs (DAGs)\nElwert & Winship (2014) on bias that may arise from conditioning on colliders\nAnd do not forget the material on Dagitty homepage, please do the exercises to get a feel for DAGs and how hard it is to assess causation from observations only.\n\n\n\n\nWe all know that: “Correlation does not imply causation”\nBut DAGs reminds us that “Wherever there is correlational smoke there is causal fire”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#experiment-or-observation",
    "href": "13-method25.html#experiment-or-observation",
    "title": "13  M: Observational studies",
    "section": "13.1 Experiment or Observation?",
    "text": "13.1 Experiment or Observation?\nObservational studies, sometimes called correlational studies, involve measurement of study units on exposure and outcome variables and, typically, a set of variables that potenitally may confound the exposure-outcome relationship.\nNote that observational versus experimental is not a crisp distinction, there is a grey zone (as always).\n\nManipulation in experiments, not in observational studies.\n\nBut how about natural experiments? For example, Snow (1855) “On the Mode of Communication of Cholera …” (discussed below)\n\nRandomization in experiments, not in observational studies.\n\nBut how about non-randomized experiments (a.k.a quasi-experiments)\nAnd how about within-subject experiments without random assignment?\n\nSelection mechanism known (ignorable) in experiments not in observational studies.\n\nThis criterion would classify within-subject experiments as experiments, and non-randomized between-subject experiments as observational studies.\n\n\n\nThe key challenge for observational studies is that it is hard to separate effects of exposure (“treatment effects”) from systematic errors (bias) induced by uncontrolled variables as selection mechanisms are partly or fully unknown.\nRohrer (2018) Conclusion: Making Causal Inferences on the Basis of Correlational Data is Very Hard\nCausal inference solely from observations require additional assumptions about sources of bias. Key assumptions may concisely be summarized in a Directed acyclical graph (DAG). Note: That the same data may be consistent with several DAGs that may be mutually inconsistent. Causal estimates only unbiased if your assumptions are correct (and it is very hard to know if they are).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#types-of-observational-studies",
    "href": "13-method25.html#types-of-observational-studies",
    "title": "13  M: Observational studies",
    "section": "13.2 Types of observational studies",
    "text": "13.2 Types of observational studies\n\nNatural experiments\nLongitudinal studies\n\nCohort studies (often prospective, starting at exposure waiting for outcome)\nCase-control studies (often retrospective, starting at outcome looking back for exposure)\n\nCross-sectional studies\n\n\n\nNatural experiments\nCholera outbreak in London 1854, transmitted by air or water?\n\n\nSnow (1855):\n“… Each company supplies both rich and poor, both large houses and small; there is no difference in the condition or occupation of the persons receiving the water of the different companies… As there is no difference whatever either in the houses or the people receiving the supply of the two Water Companies, or in any of the physical conditions with which they are surrounded, it is obvious that no experiment could have been devised which would more thoroughly test the effect of water supply on the progress of Cholera than this, which circumstances placed ready made before the observer…”\n\nRegression discontinuity design, in theory a design with ignorable assignment mechanisms (under the strong assumption that its relationship to the outcome is modelled correctly, below assuming a linear relationship.)\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal studies\n\nCohort studies: A group of people (cohort) who share characteristics, e.g., birth year, road-traffic noise exposure, work place, etc. are followed over time, and incidence of outcome (e.g., heart attack) is registered during the study period.\n\nIn terms of internal validity: Commonly consider the best observational-study design for causal inference\nMay be costly both in terms of sample size (if rare outcomes) and time (if outcomes take time to develop).\n\nCase-control studies: Select a group of people with the disease (outcome) and a comparison group without, and assess and compare their exsposures to the potential causal factor back in time.\n\nCost-efficient for rare outcomes\nRecall bias may be a threat\n\n\n\n\n\nCross-sectional studies\nMeasuring both independent variables (exposure, covariates) and dependent variable (outcome) at the same time in a population (or sample) at one point in time.\n\nEasy to conduct in short time\nReversed causation a major threat (apart from confounding and selection bias)\nMono-method bias a major threat in studies using a single measuring instrument (e.g., a single questionnaire, as all too often seen in social science research).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#control-for-adjust-for-conditioning-on",
    "href": "13-method25.html#control-for-adjust-for-conditioning-on",
    "title": "13  M: Observational studies",
    "section": "13.3 Control for, adjust for, conditioning on, …",
    "text": "13.3 Control for, adjust for, conditioning on, …\nWith the purpose to achieve other things equal (ceteris paribus).\n\nRepeated measures of single unit\nMatching on background variables\nRandom assignment to treatment groups\nRestriction or stratification on background variables\nStatistical control\n\n\nMatching\nGoal: Unit homogeneity = exchangeable units\n\nAs part of design (before data collection)\nMay be used together with random assignment\nAfter data has been collected: Data pruning\nStrength: Model independent\n\n\n\nRestriction or stratification on background variables\nRestriction may increase internal validity, at the expense of external validity\n\nAs part of design (before data collection), restrict data collection to a specific group, or make focus on several subgroups (e.g., age groups) and make sure to have enough data in each group for separate analysis.\nAfter data has been collected: Analysis restricted to a specific subgroup, or separate analysis of data from subgroups (of enough observations within subgroup)\nStrength: Model independent\n\n\n\nStatistical control\nAdd covariates in a regression model.\nWeakness: Model dependent, i.e., based on assumptions of, for example, linear relationships between variables (as assumed in multiple linear regression). If critical relationships are non-linear, for example, between confounders and the exposure, then confounding may still bias the causal effect estimate although the confounding variables have been included in the linear regression model (so called “residual confounding”)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#bias-in-observational-studies",
    "href": "13-method25.html#bias-in-observational-studies",
    "title": "13  M: Observational studies",
    "section": "13.4 Bias in observational studies",
    "text": "13.4 Bias in observational studies\n\n\nConfounding bias\nConfounding, also known as “omitted variable bias” is maybe the most obvious alternative explanation to causal explanations. A third variable, not controlled for, may have influenced both exposure and outcome, leading to biased estimates of the causal effect.\nA simple example: Hearing impairment is more common among construction workers than office workers. A potential causal factor is occupational noise exposure; after all, construction sites tend to be noisier than offices. But socioeconomic status is a potential confounder. Individuals from low socioeconomic background may be more likely to become construction workers than individuals from high socioeconomic background, and we know that socioeconomic status is related to most health outcomes, probably also risk for hearing impairment. Here is a simple DAG:\n\n\nCode\nconfound &lt;- dagitty( \"dag {\n   Occupational_Noise -&gt; Hearing_Impairment\n   Occupational_Noise &lt;- Socioeconomic_Status-&gt; Hearing_Impairment\n}\")\n\ncoordinates(confound) &lt;- list(\n  x = c(Occupational_Noise = 1, Socioeconomic_Status = 2, Hearing_Impairment = 3),\n  y = c(Occupational_Noise = 2, Socioeconomic_Status = 1, Hearing_Impairment = 2))\n\nplot(confound)\n\n\n\n\n\n\n\n\n\nNote that according to this DAG, occupational noise do indeed cause hearing impairment, but the size of the causal effect may be biased unless variation in socioeconomic status is taken into account.\nNegative confounding\nA confounder may hide or mask a casual relationship between two variables. Consider this DAG:\n\n\nCode\nconfound2 &lt;- dagitty( \"dag {\n   X -&gt; Y\n   X &lt;- Z -&gt; Y\n}\")\n\ncoordinates(confound2) &lt;- list(\n  x = c(X = 1, Z = 2, Y = 3),\n  y = c(X = 2, Z = 1, Y = 2))\n\nplot(confound2)\n\n\n\n\n\n\n\n\n\nHere a simple simulation, assuming that the average causal effect of X on Y is 0.5, that is, increasing X with one unit would on average increase Y with 0.5 units.\n\n\nCode\nset.seed(123)\nn &lt;- 1e5\n\n# Define causal effects\nb_zx &lt;- -1 # Causal effect of z on x\nb_zy &lt;-  1   # Causal effect of z on y\nb_xy &lt;- 0.5  # Causal effect of x on y\n\n# Define variables (structural equations)\nz &lt;- rnorm(n)\nx &lt;- rnorm(n) + b_zx*z\ny &lt;- rnorm(n) + b_zy*z + b_xy*x\n\n# Try this if you prefer random effects:\n# x &lt;- rnorm(n) + rnorm(n, mean = b_zx, sd = 0.2) * z\n# y &lt;- rnorm(n) + rnorm(n, mean = b_zy, sd = 0.2) * z + \n#       rnorm(n, mean = b_xy, sd = 0.2) *x\n\n# Estimate total average causal effect of x on y\nlm_crude    &lt;- lm(y ~ x)  # Crude model\nlm_adjusted &lt;- lm(y ~ x + z)  # Model adjusted for z\n\n# Print results\nlm_crude\n\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n   0.002486     0.003130  \n\n\nCode\nlm_adjusted\n\n\n\nCall:\nlm(formula = y ~ x + z)\n\nCoefficients:\n(Intercept)            x            z  \n -0.0005948    0.5000543    0.9979861  \n\n\n… and it may also reverse the causal effect estimate (type S error):\n\n\nCode\nset.seed(123)\nn &lt;- 1e5\n\n# Define causal effects\nb_zx &lt;- -1 # Causal effect of z on x\nb_zy &lt;-  1.5   # Causal effect of z on y\nb_xy &lt;- 0.5  # Causal effect of x on y\n\n# Define variables (structural equations)\nz &lt;- rnorm(n)\nx &lt;- rnorm(n) + b_zx*z\ny &lt;- rnorm(n) + b_zy*z + b_xy*x\n\n# Estimate total average causal effect of x on y\nlm_crude    &lt;- lm(y ~ x)  # Crude model\nlm_adjusted &lt;- lm(y ~ x + z)  # Model adjusted for z\n\n# Print results\nlm_crude\n\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    0.00403     -0.24583  \n\n\nCode\nlm_adjusted\n\n\n\nCall:\nlm(formula = y ~ x + z)\n\nCoefficients:\n(Intercept)            x            z  \n -0.0005948    0.5000543    1.4979861  \n\n\n\nResidual confounding\nResidual confounding is bias that remains despite our attempt to adjust for a confounder. This could happen for several reasons, for example:\n\nOur measure may not cover important aspects of the construct. Socioeconomic status measured by income may not capture aspects related to education, parents income, or socioeconomic status at the neighborhood level (living in an wealthy area of town may have beneficial consequences on health independent of socioeconomic status at the individual level).\nAdding a confounder to a regression model may not be enough because the relationship assumed by the model (e.g., linear) may not be a good approximation of the true relationship. Methods like matching or restriction are less vulnerable in this regard as they are model independent (make no assumptions about functional relationships between covariate and outcome).\nRestriction not tight enough, for instance, if analysis are restricted to adults younger than 40 years old, this may not be enough to adjust for age as there may still be an effect of age on exposure and outcome in this age span.\n\n\n\n\nWhy not control for all my covariats, how can I loose?\nA key insight from learning about Directed Acyclic Graphs (DAGs) is that controlling for covariates can sometimes introduce bias. This can occur in two main ways: by controlling for a mediating variable, leading to overadjustment bias, or by controlling for a collider variable, resulting in collider bias. It’s also important to note that controlling for a descendant of such variables can similarly introduce bias, though typically to a lesser degree.\n\n\nOveradjustment bias\n“Overadjustment bias” or “overcontrol bias” refer to bias that is introduced by controlling for a mediating variable. In the example discussed above, controlling for hearing protection at work may lead to bias. Possibly overestimating the total average causal effect by eliminating a mediator that might reduce the adverse effect.\n\n\nCode\noveradj &lt;- dagitty( \"dag {\n   Occupational_Noise -&gt; Hearing_Impairment &lt;- Hearing_Protection\n   Occupational_Noise -&gt; Hearing_Protection\n   Occupational_Noise &lt;- Socioeconomic_Status-&gt; Hearing_Impairment\n}\")\n\ncoordinates(overadj) &lt;- list(\n  x = c(Occupational_Noise = 1, Socioeconomic_Status = 2, Hearing_Protection = 2, \n        Hearing_Impairment = 3),\n  y = c(Occupational_Noise = 2, Socioeconomic_Status = 1, Hearing_Protection = 1.5,\n        Hearing_Impairment = 2))\n\nplot(overadj)\n\n\n\n\n\n\n\n\n\n\n\n\nCollider bias\nConditioning on a variable that is a collider may introduce bias. In this DAG, Stress (physiological reactivity to stressors) is a collider on the path\n\\(Occupational \\ Noise \\rightarrow Stress \\leftarrow Genetic \\rightarrow Hearing \\ Impariment\\),\nwhere Genetic may be some unmeasured biological vulnerability linked to both physiological stress reactions and hearing impairment. Adjusting for Stress may introduce collider bias by opening up a backdoor path between exposure and outcome.\n\n\nCode\ncollide &lt;- dagitty( \"dag {\n   Occupational_Noise -&gt; Hearing_Impairment &lt;- Hearing_Protection\n   Occupational_Noise -&gt; Hearing_Protection\n   Occupational_Noise &lt;- Socioeconomic_Status-&gt; Hearing_Impairment\n   Occupational_Noise -&gt; Stress &lt;- Genetic -&gt; Hearing_Impairment\n}\")\n\ncoordinates(collide) &lt;- list(\n  x = c(Occupational_Noise = 1, Socioeconomic_Status = 2, Hearing_Protection = 2, \n        Hearing_Impairment = 3, Stress = 2, Genetic = 2.5),\n  y = c(Occupational_Noise = 2, Socioeconomic_Status = 1, Hearing_Protection = 1.5,\n        Hearing_Impairment = 2, Stress = 2.75, Genetic = 2.5))\n\nplot(collide)\n\n\n\n\n\n\n\n\n\n\nHere a simple example of collider bias:\n\\(Intelligence \\rightarrow Income \\leftarrow Social \\ skills\\)\nAssume that intelligence is independent of social skills in the general population, and that both have a positive influence on income. Assume we found a negative association between IQ scores and scores on a social skill test in a sample of chief executive officers (CEOs) of large companies. A causal interpretation of this could be that: “Maybe people with high IQ spend a lot of time reading books, and therefore do not develop their social skills.” An alternative explanation would be that the observed association was spurious, an example of collider bias. Here a simulation:\n\n\nCode\nset.seed(431)\nn &lt;- 3e3\niq &lt;- rnorm(n) # IQ scores, standardized to mean = 0, sd = 1\nss &lt;- rnorm(n)  # Social skill scale, standardized to mean = 0, sd = 1\nincome &lt;- rnorm(n) + rnorm(n, 0.4, 0.2) * iq  +rnorm(n, 0.4, 0.2) * ss\n\n# Plot full popualtion\nplot(iq, ss, xlab = \"Intelligence (z-score)\", ylab = \"Social skills (z-score)\")\nabline(lm(ss ~iq))\n\n# Add line for sample restricted to high income people\npoints(iq[income &gt; 2], ss[income &gt; 2], pch = 21, bg = \"blue\")\nabline(lm(ss[income &gt; 2] ~iq[income &gt; 2]), lty = 2, col = \"blue\")\n\n\n\n\n\n\n\n\n\nOpen symbols and black regression line refer to the full population; blue points and blue regression line refer to a sample restricted to high income people.\n\n\n\nBias summary\nAssume that we observed that X and Y were associated in our sample. “X cause Y” (H1) is certainly not the only possibility, here is my list of potential explanations (I am sure you can find more examples):\n\nH0: Sampling error: X and Y are not causally related, or otherwise associated in the population.\n\nPossible DAG: \\(X \\rightarrow Z, \\ \\ Y \\leftarrow W\\)\n\nH1: Causation: X causes Y.\n\nPossible DAG: \\(X \\rightarrow Z \\rightarrow Y\\)\n\nH2: Reversed causation: Y causes X.\n\nPossible DAG: \\(X \\leftarrow Z \\leftarrow Y\\)\n\nH3: Confounding: Z causes both X and Y\n\nPossible DAG: \\(X \\leftarrow Z \\rightarrow Y\\)\n\nH4: Collider bias: X and Y both causes Z and we have conditioned on Z (e.g., if Z is income and our sampel only included individuals with high income).\n\nPossible DAG: \\(X \\rightarrow \\boxed{Z} \\leftarrow Y\\)\n\n\n\nAssume that we observed that X and Y were not associated in our sample. “X does not cause Y” (H0) is certainly not the only possibility, here is my list of potential explanations (I am sure you can find more examples):\n\nH0: Independence: X and Y are not causally related, or otherwise associated in the population.\n\nPossible DAG: \\(X \\rightarrow Z, \\ \\ Y \\leftarrow W\\)\n\nH1: Causation masked by sampling error: X causes Y\n\nPossible DAG: \\(X \\rightarrow Z \\rightarrow Y\\) (Note that DAGs assume error free variables.)\n\nH2: Reversed causation masked by sampling error: Y causes X\n\nPossible DAG: \\(X \\leftarrow Z \\leftarrow Y\\) (Note that DAGs assume error free variables.)\n\nH3: Causation masked by conditioning on a mediator: X causes Y\n\nPossible DAG: \\(X \\rightarrow \\boxed{Z} \\rightarrow Y\\)\n\nH4: Reversed causation masked by conditioning on a mediator: Y causes X\n\nPossible DAG: \\(X \\leftarrow \\boxed{Z} \\leftarrow Y\\)\n\nH5: Negative confounding: X causes Y, masked by negative confounding of Z\n\nPossible DAG: \\(X \\leftarrow Z \\rightarrow Y, \\ \\ X \\rightarrow Y\\) (Note that DAGs assume that perfect cancellation is not the case. Still, the observed association may be very weak suggesting a negligable association despite a sizebale causal relationship between X and Y).\n\nH6: Negative confounding and reversed causation: Y causes X, masked by negative confounding of Z (Note comment on H5)\n\nPossible DAG: \\(X \\leftarrow Z \\rightarrow Y, \\ \\ X \\leftarrow Y\\)\n\nH7: Collider bias X causes Y, masked by collider bias due to conditioning on common effect Z.\n\nPossible DAG: \\(X \\rightarrow \\boxed{Z} \\leftarrow Y, \\ \\ X \\rightarrow Y\\)\n\nH8: Collider bias and reversed causation Y causes X, masked by collider bias due to conditioning on common effect Z.\n\nPossible DAG: \\(X \\rightarrow \\boxed{Z} \\leftarrow Y, \\ \\ X \\leftarrow Y\\)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#why-experiments-are-superior-to-observational-studies-for-causal-inference",
    "href": "13-method25.html#why-experiments-are-superior-to-observational-studies-for-causal-inference",
    "title": "13  M: Observational studies",
    "section": "13.5 Why experiments are superior to observational studies for causal inference",
    "text": "13.5 Why experiments are superior to observational studies for causal inference\nThe simple DAG answer: Experiments disconnect the exposure variable (X) from other variables in the causal diagram, that is, the manipulation changes X from being an endogenous variable to an exogenous variable. Assuming of course that assignment mechanism, non-compliance and missing data are “ignorable”.\nShort-cut DAG:\nThe full DAG may be hard to conceptualize, for example if the research field is not well-developed with little theory to guide the construction of the full DAG. A short-cut is then to conceptualize all casual paths leading in to X and then try to close these by conditioning on relevant variables, mimicking an experiment where X is manipulated.\nClosing all paths going in to X is, in theory, analogous to manipulating X.\n\n\nCode\nshortcut &lt;- dagitty( \"dag {\n   W -&gt; Z1 -&gt; X\n   Y &lt;- V &lt;- Z2 -&gt; X &lt;- W\n   X -&gt; M -&gt; Y\n   Y &lt;- Z3 -&gt; X\n   Z2 &lt;- Z4 -&gt; X\n   Z4 -&gt; M\n   \n}\")\n\ncoordinates(shortcut) &lt;- list(\n  x = c(W = 1, Z1 = 2, X = 3, V = 5, Z2 = 4, M = 4, Y = 5, Z3 = 4, Z4 = 2.5),\n  y = c(W = 2, Z1 = 1, X = 1, V = 2, Z2 = 2, M = 1, Y = 1, Z3 = 1.5, Z4 = 1.5))\n\nplot(shortcut)\n\n\n\n\n\n\n\n\n\n\nThese are two helpful suggestions:\n\nAdjust for all pre-treatment variables. Doing so theoretically blocks all causal paths to the exposure variable, effectively eliminating confounding.\nAvoid adjusting for any post-treatment variables. As doing so can introduce overadjustment bias or collider bias.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#practice",
    "href": "13-method25.html#practice",
    "title": "13  M: Observational studies",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n13E1.\nSuppose that in the population of college applicants, being good at baseball is independent of having a good math score on a certain standardized test (with respect to some measure of ‘good’). A certain college has a simple admission procedure: admit an applicant if and only if the applicant is good at baseball or has a good math score or both. Give an intuitive explanation of why it makes sense that among students that this college admits, having a good math score is negatively associated with being good at baseball, i.e., conditioning on having a good math score decreases the chance of being good at baseball.\nOld exam question (stolen from Blitzstein & Hwang, 2019, an excellent introduction to probability)\n\n\n\n13E2. Adjustment of a mediator may lead to overadjustment bias or collider bias or both when estimating the total average causal effect of exposure on outcome. Explain.\n\n\n\n13E3. Many observational studies on violent video gaming and aggression only include boys. Why may this be a good idea?\n\n\n\nMedium\n\n13M1. Draw a simple DAG illustrating confounding, and discuss how the sign of the relationships may lead to negative confounding.\n\n\n\n13M2. Residual confounding is bias that remains despite our attempt to control for a confounder. Explain why residual confounding may be more of a problem if the covariate is controlled for by statistical control (adding it to a regression model) then by matching on the covariate.\n\n\n\n13M3. When estimating causal effects in experiments or observational studies, including variables in a regression model is a way of adjusting for covariates. Two simple rules when estimating total causal effects:\nRule 1. Adjust for all measured pre-treatment covariates.\nRule 2. Don’t adjust for any post-treatment covariates.\n\nExplain why Rule 1 is a good idea.\nExplain why Rule 2 is a good idea.\n\n\n\n\nHard\n\n13H1. Assume that X is a dichotomous variable that causes Y, and that Z causes both X and Y. Furthermore, assume that \\(Z \\rightarrow Y\\) is non-linear (e.g., quadratic). Simulate data and estimate the total average causal effect of X on Y in two ways:\n\nStatistical control: Add Z as covariate to the linear model: \\(Y = b_0 + b_1X + b_2Z\\).\nMatching: Only include pairs of X = 0 and X = 1 with similar values on Z (matching as data pruning).\n\nDiscuss the two methods ability to recover the true casual effect for your simulated scenario.\n\n\n\n13H2. Matching is an adjustment strategy that can be applied both at the design stage of a study and after data collection. When implemented after data collection, it is sometimes referred to as “data pruning.” Explain the rationale behind this terminology and discuss how matching differs when applied at the design stage versus after data collection.\n\n\n\n13H3. Two rules were mentioned above (13M3):\nRule 1: Adjust for all measured pre-treatment covariates. Rule 2: Avoid adjusting for any post-treatment covariates.\nThese are excellent general guidelines. However, like most rules, they come with exceptions!\n\nDraw a Directed Acyclic Graph (DAG) that represents a scenario where it would be beneficial to violate Rule 1.\nDraw a Directed Acyclic Graph (DAG) that represents a scenario where it would be beneficial to violate Rule 2.\n\n\n\n\n13H4. Researchers wanted to compare kids from a low and a high socioeconomic area on their response to a new pedagogic method for teaching math. They first administered a short math test to all kids in each area and noted the the average score among the kids from the high socioeconomic area was much higher than the average score among the kids from the low socioeconomic area. To achieve balanced groups, they took a random sample of 50 kids from the low socioeconomic area and then matched each kid with a kid from the high socioeconomic area with a similar score on the math test. Both groups were then subjected to the new pedagogic method, after which they were tested again on a similar test. The 50 low socioeconomic kids had about the same average result as before, whereas the 50 high socioeconomic kids had a higher average score after than before. The conclusion was that the method seems to work, but only for kids with a high socioeconomic background.\nGive an alternative explanation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "13-method25.html#session-info",
    "href": "13-method25.html#session-info",
    "title": "13  M: Observational studies",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       yaml_2.3.10       rlang_1.1.6      \n[17] jsonlite_2.0.0    V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nBlitzstein, J. K., & Hwang, J. (2019). Introduction to probability. Chapman; Hall/CRC.\n\n\nElwert, F., & Winship, C. (2014). Endogenous selection bias: The problem of conditioning on a collider variable. Annual Review of Sociology, 40, 31–53.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42.\n\n\nSnow, J. (1855). On the mode of communication of cholera. John Churchill.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>M: Observational studies</span>"
    ]
  },
  {
    "objectID": "14-stat25.html",
    "href": "14-stat25.html",
    "title": "14  S: Linear Regression: Non-linear Transformations and Interactions",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>S: Linear Regression: Non-linear Transformations and Interactions</span>"
    ]
  },
  {
    "objectID": "14-stat25.html#topics",
    "href": "14-stat25.html#topics",
    "title": "14  S: Linear Regression: Non-linear Transformations and Interactions",
    "section": "",
    "text": "Non-linear relationships\n\nlogarithmic, exponential and power functions become linear after suitable log-transformations.\n\nInteraction\n\nBetween categorical variables (ANOVA type of data)\nBetween categorical and continuous variables\nBetween continuous variables\n\nLinear interaction: Slope of one variable linearly related to another variable.\n\n\n\nReadings:\nGelman et al. (2021), Chapter 10 (p. 134-) and 12 (p. 186-) illustrate interaction analysis using the kid_iq data. Chapter 16 (p. 301-) discusses sample size requirements (“power”) for interaction analysis.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>S: Linear Regression: Non-linear Transformations and Interactions</span>"
    ]
  },
  {
    "objectID": "14-stat25.html#non-linear-transformations",
    "href": "14-stat25.html#non-linear-transformations",
    "title": "14  S: Linear Regression: Non-linear Transformations and Interactions",
    "section": "14.1 Non-linear transformations",
    "text": "14.1 Non-linear transformations\nNon-linear transformations of predictor or outcome variables, e.g.,\n\ndichtomization, if coded 0 and 1, the regression coefficient has a simple interpretation, namely the difference in outcome between units coded 1 and units coded 0 (called treatment coding or dummy coding). Dichotomization may be a way to make variables of unknown scale level usable in regression analysis, e.g., a questionnaire item, with response alternative “not at all”, “little”, “moderately”, “much”, “very much”, maybe be coded 1 for those who answered “much” or “very much” and zero for the others.\ncategorization, several categories, don’t forget to make them a factor if used as predictor in regression analysis.\nquadratic (as discussed last time), used to model curvilinear relationships, e.g., \\(Happiness = b_0 + b_1age + b_2(age^2)\\), people least happy in middle age (typically, Age would be centered in these analyses)\nlog-transform, may normalize variables (or residuals) with positive skew\nlog-transform and/or exponentiation of x and/or y-variables, to model logarithmic, exponential or or power law relationships between variables, as illustrated below (I used log base 2 in these illustrations just to make it easy to read: 1 unit increase is a doubling; Gelman et al. (2021) prefer the natural log, and I use this in the example in the weight and height example).\n\n\n\nCode\npar(mfrow = c(3, 2))\n\n# From logarithmic to linear\nx &lt;- 1:100\ny &lt;- log2(x)\nplot(x, y, cex = 0.8, main = \"Logarithmic function, y = log(x)\")\nlines(x, y, col = 'blue')\nplot(log2(x), y, cex = 0.8)\nlines(log2(x), y, col = 'blue')\n\n# From exponential to linear\nx &lt;- seq(0.1, 4, by = 0.05)\ny &lt;- exp(x)\nplot(x, y, cex = 0.8, main = \"Exponential function, y = exp(x)\")\nlines(x, y, col = 'blue')\nplot(x, log2(y), cex = 0.8)\nlines(x, log2(y), col = 'blue')\n# Or:\n# plot(exp(x), y, cex = 0.8)\n#lines(exp(x), y, col = 'blue')\n\n\n# From power-law to linear\nx &lt;- seq(0.1, 10, by = 0.1)\ny &lt;- 1*x^(log2(1.5))  # Doubling of x increases y with 1.5\nplot(x, y,  cex = 0.8, main = \"Power function, y = x^b\")\nlines(x, y, col = 'blue')\nplot(log2(x), log2(y),  cex = 0.8)\nlines(log2(x), log2(y), col = 'blue')\n\n\n\n\n\n\n\n\n\n\n\nNon-linear trend turned linear, example\nHere an example of a non-linear relationship that looks linear after log-transform. Anthropological data from the rethinking-package on height as a function of weight among children from a tribe in East Africa.\n\n\nCode\ndata(Howell1)  # From rethinking package\nd &lt;- Howell1\nrm(Howell1)\n\n# Only include children: &lt; 18 years old\nd2 &lt;- d[d$age &lt; 18, ]\nsummary(d2)\n\n\n     height           weight            age              male       \n Min.   : 53.98   Min.   : 4.252   Min.   : 0.000   Min.   :0.0000  \n 1st Qu.: 89.13   1st Qu.:11.708   1st Qu.: 3.000   1st Qu.:0.0000  \n Median :111.12   Median :16.981   Median : 7.000   Median :0.0000  \n Mean   :108.32   Mean   :18.414   Mean   : 7.722   Mean   :0.4792  \n 3rd Qu.:127.72   3rd Qu.:23.417   3rd Qu.:12.000   3rd Qu.:1.0000  \n Max.   :158.12   Max.   :44.736   Max.   :17.000   Max.   :1.0000  \n\n\n\nLinear fit doesn’t look too god!\n\n\nCode\npar(mfrow = c(1, 2))\n# Plot data and model predictions\nplot(d2$weight, d2$height, xlab = \"Weight [kg]\", ylab = \"Height [cm]\")\nmlin &lt;- lm(height ~ weight, data = d2)\nabline(mlin, col = \"blue\", lwd = 1.5)\n\n# Residual plot\nplot(mlin$fitted.values, mlin$residuals, \n     xlab = \"Fitted values [cm]\", ylab = \"Residuals [cm]\")\nabline(0, 0, col = \"black\", lty = 2)\nlines(lowess(mlin$fitted.values, mlin$residuals), col = \"red\", lty = 3)\nmtext(side = 3, text = \"Residual plot\", cex = 0.9)\n\n\n\n\n\n\n\n\n\nCode\nsummary(mlin)\n\n\n\nCall:\nlm(formula = height ~ weight, data = d2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.960  -5.083   1.640   6.048  19.142 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 58.23060    1.40454   41.46   &lt;2e-16 ***\nweight       2.72009    0.06865   39.62   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.481 on 190 degrees of freedom\nMultiple R-squared:  0.892, Adjusted R-squared:  0.8915 \nF-statistic:  1570 on 1 and 190 DF,  p-value: &lt; 2.2e-16\n\n\n\nLooks better (but not perfect!) with log-weight as predictor\n\n\nCode\npar(mfrow = c(1, 2))\n# Plot in log units\nplot(log(d2$weight), d2$height, xlab = \"log(Weight [kg])\", ylab = \"Height [cm]\")\nmlog &lt;- lm(height ~ log(weight), data = d2)\nabline(mlog, col = \"blue\", lwd = 1.5)\nsummary(mlog)\n\n\n\nCall:\nlm(formula = height ~ log(weight), data = d2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.3503  -3.0401   0.2292   2.9634  17.7553 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -32.8566     1.9179  -17.13   &lt;2e-16 ***\nlog(weight)  50.5309     0.6757   74.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.679 on 190 degrees of freedom\nMultiple R-squared:  0.9671,    Adjusted R-squared:  0.967 \nF-statistic:  5592 on 1 and 190 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Plot in linear units\nplot(d2$weight, d2$height, xlab = \"Weight [kg]\", ylab = \"Height [cm]\")\nxline &lt;- seq(1, 50, 0.1)\ncflog &lt;- mlog$coefficients\nyline &lt;- cflog[1] + cflog[2]*log(xline)\nlines(xline, yline, col = \"blue\", lwd = 1.5)\n\n\n\n\n\n\n\n\n\nCode\n# Residual plot\nplot(mlog$fitted.values, mlog$residuals, \n     xlab = \"Fitted values [cm]\", ylab = \"Residuals [cm]\")\nabline(0, 0, col = \"black\", lty = 2)\nlines(lowess(mlog$fitted.values, mlog$residuals), col = \"red\", lty = 3)\nmtext(side = 3, text = \"Residual plot\", cex = 0.9)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>S: Linear Regression: Non-linear Transformations and Interactions</span>"
    ]
  },
  {
    "objectID": "14-stat25.html#interactions",
    "href": "14-stat25.html#interactions",
    "title": "14  S: Linear Regression: Non-linear Transformations and Interactions",
    "section": "14.2 Interactions",
    "text": "14.2 Interactions\nWe will again use the data set in kidiq.txt to illustrate several points below. The data is from a survey of adult American women and their children. Gelman et al. (2021) describe the data set and use it at several places, e.g pp. 130-136, 156-158, 161, 185-187, and 197.\nCode book:\n\nkid_score  Test score children, on IQ-like measure (in the sample with mean and standard deviation of 87 and 20 points, respectively).\nmom_hs  Indicator variable: mother did complete high-school (1) or not (0)\nmom_iq  Test score mothers, transformed to IQ score: mean = 100, sd = 15.\nmom_work  Working status of mother, coded 1-4:\n\n1: mother did not work in first three years of child’s life,\n2: mother worked in second or third year of child’s life,\n3: mother worked part-time in first year of child’s life,\n4: mother worked full-time in first year of child’s life.\n\nmom_age Age of mothers, years\n\n\nImport data and transform variables:\n\n\nCode\nd &lt;- read.table(\"./datasets/kidiq.txt\", sep = \",\", header = TRUE)\n\n# Dichotomize mothers IQ (above mean = 1, below mean = 0), to be used as \n# indicator variable in regression analysis\nd$mom_iq_hi &lt;- 1 * (d$mom_iq &gt; mean(d$mom_iq))\n\n# Center mom_iq to be used as continuous variable in regression analysis\nd$mom_iq_c &lt;- d$mom_iq - mean(d$mom_iq)\n\n# Center mom_age to mean age \nd$mom_age_c &lt;- d$mom_age - mean(d$mom_age)\n\n# Factor variable of mom_work\nd$mom_work_f &lt;- factor(d$mom_work, levels = c(1, 2, 3, 4),\n                       labels = c(\"1_nowork\", \"2_worky2y3\", \n                                  \"3_parttime\", \"4_fulltime\"))\n\n# Linear transform kid_score to iq-scale: mean = 100, sd = 15\n# Can be done in one step, I prefer to do it in two steps, via z-scores\nzkid &lt;- (d$kid_score - mean(d$kid_score)) / sd(d$kid_score)\nd$kid_iq &lt;- zkid * 15 + 100\n\nstr(d)\n\n\n'data.frame':   434 obs. of  10 variables:\n $ kid_score : int  65 98 85 83 115 98 69 106 102 95 ...\n $ mom_hs    : int  1 1 1 1 1 0 1 1 1 1 ...\n $ mom_iq    : num  121.1 89.4 115.4 99.4 92.7 ...\n $ mom_work  : int  4 4 4 3 4 1 4 3 1 1 ...\n $ mom_age   : int  27 25 27 25 27 18 20 23 24 19 ...\n $ mom_iq_hi : num  1 0 1 0 0 1 1 1 0 0 ...\n $ mom_iq_c  : num  21.12 -10.64 15.44 -0.55 -7.25 ...\n $ mom_age_c : num  4.21 2.21 4.21 2.21 4.21 ...\n $ mom_work_f: Factor w/ 4 levels \"1_nowork\",\"2_worky2y3\",..: 4 4 4 3 4 1 4 3 1 1 ...\n $ kid_iq    : num  84 108.2 98.7 97.2 120.7 ...\n\n\n\nInteraction analysis with two categorical variables\nFactorial designs include all combinations of the levels of two (or more) categorical variables. They are often used for interaction analyses.\nHere illustrated with two dichotomous variables as predictors, mom_hs and mom_iq_hi, essentially a 2x2 between-participant ANOVA with kid_iq as outcome. Below sample sizes and statistics for kid_iq split by predictors:\n\n\nCode\nmy_stat &lt;- function(x) {\n  n &lt;- length(x)\n  m &lt;- mean(x, na.rm = TRUE)\n  s &lt;- sd(x, na.rm = TRUE)\n  se &lt;- s/sqrt(length(x))\n  out &lt;- c(n = n, m = m, sd = s, se = se)\n  out\n}\n\nss &lt;- aggregate(list(kid_iq = d$kid_iq), \n          list(mom_hs = d$mom_hs, mom_iqhi = d$mom_iq_hi), \n          my_stat)\nround(ss, 1)\n\n\n  mom_hs mom_iqhi kid_iq.n kid_iq.m kid_iq.sd kid_iq.se\n1      0        0     71.0     88.7      15.3       1.8\n2      1        0    168.0     97.4      13.9       1.1\n3      0        1     22.0    107.6      11.8       2.5\n4      1        1    173.0    106.2      12.7       1.0\n\n\nFit regression model:\n\\(y_i = Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1D_i + b_2G_i + b_3D_iG_i\\),\nwhere D is the dichotomous mom_hs variable and G is the dichotomous mom_iq_hi variable.\nThis linear model can be broken down in four parts, one for each of the 2x2 combinations of predictors:\n\n\n\nmom_hs\nmom_iq_hi\npredicted mean\nobserved mean\n\n\n\n\n0\n0\n\\(b_0\\)\n88.7\n\n\n1\n0\n\\(b_0 + b_1\\)\n97.4\n\n\n0\n1\n\\(b_0 + b_2\\)\n107.6\n\n\n1\n1\n\\(b_0 + b_1 + b_2 + b_3\\)\n106.2\n\n\n\n\nFit model\n\n\nCode\nd_interact &lt;- glm(kid_iq ~ mom_hs + mom_iq_hi + mom_hs*mom_iq_hi, data = d)\nsummary(d_interact)\n\n\n\nCall:\nglm(formula = kid_iq ~ mom_hs + mom_iq_hi + mom_hs * mom_iq_hi, \n    data = d)\n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        88.742      1.612  55.056  &lt; 2e-16 ***\nmom_hs              8.624      1.923   4.486 9.32e-06 ***\nmom_iq_hi          18.856      3.314   5.690 2.36e-08 ***\nmom_hs:mom_iq_hi  -10.012      3.626  -2.761  0.00601 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 184.4612)\n\n    Null deviance: 97425  on 433  degrees of freedom\nResidual deviance: 79318  on 430  degrees of freedom\nAIC: 3502\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nconfint(d_interact)\n\n\nWaiting for profiling to be done...\n\n\n                      2.5 %   97.5 %\n(Intercept)       85.583239 91.90156\nmom_hs             4.856419 12.39250\nmom_iq_hi         12.360588 25.35126\nmom_hs:mom_iq_hi -17.118388 -2.90527\n\n\n\nVisualize model\n\n\nCode\n# Empty plot\nplot(d$mom_iq_hi, d$kid_iq, pch = '', xlim = c(-0.2, 1.2), ylim = c(50, 150),\n     axes = FALSE, xlab = \"Mother IQ below (0) or above (1) average\",\n     ylab = \"Kid IQ\")\n\n# Add data points, with some jitter along x for visibility\noffset &lt;- 0.02\nj0 &lt;- rnorm(length(d$mom_iq_hi[d$mom_hs == 0]), -offset, 0.01)  # x-jitter\nj1 &lt;- rnorm(length(d$mom_iq_hi[d$mom_hs == 1]), offset, 0.01)  # x-jitter\npoints(d$mom_iq_hi[d$mom_hs == 0] + j0, d$kid_iq[d$mom_hs == 0], \n       pch = 21, bg = \"blue\")\npoints(d$mom_iq_hi[d$mom_hs == 1] + j1, d$kid_iq[d$mom_hs == 1],\n       pch = 21, bg = \"white\")\n\n# Add axis\naxis(1, at = c(-0.2, 0, 1, 1.2), labels = c(\"\", \"0\", \"1\", \"\"), pos = 50)\naxis(2, at = seq(50, 150, 10), pos = -0.2, las = 2)\n\n# Add regression lines\ncf &lt;- d_interact$coefficients\nxline &lt;- c(0, 1)\nyline0 &lt;- c(cf[1], cf[1] + cf[3])\nyline1 &lt;- c(cf[1] + cf[2], sum(cf))\nlines(xline - offset, yline0, col = \"blue\")\nlines(xline + offset, yline1, lty = 2)\n\n\n\n\n\n\n\n\n\n\nCompare with Analysis of variance (ANOVA), same analysis but less informative output. (aov() would be exactly the same as using lm() and another set of contrasts)\n\n\nCode\na1 &lt;- aov(kid_iq ~ mom_hs + mom_iq_hi + mom_hs:mom_iq_hi, data = d)\nsummary(a1)\n\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmom_hs             1   5468    5468  29.645 8.74e-08 ***\nmom_iq_hi          1  11232   11232  60.890 4.63e-14 ***\nmom_hs:mom_iq_hi   1   1406    1406   7.624  0.00601 ** \nResiduals        430  79318     184                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients and confidence intervals from ANOVA model\n\n\nCode\na1$coefficients\n\n\n     (Intercept)           mom_hs        mom_iq_hi mom_hs:mom_iq_hi \n       88.742397         8.624461        18.855922       -10.011829 \n\n\nCode\nconfint(a1)\n\n\n                      2.5 %    97.5 %\n(Intercept)       85.574322 91.910472\nmom_hs             4.845784 12.403138\nmom_iq_hi         12.342254 25.369591\nmom_hs:mom_iq_hi -17.138447 -2.885211\n\n\n\n\n\nInteraction analysis with one categorical and one continuous variable\nLet’s start with an additive model, using mom_hs and mom_iq as independent variables\n\\(y_i = Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1D_i + b_2x_i\\),\nwhere D is the dichotomous mom_hs variable and x is the continuous variable mom_iq.\n\nFit model, using glm():\n\n\nCode\n# Adjusted: + mom_iq\nm1 &lt;- glm(kid_iq ~ mom_hs + mom_iq_c, data = d)\nsummary(m1)\n\n\n\nCall:\nglm(formula = kid_iq ~ mom_hs + mom_iq_c, data = d)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 96.56423    1.42844  67.601  &lt; 2e-16 ***\nmom_hs       4.37279    1.62548   2.690  0.00742 ** \nmom_iq_c     0.41442    0.04452   9.309  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 177.6375)\n\n    Null deviance: 97425  on 433  degrees of freedom\nResidual deviance: 76562  on 431  degrees of freedom\nAIC: 3484.6\n\nNumber of Fisher Scoring iterations: 2\n\n\nVisualize additive model:\n\n\nCode\n## Draw figure of additive model\n\n# Empty plot\nplot(d$mom_iq_c, d$kid_iq, axes = FALSE, pch = \"\",\n     xlim = c(-30, 40), ylim = c(50, 150), xlab = \"Mother IQ\", ylab = \"Kid IQ\")\n# Add points for mom_hs = 0 (blue) and mom_hs = 1 )(white)\npoints(d$mom_iq_c[d$mom_hs == 0], d$kid_iq[d$mom_hs == 0], \n       pch = 21, bg = \"blue\")\npoints(d$mom_iq_c[d$mom_hs == 1], d$kid_iq[d$mom_hs == 1], \n       pch = 21, bg = \"white\")\n# Add regression lines\ncf &lt;- m1$coefficients  # Coefficients of model\nxline &lt;- c(-30, 40)  # Two points along the x-axis\nyline0 &lt;- cf[1] + cf[3]*xline # Predictions for mom_hs = 0\nyline1 &lt;- cf[1] + cf[2] + cf[3]*xline # Predictions for mom_hs = 1\nlines(xline, yline0 , col = \"blue\")\nlines(xline, yline1 , col = \"black\", lty = 3)\n\n# Confidence bands around regression line (+- 2SE of predicted mean) --\n# Calculate CI-bands mom_hs = 0  \nxline &lt;- seq(-30, 40, 0.1)\nhs0 &lt;- rep(0, length(xline))\nnewdata0 &lt;- data.frame(mom_iq_c = xline, mom_hs = hs0)\npredict0 &lt;- predict(m1, newdata=newdata0, se.fit = TRUE)\nci0lo &lt;- predict0$fit - 2*predict0$se.fit\nci0hi &lt;- predict0$fit + 2*predict0$se.fit\n\n# Shade area mom_hs = 0 \npolygon(x = c(xline, rev(xline)), y = c(ci0lo, rev(ci0hi)), \n        col =  rgb(0, 0, 1, 0.1), border = FALSE)      \n\n# Calculate CI-bands mom_hs = 1 \nhs1 &lt;- rep(1, length(xline))\nnewdata1 &lt;- data.frame(mom_iq_c = xline, mom_hs = hs1)\npredict1 &lt;- predict(m1, newdata=newdata1, se.fit = TRUE)\nci1lo &lt;- predict1$fit - 1.96*predict1$se.fit\nci1hi &lt;- predict1$fit + 1.96*predict1$se.fit\n\n\n# Shade area mom_hs = 1 \npolygon(x = c(xline, rev(xline)), y = c(ci1lo, rev(ci1hi)), \n        col =  rgb(0, 0, 0, 0.1), border = FALSE)  # shaded area\n# ---\n\n# Add axis (transforming back mom_iq to mean = 100)\naxis(1, at = seq(-30, 40, 10), \n     labels = seq(-30, 40, 10) + mean(d$mom_iq), # Transforming back to mom_iq\n     pos = 50) \naxis(2, at = seq(50, 150, 10), las = 2, pos = -30)\n\n# Add legend\npoints(-25, 148, pch = 21, bg = \"blue\", cex = 0.8)\npoints(-25, 143, pch = 21, bg = \"white\", cex = 0.8)\ntext(x = -25, y = 148, labels = \"Mother did not complete high-school\", \n     pos = 4, cex = 0.7)\ntext(x = -25, y = 143, labels = \"Mother completed high-school\", \n     pos = 4, cex = 0.7)\n\n\n\n\n\n\n\n\n\nThe model implies parallel lines, but the data seem to suggest that a steeper slope should be considered for mom_hs = 0 (blue points) compared to for mom_hs = 1 (white points). Different slope is the same as an interaction, so let’s add an interaction term:\n\\(y_i = Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1D_i + b_2x_i + b_3D_ix_i\\).\nFit model with interaction:\n\n\nCode\nm1_interact &lt;- glm(kid_iq ~ mom_hs + mom_iq_c + mom_hs:mom_iq_c, data = d)\nsummary(m1_interact)\n\n\n\nCall:\nglm(formula = kid_iq ~ mom_hs + mom_iq_c + mom_hs:mom_iq_c, data = d)\n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      98.9782     1.6302  60.716  &lt; 2e-16 ***\nmom_hs            2.0877     1.7834   1.171  0.24239    \nmom_iq_c          0.7120     0.1090   6.531 1.84e-10 ***\nmom_hs:mom_iq_c  -0.3559     0.1192  -2.985  0.00299 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 174.4352)\n\n    Null deviance: 97425  on 433  degrees of freedom\nResidual deviance: 75007  on 430  degrees of freedom\nAIC: 3477.7\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nconfint(m1_interact)\n\n\nWaiting for profiling to be done...\n\n\n                     2.5 %      97.5 %\n(Intercept)     95.7831137 102.1733465\nmom_hs          -1.4076660   5.5830621\nmom_iq_c         0.4983719   0.9257190\nmom_hs:mom_iq_c -0.5895545  -0.1222411\n\n\n\nVisualize interaction model:\n\n\nCode\nm1_interact &lt;- glm(kid_iq ~ mom_hs + mom_iq_c + mom_hs:mom_iq_c, data = d)\nsummary(m1_interact)\n\n\n\nCall:\nglm(formula = kid_iq ~ mom_hs + mom_iq_c + mom_hs:mom_iq_c, data = d)\n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      98.9782     1.6302  60.716  &lt; 2e-16 ***\nmom_hs            2.0877     1.7834   1.171  0.24239    \nmom_iq_c          0.7120     0.1090   6.531 1.84e-10 ***\nmom_hs:mom_iq_c  -0.3559     0.1192  -2.985  0.00299 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 174.4352)\n\n    Null deviance: 97425  on 433  degrees of freedom\nResidual deviance: 75007  on 430  degrees of freedom\nAIC: 3477.7\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nconfint(m1_interact)\n\n\nWaiting for profiling to be done...\n\n\n                     2.5 %      97.5 %\n(Intercept)     95.7831137 102.1733465\nmom_hs          -1.4076660   5.5830621\nmom_iq_c         0.4983719   0.9257190\nmom_hs:mom_iq_c -0.5895545  -0.1222411\n\n\nCode\n# Empty plot\nplot(d$mom_iq_c, d$kid_iq, axes = FALSE, pch = \"\",\n     xlim = c(-30, 40), ylim = c(50, 150), xlab = \"Mother IQ\", ylab = \"Kid IQ\")\n# Add points for mom_hs = 0 (blue) and mom_hs = 1 )(white)\npoints(d$mom_iq_c[d$mom_hs == 0], d$kid_iq[d$mom_hs == 0], \n       pch = 21, bg = \"blue\")\npoints(d$mom_iq_c[d$mom_hs == 1], d$kid_iq[d$mom_hs == 1], \n       pch = 21, bg = \"white\")\n\n# Add regression lines\ncf &lt;- m1_interact$coefficients  # Coefficients of model\nxline &lt;- c(-30, 40)  # Two points along the x-axis\nyline0 &lt;- cf[1] + cf[3]*xline # Predictions for mom_hs = 0\nyline1 &lt;- cf[1] + cf[2] + (cf[3]+cf[4])*xline # Predictions for mom_hs = 1\nlines(xline, yline0 , col = \"blue\")\nlines(xline, yline1 , col = \"black\", lty = 3)\n\n# Add axis (transforming back mom_iq to mean = 100)\naxis(1, at = seq(-30, 40, 10), \n     labels = seq(-30, 40, 10) + mean(d$mom_iq), # Transforming back to mom_iq\n     pos = 50) \naxis(2, at = seq(50, 150, 10), las = 2, pos = -30)\n\n# Add legend\npoints(-25, 148, pch = 21, bg = \"blue\", cex = 0.8)\npoints(-25, 143, pch = 21, bg = \"white\", cex = 0.8)\ntext(x = -25, y = 148, labels = \"Mother did not complete high-school\", \n     pos = 4, cex = 0.7)\ntext(x = -25, y = 143, labels = \"Mother completed high-school\", \n     pos = 4, cex = 0.7)\n\n# Confidence bands around regression line (+/- 2SE of predicted mean)\n# Calculate CI-bands mom_hs = 0  \nxline &lt;- seq(-30, 40, 0.1)\nhs0 &lt;- rep(0, length(xline))\nnewdata0 &lt;- data.frame(mom_iq_c = xline, mom_hs = hs0)\npredict0 &lt;- predict(m1_interact, newdata=newdata0, se.fit = TRUE)\nci0lo &lt;- predict0$fit - 2*predict0$se.fit\nci0hi &lt;- predict0$fit + 2*predict0$se.fit\n\n# Shade area mom_hs = 0 \npolygon(x = c(xline, rev(xline)), y = c(ci0lo, rev(ci0hi)), \n        col =  rgb(0, 0, 1, 0.1), border = FALSE)      \n\n# Calculate CI-bands mom_hs = 1 \nhs1 &lt;- rep(1, length(xline))\nnewdata1 &lt;- data.frame(mom_iq_c = xline, mom_hs = hs1)\npredict1 &lt;- predict(m1_interact, newdata=newdata1, se.fit = TRUE)\nci1lo &lt;- predict1$fit - 1.96*predict1$se.fit\nci1hi &lt;- predict1$fit + 1.96*predict1$se.fit\n\n# Shade area mom_hs = 1 \npolygon(x = c(xline, rev(xline)), y = c(ci1lo, rev(ci1hi)), \n        col =  rgb(0, 0, 0, 0.1), border = FALSE)  # shaded area\n\n\n\n\n\n\n\n\n\n\nPlease compare the interaction model to doing two separate regression analysis, one for each of the two groups defined by mom_hs. Note that this allows different \\(\\sigma\\) for the two groups, otherwise coefficients should be very similar.\nBivariate model for kids to mothers who did not complete high-school:\n\\(y_i = Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = a_0 + a_1x_i, \\ \\ \\ \\  D_i = 0\\),\n\n\nCode\n# Data frame with only mom_hs == 0\nhs0 &lt;- d[d$mom_hs == 0, ]  \nm_hs0 &lt;- glm(kid_iq ~ mom_iq_c, data = hs0)\nm_hs0\n\n\n\nCall:  glm(formula = kid_iq ~ mom_iq_c, data = hs0)\n\nCoefficients:\n(Intercept)     mom_iq_c  \n     98.978        0.712  \n\nDegrees of Freedom: 92 Total (i.e. Null);  91 Residual\nNull Deviance:      25320 \nResidual Deviance: 17880    AIC: 759\n\n\nCode\nconfint(m_hs0)\n\n\nWaiting for profiling to be done...\n\n\n                 2.5 %      97.5 %\n(Intercept) 95.5873044 102.3691558\nmom_iq_c     0.4852771   0.9388138\n\n\n\n… and bivariate model for kids to mothers who did complete high-school\n\\(y_i = Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = g_0 + g_1x_i, \\ \\ \\ \\  D_i = 1\\).\n\n\nCode\n# Data frame with only mom_hs == 1\nhs1 &lt;- d[d$mom_hs == 1, ]  \nm_hs1 &lt;- glm(kid_iq ~ mom_iq_c, data = hs1)\nm_hs1\n\n\n\nCall:  glm(formula = kid_iq ~ mom_iq_c, data = hs1)\n\nCoefficients:\n(Intercept)     mom_iq_c  \n   101.0659       0.3561  \n\nDegrees of Freedom: 340 Total (i.e. Null);  339 Residual\nNull Deviance:      66640 \nResidual Deviance: 57130    AIC: 2720\n\n\nCode\nconfint(m_hs1)\n\n\nWaiting for profiling to be done...\n\n\n                 2.5 %      97.5 %\n(Intercept) 99.6728441 102.4590121\nmom_iq_c     0.2632179   0.4490773\n\n\n\n\n\nInteraction analysis with two continous variables\nLinear interaction: Slope of one variable linearly related to another variable:\n\\(Y = a_0 + a_1X_1 + a_2X_2\\), and\n\\(a_1 = b_0 + b_1X_2\\)\n\nCombining these two gives:\n\\(Y = a_0 + (b_0 + b_1X_2)X_1 + a_2X_2\\)\n\\(Y = a_0 + (b_0X_1 + b_1X_2X_1) + a_2X_2\\)\n\\(Y = a_0 + b_0X_1 + a_2X_2 + b_1X_1X_2\\)\nThe same would have been obtained had slope of \\(X_2\\) been linearly related to slope of \\(X_1\\).\nHere an example with mom_iq and mom_age as continuous variables in an interaction analysis. It is generally a good idea to use centered predictors.\n\n\nCode\nm2 &lt;- glm(kid_iq ~ mom_iq_c + mom_age_c + mom_iq_c:mom_age_c, data = d)\nsummary(m2)\n\n\n\nCall:\nglm(formula = kid_iq ~ mom_iq_c + mom_age_c + mom_iq_c:mom_age_c, \n    data = d)\n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        100.05798    0.64674 154.711   &lt;2e-16 ***\nmom_iq_c             0.45122    0.04386  10.288   &lt;2e-16 ***\nmom_age_c            0.31029    0.24107   1.287    0.199    \nmom_iq_c:mom_age_c  -0.01566    0.01584  -0.989    0.323    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 180.0384)\n\n    Null deviance: 97425  on 433  degrees of freedom\nResidual deviance: 77417  on 430  degrees of freedom\nAIC: 3491.5\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nconfint(m2)\n\n\nWaiting for profiling to be done...\n\n\n                         2.5 %       97.5 %\n(Intercept)        98.79039224 101.32557605\nmom_iq_c            0.36526144   0.53718097\nmom_age_c          -0.16220394   0.78277753\nmom_iq_c:mom_age_c -0.04670478   0.01538788\n\n\n\nVisualize model, always needed to understand the meaning of linear interactions. Here illustrated in separate plots for low, medium and high levels of age.\n\n\nCode\n# User-made plot functions\nempty_plot &lt;- function() {\n  # Empty plot\nplot(d$mom_iq_c, d$kid_iq, axes = FALSE, pch = \"\",\n     xlim = c(-30, 40), ylim = c(80, 130), xlab = \"Mother IQ\", ylab = \"Kid IQ\")\n}\n\nadd_data &lt;- function(a) {\n  \n  # points(d$mom_iq_c, d$kid_iq, pch = 21, bg = \"grey\")\n\n  # Add confidence band : +/- 2SE\n  cf &lt;- m2$coefficientscf \n  xline &lt;- seq(-30, 40, 0.1)\n  age &lt;- rep(a, length(xline)) # age_c  = a \n  newdata &lt;- data.frame(mom_iq_c = xline, mom_age_c = age)\n  mm &lt;- predict(m2, newdata = newdata)\n  pred &lt;- predict(m2, newdata = newdata, se.fit = TRUE)\n  cilo &lt;- pred$fit - 2*pred$se.fit\n  cihi &lt;- pred$fit + 2*pred$se.fit\n  polygon(x = c(xline, rev(xline)), y = c(cilo, rev(cihi)), \n        col =  rgb(0, 0, 0, 0.1), border = FALSE)  # shaded area\n  \n  # Add regression line\n  lines(xline, mm, lty = 2)\n  \n  # Add axis (transforming back mom_iq to mean = 100)\n  axis(1, at = seq(-30, 40, 20), \n     labels = seq(-30, 40, 20) + mean(d$mom_iq), # Transforming back to mom_iq\n     pos = 80) \n  axis(2, at = seq(50, 150, 10), las = 2, pos = -30)\n}\n\n# Draw three plots\npar(mfrow = c(1, 3))\n\n# age_c =  -2, approx 21 y\nempty_plot()\nadd_data(a = -4)\ntext(-10, 125, \"Mother's age 19 y\", cex = 1, pos = 4)\n\n# age_c =  0, approx 23 y\nempty_plot()\nadd_data(a = 0)\ntext(-10, 125, \"Mother's age 23 y\", cex = 1, pos = 4)\n\n# age_c =  2, approx 25 y\nempty_plot()\nadd_data(a = 4)\ntext(-10, 125, \"Mother's age 27 y\", cex = 1, pos = 4)\n\n\n\n\n\n\n\n\n\n\nFigure clearly shows similar slopes for the three age groups, in line with the regression output suggesting no strong linear interaction.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>S: Linear Regression: Non-linear Transformations and Interactions</span>"
    ]
  },
  {
    "objectID": "14-stat25.html#practice",
    "href": "14-stat25.html#practice",
    "title": "14  S: Linear Regression: Non-linear Transformations and Interactions",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n14E1. Give examples of when you would consider to log-transform …\n\nA predictor variable.\nAn outcome variable.\n\n\n\n\n14E2.\nBelow is a plot of age and waist circumference in a sample of Swedish women (open circles) and men (filled circles). The lines show the fit of the linear model\n\\(Waist = b_0 + b_1(Age-20) + b_2Male + b_3((Age-20)*Male)\\),\nwhere \\(Waist\\) is waist circumference in centimeters, \\(Age\\) is in years and \\(Male\\) is an indicator variable coded 0 for women and 1 for men. The solid line is for women (\\(Male = 0\\)) and the dashed line for men (\\(Male = 1\\))\nEstimate by eye the four coefficients \\(b_0, ..., b_3\\)\n\n\n\n\n\n\n\n\n\n\n\n\n14E3.\nThe figure in 14E2 has two lines.\n\\(y = a_0 + a_1Age\\) for females (solid line), and\n\\(y = d_0 + d_1Age\\) for males (dotted line).\nRelate each of these coefficients (\\(a_0, a_1, d_0, d_1\\)) to the coefficients of the fitted model \\(b_0, b_1, b_2, b_3\\) displayed in 14E2.\n\nHere is the model fit from 14E2!\n\n\nCode\nround(cf, 1)\n\n\n(Intercept)       age20        male  age20:male \n       88.7         0.2         7.2         0.2 \n\n\n\n\n\nMedium\n\n14M1. Interactions should always be understood in relation to the statistical model. An interaction with the outcome in linear units may not be present if the outcome is modeled in log-units. Explain and visualize.\n\n\n\n\nHard\n\n14H1. Use stan_glm() to fit the interaction model above: kid_iq ~ mom_hs + mom_iq_c + mom_hs:mom_iq_c and draw the figure with CI-bands around model predictions (x-axis: mom_iq (non-centered!), separate lines for mom_hs)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>S: Linear Regression: Non-linear Transformations and Interactions</span>"
    ]
  },
  {
    "objectID": "14-stat25.html#session-info",
    "href": "14-stat25.html#session-info",
    "title": "14  S: Linear Regression: Non-linear Transformations and Interactions",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n[1] rstanarm_2.32.1     Rcpp_1.0.14         rethinking_2.42    \n[4] posterior_1.6.1     cmdstanr_0.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     dplyr_1.1.4          farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] shinystan_2.6.0      promises_1.3.3       shinyjs_2.1.0       \n[10] digest_0.6.37        mime_0.13            lifecycle_1.0.4     \n[13] StanHeaders_2.32.10  survival_3.7-0       processx_3.8.6      \n[16] magrittr_2.0.3       compiler_4.4.2       rlang_1.1.6         \n[19] tools_4.4.2          igraph_2.1.4         yaml_2.3.10         \n[22] knitr_1.50           htmlwidgets_1.6.4    pkgbuild_1.4.8      \n[25] curl_6.4.0           plyr_1.8.9           RColorBrewer_1.1-3  \n[28] dygraphs_1.1.1.6     abind_1.4-8          miniUI_0.1.2        \n[31] grid_4.4.2           stats4_4.4.2         xts_0.14.1          \n[34] xtable_1.8-4         inline_0.3.21        ggplot2_3.5.2       \n[37] scales_1.4.0         gtools_3.9.5         MASS_7.3-61         \n[40] cli_3.6.5            mvtnorm_1.3-3        rmarkdown_2.29      \n[43] reformulas_0.4.1     generics_0.1.4       RcppParallel_5.1.10 \n[46] rstudioapi_0.17.1    reshape2_1.4.4       minqa_1.2.8         \n[49] rstan_2.32.7         stringr_1.5.1        splines_4.4.2       \n[52] shinythemes_1.2.0    bayesplot_1.13.0     matrixStats_1.5.0   \n[55] base64enc_0.1-3      vctrs_0.6.5          boot_1.3-31         \n[58] Matrix_1.7-1         V8_6.0.4             jsonlite_2.0.0      \n[61] crosstalk_1.2.1      glue_1.8.0           nloptr_2.2.1        \n[64] codetools_0.2-20     ps_1.9.1             DT_0.33             \n[67] distributional_0.5.0 stringi_1.8.7        shape_1.4.6.1       \n[70] gtable_0.3.6         later_1.4.2          QuickJSR_1.8.0      \n[73] lme4_1.1-37          tibble_3.3.0         colourpicker_1.3.0  \n[76] pillar_1.10.2        htmltools_0.5.8.1    R6_2.6.1            \n[79] Rdpack_2.6.4         evaluate_1.0.3       shiny_1.11.1        \n[82] lattice_0.22-6       markdown_2.0         rbibutils_2.3       \n[85] backports_1.5.0      threejs_0.3.4        httpuv_1.6.16       \n[88] rstantools_2.4.0     nlme_3.1-166         coda_0.19-4.1       \n[91] gridExtra_2.3        checkmate_2.3.2      xfun_0.52           \n[94] zoo_1.8-14           pkgconfig_2.0.3     \n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>S: Linear Regression: Non-linear Transformations and Interactions</span>"
    ]
  },
  {
    "objectID": "15-method25.html",
    "href": "15-method25.html",
    "title": "15  M: Fallacies and paradoxes",
    "section": "",
    "text": "Topics\nHere a lsit of some errors, fallacies, and paradoxes",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "15-method25.html#topics",
    "href": "15-method25.html#topics",
    "title": "15  M: Fallacies and paradoxes",
    "section": "",
    "text": "Examples of errors, fallacies and paradoxes related to:\n\nRandom fluctuations\nReversed causation\nConfounding\nSelection\n\nSimpson’s paradox\nThe obesity paradox\nMediation analysis and risk for colldier bias\n\nTheoretical articles to read:\n\nPearl (2014) on Simpson’s paradox, analyzed using directed acyclical graphs (DAGs).\nBanack & Kaufman (2014) on the Obesity paradox\nCheck out the the Simpson machine. Described in Pearl (2014), implemented on dagitty.net.\nElwert & Winship (2014) on bias that may arise from conditioning on colliders, e.g., mediation fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom fluctuation\nReversed causation\nConfounding\nSelection\n\n\n\n\nSampling error\nRecall bias\nSimpson I\nSimpson II (overadjustment bias)\n\n\nRegression fallacy\nEarly disease onset\nEcological fallacy\nCollider bias (Berkson error)\n\n\nSmall sample fallacy\nDiagnosis changes risk behavior\nMono-method bias\nAttrition bias",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "15-method25.html#random-errors",
    "href": "15-method25.html#random-errors",
    "title": "15  M: Fallacies and paradoxes",
    "section": "15.1 Random errors",
    "text": "15.1 Random errors\n\nRegression fallacy\nRegression fallacy: Regression to the mean interpreted as a causal effect.\nRemember: A flurry of deaths by natural causes in a village led to speculation about some new and unusual threat. A group of priests attributed the problem to the sacrilege of allowing women to attend funerals, formerly a forbidden practice. The remedy was a decree that barred women from funerals in the area. The decree was quickly enforced, and the rash of unusual deaths subsided. This proves that the priests were correct.\n\n\nSmall sample fallacy\nFunnel plot\n\n\nCode\nset.seed(123)\n\n# Sample sizes\nnn &lt;- sample(10:1000, size = 250, replace = TRUE)  # Random uniform\n\n# Outcome\nx &lt;- sapply(nn, function(x) mean(rnorm(x)))  # True effect = 0\n\n# n for small effect sizes\nlargest_neg &lt;- nn[x == min(x)]  # Sample size in study with smallest effect size\nlarge_neg &lt;- nn[x &lt; -0.2]     # Sample sizes in studies with largest negative effect sizes\n\n# n for large effect sizes\nlargest_pos &lt;- nn[x == max(x)] # Sample size in study with largest effect size\nlarge_pos &lt;- nn[x &gt; 0.2]     # Sample sizes in studies with largest psoitive effect sizes\n\n# Historgam of sampel sizes\n# hist(nn)\n\n# Funnel plot\nplot(x, nn, xlim = c(-0.5, 0.5), ylab = \"Study size\", xlab = \"Outcome\")\n\n\n\n\n\n\n\n\n\nCode\n# Sampel sizes in studies with large effect sizes\nlargest_neg \n\n\n[1] 32\n\n\nCode\nlarge_pos\n\n\n[1] 29",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "15-method25.html#reversed-causation",
    "href": "15-method25.html#reversed-causation",
    "title": "15  M: Fallacies and paradoxes",
    "section": "15.2 Reversed causation",
    "text": "15.2 Reversed causation\n\nRecall bias\n\n\nCode\nlibrary(dagitty) # R version of http://www.dagitty.net\n\npizza &lt;- dagitty( \"dag {\n   Pizza -&gt; Pizza_recall\n   CVD -&gt; Pizza_recall\n}\")\n\ncoordinates(pizza) &lt;- list(\n  x = c(Pizza = 1, Pizza_recall = 2, CVD = 4),\n  y = c(Pizza = 1, Pizza_recall = 0, CVD = 1))\n\nplot(pizza)\n\n\n\n\n\n\n\n\n\n\nIn the early 2000s, there was considerable publicity arising from a claim that the measles, mumps and rubella (MMR) vaccine was related to and possibly caused autism in children (the originating claim was subsequently found to be based on fraudulent data and the publication was withdrawn) (Andrews 2002). Researchers found that parents of autistic children diagnosed after the publicity tended to recall the start of autism as being soon after the MMR jab more often than parents of similar children who were diagnosed prior to the publicity. Source: Catalog of bias\n\n\n\n\n\n\n\n\n\n\n\n\nEarly disease onset",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "15-method25.html#confounding",
    "href": "15-method25.html#confounding",
    "title": "15  M: Fallacies and paradoxes",
    "section": "15.3 Confounding",
    "text": "15.3 Confounding\n\nEcological fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMono-method bias\n\n\n\n\n\n\n\n\n\nLatent variables (unmeasured): Job_Satisfaction, Lunch_Room_Satisfaction, Response bias\nManifest variables (measured): JS_score, LRS_score\n\n\n\nSimpson’s paradox\nTo aggregate or disaggregate, that is the question! Causal inference need assumptions (a casual story), data is not enough.\n\nSimpson’s paradox I: Truth in disaggregated data\nSimpson’s paradox II: Truth in aggregated data\n\n\nA demonstration of the paradox “… comes from the field of law and concerns the influence of race on death sentences in the US. One paper showed the death sentence rate versus race of the offender, stratified by race of the victim, for a number of states. The tables for the state of Indiana reveal Simpson’s paradox (Table 3). In Indiana whites are nearly twice as likely to receive the death penalty as African-Americans. However, when the data are stratified by the race of the victim, it is African-Americans who have the higher death sentence rate. This occurs both when the victim is white and when the victim is African-American.” (Norton & Divine, 2015)\nExercise: Draw DAG consistent with the table below.\n\n\n\n\nNorton & Divine (2015), Table 3\n\n\n\nAnd again: Draw DAG consistent with the table below.\nStory: “Suppose (hypothetical) data are analysed to determine whether a new treatment (A) is superior to the standard treatment (B) for septic shock. The combined data show that the proportion surviving to hospital discharge is 86% with treatment A, but only 70% with treatment B. However, if the patients are stratified into two subgroups, depending on whether their diastolic blood pressure (DBP) is less than 50 mmHg, within each stratum (Table 4) the proportions of patients alive at hospital discharge are identical for each treatment.” Norton & Divine (2015)\n\n\n\nNorton & Divine (2015), Table 4\n\n\n\ncf. Pearl (2014), Fig 1:\n\n\nand the Simpson machine\n\n\nCode\n# Code taken from http://www.dagitty.net/learn/simpson/index.html\n\nsimpson.simulator &lt;- function(N,s,ce){\n    Z1 &lt;- rnorm(N,0,s)\n    Z3 &lt;- rnorm(N,0,s) + Z1\n    Z5 &lt;- rnorm(N,0,s) + Z3\n    U &lt;- rnorm(N,0,s) + Z1\n    Z4 &lt;- rnorm(N,0,s) + Z5 + U\n    Z2 &lt;- rnorm(N,0,s) + Z3 + U\n    X &lt;- rnorm(N,0,s) + U\n    Y &lt;- rnorm(N,0,s) + ce*X + 10*Z5\n    data.frame(Y,X,Z1,Z2,Z3,Z4,Z5)\n}\n\n# 1st parameter: sample size\n# 2nd parameter: noise standard deviation\n# 3rd parameter: true causal effect\nD &lt;- simpson.simulator(1000,0.1,1)\n\n# adjusted for {Z1, Z2}\nm &lt;- lm(D[,c(1,2,3, 4)])\nsummary(m)\n\n\n\nCall:\nlm(formula = D[, c(1, 2, 3, 4)])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1176 -0.8536 -0.0530  0.7925  5.0937 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.004531   0.041454  -0.109  0.91298    \nX           -1.046319   0.332368  -3.148  0.00169 ** \nZ1           4.051204   0.664769   6.094 1.57e-09 ***\nZ2           4.045906   0.259529  15.589  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.31 on 996 degrees of freedom\nMultiple R-squared:  0.5024,    Adjusted R-squared:  0.5009 \nF-statistic: 335.1 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(m,'X')\n\n\n      2.5 %     97.5 %\nX -1.698541 -0.3940963",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "15-method25.html#selection",
    "href": "15-method25.html#selection",
    "title": "15  M: Fallacies and paradoxes",
    "section": "15.4 Selection",
    "text": "15.4 Selection\n\nObesity paradox\nObesity DAG (cf. Fig. 1, Banack & Kaufman, 2014)\n\n\nCode\nlibrary(dagitty) # R version of http://www.dagitty.net\n\nobesity_dag &lt;- dagitty( \"dag {\n   Obesity -&gt; CVD -&gt; Mortality\n   Obesity -&gt; Mortality\n   CVD &lt;- U -&gt; Mortality\n}\")\n\ncoordinates(obesity_dag) &lt;- list(\n  x = c(Obesity = 1, CVD = 3, Mortality = 5, U = 4),\n  y = c(Obesity = 2.5, CVD = 2, Mortality = 2, U = 1))\n\nplot(obesity_dag)\n\n\n\n\n\n\n\n\n\n\nCVD = Cardiovascular disease, U = unmeasured factor(S)\n\nSimulation from Banack & Kaufman, 2014\n   NB! 615 should be 620\n\n\nMediation fallacy\n\nBaron & Kenny (1986) The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology, 51, 1173-1182.\nGoogle scholar 2021-12-02: 100,000+ citations!\n\n\n\nCode\nplot(0, pch = \"\", axes = FALSE, xlab = \"\", ylab = \"\",\n     xlim = c(0, 100), ylim = c(0, 100))\n\n# Upper fig\ntext(x = 40, y = 100, \"Total causal effect of X on Y = c\", col = \"blue\", cex = 0.8)\npoints(0, 85, pch = \"X\")\narrows(x0 = 3, x1 = 87, y0 = 85, y1 = 85, length = 0.1)\npoints(90, 85, pch = \"Y\")\npoints(40, 90, pch = \"c\")\n\n# Lower fig\npoints(0, 0, pch = \"X\")\narrows(x0 = 3, x1 = 87, y0 = 0, y1 = 0, length = 0.1)\npoints(90, 0, pch = \"Y\")\ntext(40, 6, \"c'\")\ntext(20, 20, \"a\")\ntext(60, 22, \"b\")\npoints(40, 30, pch = \"M\")\narrows(x0 = 3, x1 = 37, y0 = 0, y1 = 26, length = 0.1)\narrows(x0 = 43, x1 = 87, y0 = 26, y1 = 2, length = 0.1)\n\ntext(x = 40, y = 55, \"Direct causal effect of X on Y = c'\", \n     col = \"blue\", cex = 0.8) \ntext(x = 40, y = 48, \"Indirect causal effect of X on Y = ab\", \n     col = \"blue\", cex = 0.8) \ntext(x = 40, y = 40, \"ab + c = c'\", col = \"blue\", cex = 0.8) \n\n\n\n\n\n\n\n\n\nGeneral idea of Baron & Kenny (1986):\n\nEvaluate the bivariate associations between X and M (a), M and Y (b), and X and Y (c). If all are substantial, go to the next step:\nFit an adjusted model \\(y = b_0 + b_1 X + b_2M\\). If the association between X and Y holding M constant (c’, estimated with \\(b_1\\)) is reduced compared to the bivariate association between X and Y (c),then this is an argument for M being an mediator according to Baron & Kenny (1986).\n\n\nWhy mediation analysis is not simple:\nCollider bias is a threat, and that is why the standard approach of Baron & Kenny (1986) is invalid in many scenarios.\nSee Elwert & Winship (2014), Fig. 11.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "15-method25.html#practice",
    "href": "15-method25.html#practice",
    "title": "15  M: Fallacies and paradoxes",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n15E1. Revisit the find-an-alternative-explanation questions labeled Easy in Chapter 1, and answer them again, if applicable with reference to the Potential outcome model, Directed Cyclical Graphs, and data simulations to elaborate on your earlier answers.\n\n\n\n15E2. Simpson’s paradox comes in two versions, leading to the question of whether to look for the truth in the aggregated or disaggregated data. Explain.\n\n\n\n15E3. Here is more on Simpson’s paradox:\nA large data set on smoking and mortality included data from women born between 1910 and 1960. Surprisingly, the proportion of women that had died before year 2010 was lower among smokers than among non-smoker, suggesting a protective effect of smoking. However, when adjusting for age the opposite result was found.\n\nDraw a DAG explaining why it was correct to adjust for age.\n\nGive a reasonable explanation to why this pattern of data was observed.\n\n\n\n\n15E4. Explain the following terms in your own words with simple examples:\n\nConfounding bias\nResidual confounding\nNegative confounding\nRecall bias\nReversed causation\nSmall sample fallacy\n\n\n\n\n15E5.\n(a) Reverse causation can often be viewed as a form of confounding. Explain with an example.\n\nGiven an example where Recall bias may lead to a confusion of cause and effect.\n\nYou may use Directed Acyclic Graphs (DAGs) to support your explanation.\n\n\n\nMedium\n\n15M1. Revisit the find-an-alternative-explanation questions labeled Medium in Chapter 1, and answer them again, if applicable with reference to the Potential outcome model, Directed Cyclical Graphs, and data simulations to elaborate on your earlier answers.\n\n\n\n15M2. In this Directed Acyclical Graph, X is the exposure, Y is the outcome, Z1, Z2, and Z3 are observed covariates, and U1 and U2 are unmeasured (unknown) variables. Explain why it would not be possible to identify the total average causal effect of X on Y, i.e. why it would not be possible to estimate it from observable population data.\n\n\nlibrary(dagitty)\nmy_dag &lt;- dagitty( “dag { X -&gt; Y X -&gt; Z1 -&gt; Y Z1 -&gt; Z2 X &lt;- U2 -&gt; Z3 -&gt; Y Z3 &lt;- U1 -&gt; Y }”)\ncoordinates(my_dag) &lt;- list( x = c(X = 1, U2 = 1, Z2 = 2, Z1 = 3, Z3 = 3, U1 = 4, Y = 4), y = c(X = 5, U2 = 1, Z2 = 3, Z1 = 4, Z3 = 2, U1 = 1, Y = 5))\nplot(my_dag)\n\n\n\nHard\n\n15H1. Revisit the find-an-alternative-explanation questions labeled Hard in Chapter 1, and answer them again, if applicable with reference to the Potential outcome model, Directed Cyclical Graphs, and data simulations to elaborate on your earlier answers.\n\n\n\n15H2. Wilson & Rule (2015) studied facial trustworthiness and risk for being sentenced to death among convicts in Florida.\n\nThey describe an alternative explanation of their hypothesis that perceived trustworthiness may lower risk for capital punishment: “Perhaps individuals who look less trustworthy commit their crimes in a more heinous manner and are thus more culpable”. Clarify by drawing a DAG of their main hypothesis and one for the alternative explanation, and briefly explain how they dealt with the alternative explanation in their study.\nAnother concern is that maybe time on death row may make you look less trustworthy. Clarify by drawing a DAG, and briefly explain how they dealt with this alternative explanation in their study.\n\nReference:\nWilson, J. P., & Rule, N. O. (2015). Facial trustworthiness predicts extreme criminal-sentencing outcomes. Psychological Science, 26(8), 1325–1331. doi.org/10.1177/0956797615590992\nArticle found here",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "15-method25.html#session-info",
    "href": "15-method25.html#session-info",
    "title": "15  M: Fallacies and paradoxes",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.37     fastmap_1.2.0     xfun_0.52         knitr_1.50       \n [5] htmltools_0.5.8.1 rmarkdown_2.29    cli_3.6.5         compiler_4.4.2   \n [9] boot_1.3-31       rstudioapi_0.17.1 tools_4.4.2       curl_6.4.0       \n[13] evaluate_1.0.3    Rcpp_1.0.14       yaml_2.3.10       rlang_1.1.6      \n[17] jsonlite_2.0.0    V8_6.0.4          htmlwidgets_1.6.4 MASS_7.3-61      \n\n\n\n\n\n\nBanack, H. R., & Kaufman, J. S. (2014). The obesity paradox: Understanding the effect of obesity on mortality among individuals with cardiovascular disease. Preventive Medicine, 62, 96–102.\n\n\nBaron, R. M., & Kenny, D. A. (1986). The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology, 51(6), 1173.\n\n\nElwert, F., & Winship, C. (2014). Endogenous selection bias: The problem of conditioning on a collider variable. Annual Review of Sociology, 40, 31–53.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.\n\n\nNorton, H. J., & Divine, G. (2015). Simpson’s paradox… and how to avoid it. Significance, 12(4), 40–43.\n\n\nPearl, J. (2014). Comment: Understanding simpson’s paradox. The American Statistician, 68(1), 8–13.\n\n\nWilson, J. P., & Rule, N. O. (2015). Facial trustworthiness predicts extreme criminal-sentencing outcomes. Psychological Science, 26(8), 1325–1331.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>M: Fallacies and paradoxes</span>"
    ]
  },
  {
    "objectID": "16-stat25.html",
    "href": "16-stat25.html",
    "title": "16  S: Logistic Regression",
    "section": "",
    "text": "Topics",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "16-stat25.html#topics",
    "href": "16-stat25.html#topics",
    "title": "16  S: Logistic Regression",
    "section": "",
    "text": "Relative versus absolute risk\n\nRisk\nOdds\nRisk difference\nRelative risk (or risk ratio)\nOdds ratio\n\nLogistic regression.\n\nLogistic regression model\nInterpretation of coefficients\n\n\nReadings:\nGelman et al. (2021), Chapter 13 covers logistic regression.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "16-stat25.html#relative-and-absolute-risk",
    "href": "16-stat25.html#relative-and-absolute-risk",
    "title": "16  S: Logistic Regression",
    "section": "16.1 Relative and absolute risk",
    "text": "16.1 Relative and absolute risk\nBefore we go into the details of logistic regression, we need to clarify the meaning of common measures of binary outcomes and contrasts, specifically:\n\nRisk (or prevalence, or proportion, or probability),\nOdds,\nRisk difference,\nRelative risk (or risk ratio), and\nOdds ratio.\n\n\nRelationship probability (“risk”) and odds:\n\n\\(odds = \\frac{p}{1 - p}\\)\n\n\\(p = \\frac{odds}{1 + odds}\\)\n\n\n\n\n\nGroup\nDisease\nNo disease\nrow sum\n\n\n\n\n1\na\nb\na+b\n\n\n0\nc\nd\nc+d\n\n\n\n\n\nRisk\n\n\\(Risk_{group 1} = a/(a+b)\\)\n\\(Risk_{group 0} = c/(c+d)\\)\n\nOdds\n\n\\(Odds_{group 1} = Risk_{group 1} / (1 - Risk_{group 1}) = a/b\\)\n\\(Odds_{group 0} = Risk_{group 0} / (1 - Risk_{group 0}) = c/d\\)\n\nRisk difference (RD)\n\n\\(RD = Risk_{group 1} - Risk_{group 0} = a/(a+b) - c/(c+d)\\), or\n\\(RD = Risk_{group 0} - Risk_{group 1} = c/(c+d) - a/(a+b)\\)\n\nRelative risk (RR)\n\n\\(RR = Risk_{group 1} / Risk_{group 0} = \\frac{a/(a+b)}{c/(c+d)}\\), or\n\\(RR = Risk_{group 0} / Risk_{group 1} = \\frac{c/(c+d)}{a/(a+b)}\\)\n\nOdds ratio (OR)\n\n\\(OR = Odds_{group 1} / Odds_{group 0} = \\frac{a/b}{c/d} = \\frac{ad}{bc}\\), or\n\\(OR = Odds_{group 0} / Odds_{group 1} = \\frac{c/d}{a/b} = \\frac{bc}{ad}\\)\n\n\nNote: With low prevalence of disease (low risk), \\(a\\) is much smaller than \\(b\\), and \\(c\\) is much smaller than \\(d\\), and OR is similar to RR.\n\\(RR = \\frac{a/(a+b)}{c/(c+d)} \\approx \\frac{a/b}{c/d} = OR\\), if \\(a \\ll b, c \\ll d\\)\nFor large \\(a\\) and \\(c\\), \\(OR &gt; RR\\)\n\n\nRandomized controlled trial: Effect of aspirin on the incidence of heart attacks.\n\n\n\nGroup\nHeart attack\nNo heart attack\n\n\n\n\n\nAspirin\n104\n10,933\n11,037\n\n\nPlacebo\n189\n10,845\n11,034\n\n\n\n293\n21,778\n22,071\n\n\n\n\n\nRisk\n\n\\(Risk_{aspirin} = 104/11037\\) = 0.0094\n\\(Risk_{placebo} = 189/11034\\) = 0.0171\n\nOdds\n\n\\(Odds_{aspirin} = 104/10933\\) = 0.0095\n\\(Odds_{placebo} = 189/10845\\) = 0.0174\n\nRisk difference (RD)\n\n\\(RD = Risk_{aspirin} - Risk_{placebo}\\) = -0.0077\n\\(RD = Risk_{placebo} - Risk_{aspirin}\\) = 0.0077\n\nRelative risk (RR)\n\n\\(RR = Risk_{aspirin} / Risk_{placebo}\\) = 0.5501\n\\(RR = Risk_{placebo} / Risk_{aspirin}\\) = 1.8178\n\nOdds ratio (OR)\n\n\\(OR = Odds_{aspirin} / Odds_{placebo}\\) = 0.5458\n\\(OR = Odds_{placebo} / Odds_{aspirin}\\) = 1.8321",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "16-stat25.html#logistic-regression",
    "href": "16-stat25.html#logistic-regression",
    "title": "16  S: Logistic Regression",
    "section": "16.2 Logistic regression",
    "text": "16.2 Logistic regression\nLogistic regression is used for binary outcome variables, typically coded 0 or 1, such as alive/dead, healthy/sick, successful/unsuccessful, correct/incorrect, etc.\nRemember the general form of of the linear model:\n\\(y_i \\sim Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_1x_i\\)\nLogistic regression can be defined similarly:\n\\(y_i \\sim Bernoulli(p_i)\\)\n\\(p_i = logit^{-1}(b_0 + b_1x_i)\\)\nOr equivalent (and more common):\n\\(y_i \\sim Bernoulli(p_i)\\)\n\\(logit(p_i) = b_0 + b_1x_i\\)\nNote that \\(Bernoulli(p)\\) is a special cases of the binomial distribution \\(Binomial(n, p)\\) with \\(n = 1\\).\n\nThe estimated parameter \\(p_i\\) is the probability (“risk”) of y = 1 given specific value(s) of the predictor(s) x. The probability \\(p_i\\) is also the mean, as the mean of an indicator variable y is the probability of y = 1. Thus, both the linear and the logistic model predicts the mean value of the outcome variable given a specific value of the predictor. A difference is that the estimated parameter \\(\\mu_i\\) in the linear model is also a possible value of a single observation, whereas the estimated parameter \\(p_i\\) in logistic regression is a value that is never seen in a single observation, as these only can take on values of 0 or 1.\n\nThe logistic model involve a linear model. In theory, a line has no boundaries and can go from \\(-\\infty\\) to \\(\\infty\\), whereas a probability (or risk or proportion or prevalence) can only take values between 0 and 1. The way logistic regression deals with this is to make a linear model not of the probability, but of the unbounded log-odds:\n\n\\(p\\)    \\([0, 1]\\)\n\\(odds = \\frac{p}{1 - p}\\)    \\([0, \\infty)\\)\n\\(log(odds) = log(\\frac{p}{1 - p})\\)    \\((-\\infty, \\infty)\\)\n\nThe function that transforms \\(p\\) to log-odds is called\n\n\\(logit(p) = log(\\frac{p}{1 - p})\\)\n\nThe linear model is nested in the inverse logit, \\(logit^{-1}\\), that restricts the output of \\(p\\) to the interval [0, 1]:\n\n\\(p_i = logit^{-1}(b_0 + b_1x_i)\\)\n\nAs seen in the left-hand figure below, when the output of the linear model (\\(b_0 + b_1x_i\\)) goes below -4 or above +4, the predicted probability (the output of the inverse-logit) is close to zero and one, respectively. When the linear model is zero, the probability is 0.5.\n\ninverse-logit() and logit():\n\n\\(p = logit^{-1}(b_0 + b_1x_i) = \\frac{e^{b_0 + b_1x_i}}{1 + e^{b_0 + b_1x_i}}\\), or in R: plogis().\n\\(logit(p_i) = log(\\frac{p_1}{1-pi}) = b_0 + b_ix_i\\), or in R: qlogis()\n\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Plot of logistic() = inv_logit()\nx &lt;- seq(-5, 5, by = 0.1)\np &lt;- plogis(x)\nplot(x, p, type = \"l\", xlab = \"Linear model: b0 + b1x\", \n     ylab = \" p = inverse-logit(b0 + b1x)\")\nabline(0,0, col = \"grey\")\nabline(1,0, col = \"grey\")\nmtext(side = 3, text = \"inverse-logit()\", cex = 0.9)\n\n# Plot of logit()\nq &lt;- qlogis(p)\nplot(p, x, type = \"l\", xlab = \"p\", \n     ylab = \"logit(p) = b0 + b1x\")\nmtext(side = 3, text = \"logit()\", cex = 0.9)\n\n\n\n\n\n\n\n\n\n\nHere is an example from Gelman et al. (2021) (fig 13.1b. p, 218) for a simple model with one predictor:\n\\(y_i \\sim Bernoulli(p_i)\\)\n\\(p_i = logit^{-1}(-1.4 + 0.33x_i)\\)\n\n\nCode\nx &lt;- seq(-20, 20, by = 0.1)\nb0 &lt;- -1.4\nb1 &lt;- 0.33\npar(mfrow = c(1, 2))\nplot(x, b0 + b1*x, type = 'l', ylab = \"log-odds = -1.4 + 0.33x\")\nmtext(side = 3, text = \"Linear model\", cex = 0.9)\ny &lt;- (exp(b0 + b1*x) / (1 + exp(b0 + b1*x)))\nplot(x, y, ylab = \"p = inv-logit(-1.4 + 0.33x)\", type = 'l', xlab = \"x\")\nabline(0,0, col = \"grey\")\nabline(1,0, col = \"grey\")\nmtext(side = 3, text = \"inv-logit(Linear model)\", cex = 0.9)\n\n\n\n\n\n\n\n\n\n\nIn their example, the function predicted the probability of supporting George Bush in the 1992 presidential election, as a function of income level from 1 to 5 (see their figure 13.2). But here I will assume another story, to justify looking at the function in its full range. So, let’s assume that y is a binary outcome variable indicating success (1) or failure (0) in solving a not too-easy mathematical problem, and x is a mean-centered variable representing the amount of mathematical training of the problem solver (-20 is the amount of training of a 7-year old school kid, +20 of a Ph.D. in mathematics, and 0 is the mean amount of training in the population).\nLet’s simulate data, using rbinom(), plot data and fit data to a logistic model using glm() (please try rstanarm::stan_glm()). ``\n\n\nCode\n# Simulate data\nset.seed(123)\nx &lt;- rep(seq(-20, 20), 100)  # Exposure\npp &lt;- plogis(-1.4 + 0.33*x)  # Probability of outcome = 1\ny &lt;- rbinom(length(x), 1, prob = pp) # Data: 0 or 1\n\n# Plot, with transparent colors\njitter &lt;- rnorm(length(x), 0, 0.02)\nplot(x + jitter, y, xlim = c(-20, 20), ylim = c(0, 1), pch = 20, \n     col = rgb(0, 0, 1, 0.02), cex = 2, xlab = \"x\", ylab = \"P(y = 1)\")\n\n# Model using glm()\nm &lt;- glm(y ~ x, family = binomial(link = \"logit\"))\nxline &lt;- seq(-20, 20, by = 0.1)\nyline &lt;- plogis(m$coefficients[1] + m$coefficients[2]*xline)\nlines(xline, yline, col = \"darkred\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nround(coef(m), 2)\n\n\n(Intercept)           x \n      -1.40        0.33 \n\n\nCode\nround(confint(m), 2)\n\n\n            2.5 % 97.5 %\n(Intercept) -1.54  -1.26\nx            0.31   0.35\n\n\n\nInterpretations of intercept, \\(b_0 = -1.4\\)\n\nThis is the output (log-odds) of the linear model when x = 0, in this case for individuals with average mathematical training. Converting this to probability yields: \\(\\frac{exp(-1.4)}{1 + exp(-1.4)} \\approx 0.2\\). That is, according to the model, about 20 % of people with average amount of mathematical training would solve the mathematical problem.\n\nInterpretations of slope, \\(b_1 = 0.33\\)\n\n\\(exp(b_1)\\) is the odds ratio comparing groups defined by x and x+1. In the example above, \\(b_1 = 0.33\\), so \\(exp(b_1) = 1.39 = \\frac{Odds(y = 1|x+1)}{Odds(y = 1|x)}\\) .\nInterpretation of \\(b_1\\) in terms of relative risk (RR) or risk difference (RD) is more complicated. Unlike the odds ratio (OR), which is the same for any comparison of x with x+1, the RD and RR depend on the value of x for which the comparison is done, see illustration in figure below.\nRule-of-thumb for the risk difference: \\(b_1/4\\) is the upper limit for difference in risk, \\(p\\), between groups defined by x and x+1 (Gelman et al’s “Divide-by-4 rule”, p. 220). In the example above, \\(b_1 = 0.33\\) and \\(b_1/4 = 0.08\\), so a difference in one unit on x corresponds to a difference of at most 0.08 in \\(p\\) (i.e., risk difference of no more than 8 % for a one-unit difference in x).\n\n\n\n\nCode\nb0 &lt;- -1.4\nb1 &lt;- 0.33\nx0 &lt;- seq(-20, 20, by = 0.1)\ny0 &lt;- (exp(b0 + b1*x0) / (1 + exp(b0 + b1*x0)))\nx1 &lt;- x0 + 1\ny1 &lt;- (exp(b0 + b1*x1) / (1 + exp(b0 + b1*x1)))\n\nrd &lt;- y1 - y0\nrr &lt;- y1/y0\nor &lt;- (y1/(1-y1)) / (y0/(1-y0))\n\npar(mfrow = c(1, 2))\n# Data and logistic function\njitter &lt;- rnorm(length(x), 0, 0.02)\nplot(x + jitter, y, xlim = c(-20, 20), ylim = c(0, 1), pch = 20, \n     col = rgb(0, 0, 1, 0.02), cex = 1, xlab = \"x\", ylab = \"Pr(y = 1|x)\")\nlines(x0, y0, col = \"darkred\")\n\n# Effect measures: RD, RR, OR\nplot(c(-20, 20), c(0, 2), pch = \"\", xlab = \"x\", \n     ylab = \"Effect size per unit change in x\")\nlines(x0, rd, col = \"blue\", lty = 2)\nlines(x0, rr, col = \"red\", lty = 3)\nlines(x0, or, col = \"darkgreen\", lty = 4)\n\n# Add text to fig\ntext(x = -20, y = 0.1, labels = \"Risk difference\", \n     pos = 4, cex = 0.7, col = \"blue\")\ntext(x = 8, y = 0.95, labels = \"Relative risk\", \n     pos = 4, cex = 0.7, col = \"red\")\ntext(x = 8, y = 1.45, labels = \"Odds ratio\", \n     pos = 4, cex = 0.7, col = \"darkgreen\")\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression with just an intercept\nAs we noted earlier for linear regression (Notes 10): Estimating the mean is the same as regressing on a constant.\n\\(y_i \\sim Normal(\\mu, \\sigma)\\)\n\\(\\mu = b_0\\),\nwhere \\(b_0\\) is the estimate of the population mean.\nThe same holds for logistic regression, where the mean is the probability of y = 1.\n\\(y_i \\sim Bernoulli(p)\\)\n\\(p = logit^{-1}(b_0)\\),\nwhere \\(b_0\\) is the log-odds, and \\(p = logit^{-1}(b_0) = \\frac{exp(b_0)}{1 + exp(b_0)}\\)\nIn the Aspirin experiment, the overall (marginal) proportion of heart attacks was 293 in 22,070 (so the sample proportion equals 0.013). We could use logistic regression to estimate the population proportion with a compatibility interval.\n\n\nCode\n# Data from aspirin experiment\nmi &lt;- c(rep(1, 104), rep(0, 10933), rep(1, 189), rep(0, 10845))\naspirin &lt;- c(rep(1, 104), rep(1, 10933), rep(0, 189), rep(0, 10845))\na &lt;- data.frame(aspirin, mi)\n\n# Fit intercpt-only model, display coefficients (log-odds)  \nasp0 &lt;- glm(mi ~ 1, family = binomial(link =\"logit\"), data = a)\ncoef(asp0)\n\n\n(Intercept) \n  -4.308483 \n\n\nCode\nconfint(asp0)\n\n\nWaiting for profiling to be done...\n\n\n    2.5 %    97.5 % \n-4.425954 -4.195331 \n\n\nCode\n# Display as probabilities, using the inverse-logit, in R called plogis()\nplogis(coef(asp0))\n\n\n(Intercept) \n 0.01327534 \n\n\nCode\nplogis(confint(asp0))\n\n\nWaiting for profiling to be done...\n\n\n     2.5 %     97.5 % \n0.01182137 0.01484215 \n\n\n\n\n\nLogistic regression with a single binary predictor\nAnd as we noted earlier for linear regression (Notes 10): Estimating a difference between two means is the same as regressing on an indicator variable:\n\\(y_i ~ Normal(\\mu_i, \\sigma)\\)\n\\(\\mu_i = b_0 + b_iD_i\\),\nwhere \\(D_i\\) is an indicator variable coded 0 for one group and 1 for the other. Thus, \\(b_0\\) is the estimate of the population mean for group 0, and \\(b_1\\) is an estimate of the difference between population group means.\nSimilarly for logistic regression:\n\\(y_i ~ Bernouilli(p_i)\\)\n\\(p_i = logit^{-1}(b_0 + b_iD_i)\\).\nHere \\(b_0\\) is the log-odds of group 0, and \\(b_i\\) is the difference in log-odds between the two groups. Note that \\(exp(b_1)\\) is the odds ratio of group 1 versus group 0. We may also convert to probabilities:\n\nProbability y = 1 for group 0: \\(Pr(y = 1|D = 0) = logit^{-1}(b_0) = \\frac{exp(b_0)}{1 + exp(b_0)}\\)\nProbability y = 1 for group 1: \\(Pr(y = 1|D = 1) = logit^{-1}(b_0 + b_1) = \\frac{exp(b_0 + b_1)}{1 + exp(b_0 + b_1)}\\)\n\nHere I create a data frame with as many rows as participants in the Aspirin experiment (see Table above), and then I use logistic regression to fit the model (MI for myocardial infarction, coded 0 or 1)\n\\(MI_i \\sim Bernouilli(p_i)\\)\n\\(p_i = logit^{-1}(b_0 + b_1Aspirin_i)\\)\n\n\nCode\n# Data from aspirin experiment\nmi &lt;- c(rep(1, 104), rep(0, 10933), rep(1, 189), rep(0, 10845))\naspirin &lt;- c(rep(1, 104), rep(1, 10933), rep(0, 189), rep(0, 10845))\na &lt;- data.frame(aspirin, mi)\ntable(aspirin = a$aspirin, mi = a$mi)\n\n\n       mi\naspirin     0     1\n      0 10845   189\n      1 10933   104\n\n\nCode\n# Logistic regression\naspfit &lt;- glm(mi ~ aspirin, family = binomial(link =\"logit\"), data = a)\nsummary(aspfit)\n\n\n\nCall:\nglm(formula = mi ~ aspirin, family = binomial(link = \"logit\"), \n    data = a)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.04971    0.07337 -55.195  &lt; 2e-16 ***\naspirin     -0.60544    0.12284  -4.929 8.28e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3114.7  on 22070  degrees of freedom\nResidual deviance: 3089.3  on 22069  degrees of freedom\nAIC: 3093.3\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nConverting \\(b_0\\) to risk for placebo group, \\(b_1\\) to odds ratio: aspirin/placebo, and \\(b_0 + b_1\\) to risk of aspirin group.\n\n\nCode\n# Coefficients\nb0 &lt;- aspfit$coefficients[1]\nb1 &lt;- aspfit$coefficients[2]\n\n# Transform using plogis() and exp()\nasp_estimates &lt;-        c(plogis(b0),      plogis(b0+b1),    exp(b1))\nnames(asp_estimates) &lt;- c(\"risk-placebo\", \"risk-aspirin\", \"odds ratio\")\nround(asp_estimates, 4)\n\n\nrisk-placebo risk-aspirin   odds ratio \n      0.0171       0.0094       0.5458",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "16-stat25.html#arsenic-data",
    "href": "16-stat25.html#arsenic-data",
    "title": "16  S: Logistic Regression",
    "section": "16.3 Arsenic data",
    "text": "16.3 Arsenic data\nHere example from Gelman et al. (2021) from arsenic level in wells in Bangladesh and users decisions to switch to less polluted wells, for details, see p. 234-237.\nFollow this link to find data Save (Ctrl+S on a PC) to download as text file\n Code book:\nMeasurements of wells and households, and follow registration of well-switching behavior at follow up a few years later.\n\nswitch Whether the household had switched wells at follow up.\narsenic Arsenic level in units of hundreds of microgram per liter. Levels below 0.5 are considered “safe”.\ndist Distance in meters to closets safe well.\ndist100 dist/100, so unit is 100-m.\nassoc Whether any members of the household are active in community organizations.\neduc The education level ff the household (years of education).\neduc4 educ/4, so units i 4y-education.\n\n\nImport data\n\n\nCode\n# Import data\nwells &lt;- read.table(\"datasets/wells.txt\", header = TRUE, sep = \",\")\nstr(wells)\n\n\n'data.frame':   3020 obs. of  7 variables:\n $ switch : int  1 1 0 1 1 1 1 1 1 1 ...\n $ arsenic: num  2.36 0.71 2.07 1.15 1.1 3.9 2.97 3.24 3.28 2.52 ...\n $ dist   : num  16.8 47.3 21 21.5 40.9 ...\n $ dist100: num  0.168 0.473 0.21 0.215 0.409 ...\n $ assoc  : int  0 0 0 0 1 1 1 0 1 1 ...\n $ educ   : int  0 0 10 12 14 9 4 10 0 0 ...\n $ educ4  : num  0 0 2.5 3 3.5 2.25 1 2.5 0 0 ...\n\n\n\nSummary\n\n\nCode\nsummary(wells, digits = 2)\n\n\n     switch        arsenic          dist           dist100           assoc     \n Min.   :0.00   Min.   :0.51   Min.   :  0.39   Min.   :0.0039   Min.   :0.00  \n 1st Qu.:0.00   1st Qu.:0.82   1st Qu.: 21.12   1st Qu.:0.2112   1st Qu.:0.00  \n Median :1.00   Median :1.30   Median : 36.76   Median :0.3676   Median :0.00  \n Mean   :0.58   Mean   :1.66   Mean   : 48.33   Mean   :0.4833   Mean   :0.42  \n 3rd Qu.:1.00   3rd Qu.:2.20   3rd Qu.: 64.04   3rd Qu.:0.6404   3rd Qu.:1.00  \n Max.   :1.00   Max.   :9.65   Max.   :339.53   Max.   :3.3953   Max.   :1.00  \n      educ          educ4    \n Min.   : 0.0   Min.   :0.0  \n 1st Qu.: 0.0   1st Qu.:0.0  \n Median : 5.0   Median :1.2  \n Mean   : 4.8   Mean   :1.2  \n 3rd Qu.: 8.0   3rd Qu.:2.0  \n Max.   :17.0   Max.   :4.2  \n\n\n\nModel with only distance as predictor\n\n\nCode\nfit_1 &lt;- stan_glm(switch ~ dist100, family = binomial(link = \"logit\"), \n                  data = wells, refresh = 0)\nprint(fit_1, digits = 1)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      switch ~ dist100\n observations: 3020\n predictors:   2\n------\n            Median MAD_SD\n(Intercept)  0.6    0.1  \ndist100     -0.6    0.1  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nVisualize data and model prediction.\n\n\nCode\n# function to jitter data points\njitter_binary &lt;- function(a, jitt = 0.05){\n  ifelse(a == 0, runif(length(a), 0, jitt), runif(length(a), 1- jitt, 1))\n}\n\nwells$switch_jitter &lt;- jitter_binary(wells$switch)\n\n# Plot jittered data\nplot(wells$dist100, wells$switch_jitter, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Distance (in 100-m) to nearest safe well\",\n     ylab = \"Pr(switching | distance)\")\ncf &lt;- coef(fit_1)\nx &lt;- seq(from = 0, to = 4, by = 0.1)\nypred &lt;- plogis(cf[1] + cf[2]*x)\nlines(x, ypred)\n\n\n\n\n\n\n\n\n\n\nModel with distance and arsenic level as predictor\nThis is Gelman Fig. 13.10a\n\n\nCode\n# Fit model (Gelman et al. calls it fit_3)\nfit_3 &lt;- stan_glm(switch ~ dist100 + arsenic, family = binomial(link = \"logit\"), \n                  data = wells, refresh = 0)\nprint(fit_3, digits = 2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      switch ~ dist100 + arsenic\n observations: 3020\n predictors:   3\n------\n            Median MAD_SD\n(Intercept)  0.00   0.08 \ndist100     -0.90   0.10 \narsenic      0.46   0.04 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nCode\n# Plot data, see Fig. 13.10 (left) in Gelman et al.\nplot(wells$dist100, wells$switch_jitter, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Distance (in 100-m) to nearest safe well\",\n     ylab = \"Pr(switching | distance)\")\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_3)\nx &lt;- seq(from = 0, to = 4, by = 0.1)\narsenic &lt;- c(0.5, 1)\nxx &lt;- expand.grid(dist100 = x, arsenic = arsenic)\nxx$ypred &lt;- plogis(cf[1] + cf[2]*xx$dist100 + cf[3]*xx$arsenic)\nhead(xx)\n\n\n  dist100 arsenic     ypred\n1     0.0     0.5 0.5582066\n2     0.1     0.5 0.5359890\n3     0.2     0.5 0.5136276\n4     0.3     0.5 0.4912115\n5     0.4     0.5 0.4688307\n6     0.5     0.5 0.4465747\n\n\nCode\n# Add lines for model prediction with two levels of arsenic (low = 0.5, high = 1)\nlines(xx$dist100[xx$arsenic == 0.5], xx$ypred[xx$arsenic == 0.5], lty = 2)\nlines(xx$dist100[xx$arsenic == 1], xx$ypred[xx$arsenic == 1], lty = 1)\n\n# Add text to plot\ntext(x = 0.5, y = 0.53, labels = \"If As = 1\", pos = 4, cex = 0.8)\ntext(x = 0.3, y = 0.37, labels = \"If As = 0.5\", pos = 4, cex = 0.8)\n\n\n\n\n\n\n\n\n\n\nHere with lines for arsenic-level corresponding to the 5, 50, and 95th percentile, respectively. I show it on the log-odds scale (left) and on the the probability scale (as Gelman et al).\n\n\nCode\npar(mfrow = c(1, 2))\n\n## Plot, with outcome on log-odds scale ----\nplot(NULL, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Distance (in 100-m) to nearest safe well\",\n     ylab = \"log-odds (Pr(switching | distance))\", \n     xlim = c(0, 4), ylim = c(-6, 6))\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_3)\nx &lt;- seq(from = 0, to = 4, by = 0.1)\narsenic &lt;- quantile(wells$arsenic, c(0.05, 0.5, 0.95))\nxx &lt;- expand.grid(dist100 = x, arsenic = arsenic)\nxx$ypred &lt;-cf[1] + cf[2]*xx$dist100 + cf[3]*xx$arsenic\nhead(xx)\n\n\n  dist100 arsenic        ypred\n1     0.0    0.56  0.261520296\n2     0.1    0.56  0.171838913\n3     0.2    0.56  0.082157530\n4     0.3    0.56 -0.007523853\n5     0.4    0.56 -0.097205236\n6     0.5    0.56 -0.186886619\n\n\nCode\n# Add lines for model prediction with three levels of arsenic (5, 50, 95th)\nlines(xx$dist100[xx$arsenic == arsenic[1]], xx$ypred[xx$arsenic == arsenic[1]], lty = 1)\nlines(xx$dist100[xx$arsenic == arsenic[2]], xx$ypred[xx$arsenic == arsenic[2]], lty = 2)\nlines(xx$dist100[xx$arsenic == arsenic[3]], xx$ypred[xx$arsenic == arsenic[3]], lty = 3)\n\n# Add text to plot\ntext(x = 0.5, y = -1.7, labels = paste(\"As:\", round(arsenic[1], 2)), pos = 4, cex = 0.8)\ntext(x = 0.5, y = 0.35, labels = paste(\"As:\", round(arsenic[2], 2)), pos = 4, cex = 0.8)\ntext(x = 0.3, y = 1.8, labels = paste(\"As:\", round(arsenic[3], 2)), pos = 4, cex = 0.8)\n\n\n## Plot data, probability scale see Fig. 13.10 (left) in Gelman et al. ----\nplot(wells$dist100, wells$switch_jitter, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Distance (in 100-m) to nearest safe well\",\n     ylab = \"Pr(switching | distance)\")\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_3)\nx &lt;- seq(from = 0, to = 4, by = 0.1)\narsenic &lt;- quantile(wells$arsenic, c(0.05, 0.5, 0.95))\nxx &lt;- expand.grid(dist100 = x, arsenic = arsenic)\nxx$ypred &lt;- plogis(cf[1] + cf[2]*xx$dist100 + cf[3]*xx$arsenic)\nhead(xx)\n\n\n  dist100 arsenic     ypred\n1     0.0    0.56 0.5650100\n2     0.1    0.56 0.5428543\n3     0.2    0.56 0.5205278\n4     0.3    0.56 0.4981190\n5     0.4    0.56 0.4757178\n6     0.5    0.56 0.4534139\n\n\nCode\n# Add lines for model prediction with three levels of arsenic (5, 50, 95th)\nlines(xx$dist100[xx$arsenic == arsenic[1]], xx$ypred[xx$arsenic == arsenic[1]], lty = 1)\nlines(xx$dist100[xx$arsenic == arsenic[2]], xx$ypred[xx$arsenic == arsenic[2]], lty = 2)\nlines(xx$dist100[xx$arsenic == arsenic[3]], xx$ypred[xx$arsenic == arsenic[3]], lty = 3)\n\n# Add text to plot\ntext(x = 0.5, y = 0.25, labels = paste(\"As:\", round(arsenic[1], 2)), pos = 4, cex = 0.8)\ntext(x = 0.5, y = 0.55, labels = paste(\"As:\", round(arsenic[2], 2)), pos = 4, cex = 0.8)\ntext(x = 0.3, y = 0.85, labels = paste(\"As:\", round(arsenic[3], 2)), pos = 4, cex = 0.8)\n\n\n\n\n\n\n\n\n\nCode\n# print arsenic levels displayed\nround(c(As = arsenic), 2)\n\n\n As.5% As.50% As.95% \n  0.56   1.30   3.79 \n\n\n\nHere with lines for distances corresponding to the 5, 50, and 95th percentile, respectively (cf. Gelman et al., Fig 13.10b).\n\n\nCode\n# Probability scale, see Fig. 13.10 (left) in Gelman et al.\n# Plot data, \nplot(wells$arsenic, wells$switch_jitter, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Arsenic concentration in well water\",\n     ylab = \"Pr(switching | distance)\", xlim = c(0, 8))\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_3)\nx &lt;- seq(from = 0.5, to = 8, by = 0.1)  # arsenic levels from 0 to 6\ndist100 &lt;- quantile(wells$dist100, c(0.05, 0.5, 0.95))\nxx &lt;- expand.grid(arsenic = x, dist100 = dist100)\nxx$ypred &lt;- plogis(cf[1] + cf[2]*xx$dist100 + cf[3]*xx$arsenic)\nhead(xx)\n\n\n  arsenic   dist100     ypred\n1     0.5 0.0798645 0.5404770\n2     0.6 0.0798645 0.5518923\n3     0.7 0.0798645 0.5632531\n4     0.8 0.0798645 0.5745479\n5     0.9 0.0798645 0.5857655\n6     1.0 0.0798645 0.5968948\n\n\nCode\n# Add lines for model prediction with three levels of dist100 (5, 50, 95th)\nlines(xx$arsenic[xx$dist100 == dist100[1]], xx$ypred[xx$dist100 == dist100[1]], lty = 1)\nlines(xx$arsenic[xx$dist100 == dist100[2]], xx$ypred[xx$dist100 == dist100[2]], lty = 2)\nlines(xx$arsenic[xx$dist100 == dist100[3]], xx$ypred[xx$dist100 == dist100[3]], lty = 3)\n\n# Add text to plot\ntext(x = 0.5, y = 0.28, labels = paste(\"Dist:\", round(100*arsenic[1]), \"m\"), \n     pos = 4, cex = 0.8)\ntext(x = 0.5, y = 0.47, labels = paste(\"Dist:\", round(100*arsenic[2]), \"m\"), \n     pos = 4, cex = 0.8)\ntext(x = 0.3, y = 0.65, labels = paste(\"Dist:\", round(100*arsenic[3]), \"m\"), \n     pos = 4, cex = 0.79)\n\n\n\n\n\n\n\n\n\n\nAdd interaction term to the model. Below with no centering.\n\n\nCode\n# Fit model (Gelman et al. calls it fit_4)\nfit_4 &lt;- stan_glm(switch ~ dist100 + arsenic + dist100:arsenic, \n                 family = binomial(link = \"logit\"), \n                  data = wells, refresh = 0)\nprint(fit_4, digits = 2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      switch ~ dist100 + arsenic + dist100:arsenic\n observations: 3020\n predictors:   4\n------\n                Median MAD_SD\n(Intercept)     -0.15   0.12 \ndist100         -0.57   0.22 \narsenic          0.56   0.07 \ndist100:arsenic -0.18   0.11 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nTo help interpretation of coefficients, here the same model with centered predictors\n\n\nCode\n# Centering\nwells$c_dist100 &lt;- wells$dist100 - mean(wells$dist100)\nwells$c_arsenic &lt;- wells$arsenic - mean(wells$arsenic)\n\n# Model (Gelman et al. calls it fit_5, p. 244)\nfit_5 &lt;- stan_glm(switch ~ c_dist100 + c_arsenic + c_dist100:c_arsenic, \n                 family = binomial(link = \"logit\"), \n                  data = wells, refresh = 0)\nprint(fit_5, digits = 2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      switch ~ c_dist100 + c_arsenic + c_dist100:c_arsenic\n observations: 3020\n predictors:   4\n------\n                    Median MAD_SD\n(Intercept)          0.35   0.04 \nc_dist100           -0.88   0.11 \nc_arsenic            0.47   0.04 \nc_dist100:c_arsenic -0.18   0.10 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nCode\npred5 &lt;- fitted(fit_5)  # Fitted value\nres5 &lt;- wells$switch - pred5  # Residual\n\n\n\nInterpretation of the coefficients:\n\nIntercept   The estimated log-odds of switching if distance and arsenic is at their means (c_dist100 = 0 and c_arsenic = 0). Use \\(logit^{-1}(Intercept)\\) to express in probability units, in R you may use plogis(). In the model above, the intercept is 0.35, so the models estimate of the probability of switching if distance and arsenic is at their mean is plogis(0.35) or about 0.59.\nc_dist100   Coefficient for distance on the logit scale (i.e., log-odds) if arsenic level is at its mean. We can use the divide-by-4 rule to see that one unit increase in dist100 would correspond to at most a difference of -0.22 on the probability scale (if interpreted causally, a reduction in probability of switching)\nc_arsenic   Coefficient for arsenic on the logit scale (i.e., log-odds) if dist100 at its mean. We can use the divide-by-4 rule to see that one unit increase in c_arsenic would correspond to at most a difference of +0.12 on the probability scale (if interpreted causally, a reduction in probability of switching)\nc_dist100:c_arsenic.   Can be seen in two ways:\n\nFor each additional unit of arsenic, the value -0.18 is added to the coefficient for distance. The coefficient is negative (-0.88), larger distance means lower probability of switching. This negative relationship will be stronger for each additional unit of arsenic.\nFor each additional unit of distance, the value -0.18 is added to the coefficient for arsenic. The coefficient is positive (0.47), higher levels means higher probability of switching. This positive relationship will be weaker for each additional unit of distance.\n\n\n\nPlot the model (only way to really understand the interaction), left on the log-odds scale, right on the probability scale.\n\n\nCode\npar(mfrow = c(1, 2))\n\n## Plot, with outcome on log-odds scale ----\nplot(NULL, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Mean-centered dist100\",\n     ylab = \"log-odds (Pr(switching | distance))\", \n     xlim = c(-1, 3.5), ylim = c(-6, 6))\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_5)\nx &lt;- seq(from = -0.5, to = 3, by = 0.1)\narsenic &lt;- quantile(wells$c_arsenic, c(0.05, 0.5, 0.95))\nxx &lt;- expand.grid(c_dist100 = x, c_arsenic = arsenic)\nxx$dxa &lt;- xx$c_dist100*xx$c_arsenic\nxx$ypred &lt;- cf[1] + cf[2]*xx$c_dist100 + cf[3]*xx$c_arsenic + cf[4]*xx$dxa\nhead(xx)\n\n\n  c_dist100 c_arsenic       dxa       ypred\n1      -0.5  -1.09693 0.5484652  0.17707787\n2      -0.4  -1.09693 0.4387722  0.10901440\n3      -0.3  -1.09693 0.3290791  0.04095093\n4      -0.2  -1.09693 0.2193861 -0.02711253\n5      -0.1  -1.09693 0.1096930 -0.09517600\n6       0.0  -1.09693 0.0000000 -0.16323946\n\n\nCode\n# Add lines for model prediction with three levels of arsenic (5, 50, 95th)\nlines(xx$c_dist100[xx$c_arsenic == arsenic[1]], xx$ypred[xx$c_arsenic == arsenic[1]], lty = 1)\nlines(xx$c_dist100[xx$c_arsenic == arsenic[2]], xx$ypred[xx$c_arsenic == arsenic[2]], lty = 2)\nlines(xx$c_dist100[xx$c_arsenic == arsenic[3]], xx$ypred[xx$c_arsenic == arsenic[3]], lty = 3)\n\n# Add text to plot\n#text(x = -1, y = 0, labels = paste(\"As:\", round(arsenic[1], 2)), pos = 4, cex = 0.8)\n#text(x = -1, y = 0.7, labels = paste(\"As:\", round(arsenic[2], 2)), pos = 4, cex = 0.8)\n#text(x = -1, y = 2.2, labels = paste(\"As:\", round(arsenic[3], 2)), pos = 4, cex = 0.8)\n\n\n## Plot data, probability scale see Fig. 13.10 (left) in Gelman et al. ----\nplot(NULL, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Mean-centered dist100\",\n     ylab = \"Pr(switching | distance)\", \n     xlim = c(-1, 3.5), ylim = c(0, 1))\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_5)\nx &lt;- seq(from = -0.5, to = 3, by = 0.1)\narsenic &lt;- quantile(wells$c_arsenic, c(0.05, 0.5, 0.95))\nxx &lt;- expand.grid(c_dist100 = x, c_arsenic = arsenic)\nxx$dxa &lt;- xx$c_dist100*xx$c_arsenic\nxx$ypred &lt;- plogis(cf[1] + cf[2]*xx$c_dist100 + cf[3]*xx$c_arsenic + cf[4]*xx$dxa)\nhead(xx)\n\n\n  c_dist100 c_arsenic       dxa     ypred\n1      -0.5  -1.09693 0.5484652 0.5441541\n2      -0.4  -1.09693 0.4387722 0.5272266\n3      -0.3  -1.09693 0.3290791 0.5102363\n4      -0.2  -1.09693 0.2193861 0.4932223\n5      -0.1  -1.09693 0.1096930 0.4762239\n6       0.0  -1.09693 0.0000000 0.4592805\n\n\nCode\n# Add lines for model prediction with three levels of arsenic (5, 50, 95th)\nlines(xx$c_dist100[xx$c_arsenic == arsenic[1]], xx$ypred[xx$c_arsenic == arsenic[1]], lty = 1)\nlines(xx$c_dist100[xx$c_arsenic == arsenic[2]], xx$ypred[xx$c_arsenic == arsenic[2]], lty = 2)\nlines(xx$c_dist100[xx$c_arsenic == arsenic[3]], xx$ypred[xx$c_arsenic == arsenic[3]], lty = 3)\n\n# Add text to plot\nma &lt;- mean(wells$arsenic)\ntext(x = -1, y = 0.4, labels = paste(\"As:\", round(arsenic[1] + ma, 2)), pos = 4, cex = 0.7)\ntext(x = -1, y = 0.7, labels = paste(\"As:\", round(arsenic[2] + ma, 2)), pos = 4, cex = 0.7)\ntext(x = -1, y = 0.95, labels = paste(\"As:\", round(arsenic[3] + ma, 2)), pos = 4, cex = 0.7)\n\n\n\n\n\n\n\n\n\n\nNote that the interaction term is negative, but that its size is uncertain. Below the posterior probability for the interaction, about 4 % above zero, so the 95 % compatibility interval includes zero.\n\n\nCode\ns5 &lt;- data.frame(fit_5)\ncolnames(s5) &lt;- c(\"b0\", \"dist\", \"arsenic\", \"dxa\")\n\nplot(density(s5$dxa), main = \"Posterior prob. interaction\", \n     xlab = \"c_dist100:c_arsenic\")\nabline(v = 0, col = \"blue\")\ntext(0.1, 0.7, labels = paste(\"P(x &gt; 0) = \", round(mean(s5$dxa &gt; 0), 3)), \n     cex = 0.8)\nlines(quantile(s5$dxa, prob = c(0.025, 0.975)), c(0.1, 0.1), col = \"blue\")\npoints(median(s5$dxa), 0.1, pch = 21, bg = \"blue\")\n\n\n\n\n\n\n\n\n\nCode\n# Posterior interval\nposterior_interval(fit_5, prob = 0.95)\n\n\n                          2.5%       97.5%\n(Intercept)          0.2744100  0.42970803\nc_dist100           -1.0821656 -0.67956972\nc_arsenic            0.3903326  0.55106241\nc_dist100:c_arsenic -0.3837574  0.01596276\n\n\n\nAdd social predictors\nGelman et al’s model, p. 252.\n\n\nCode\nwells$c_educ4 &lt;- wells$educ4 - mean(wells$educ4)\n\nfit_8 &lt;- stan_glm(switch ~ c_dist100 + c_arsenic + c_educ4 +\n                      c_dist100:c_educ4 + c_arsenic:c_educ4,\n                  family = binomial(link=\"logit\"), data = wells, refresh = 0)\nprint(fit_8)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      switch ~ c_dist100 + c_arsenic + c_educ4 + c_dist100:c_educ4 + \n       c_arsenic:c_educ4\n observations: 3020\n predictors:   6\n------\n                  Median MAD_SD\n(Intercept)        0.3    0.0  \nc_dist100         -0.9    0.1  \nc_arsenic          0.5    0.0  \nc_educ4            0.2    0.0  \nc_dist100:c_educ4  0.3    0.1  \nc_arsenic:c_educ4  0.1    0.0  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "16-stat25.html#check-residuals-in-logistic-regression-optional",
    "href": "16-stat25.html#check-residuals-in-logistic-regression-optional",
    "title": "16  S: Logistic Regression",
    "section": "16.4 Check residuals in logistic regression (optional!)",
    "text": "16.4 Check residuals in logistic regression (optional!)\nSee Gelman et al. Section 14.5 (pp. 253-256).\n\n\nCode\n# This function is from Gelman et al. It calculates binned residuals.\n# Input: x = fitted values (y-hat) or a predictor variable, y = residuals\nbinned_resids &lt;- function (x, y, nclass=sqrt(length(x))){\n  breaks.index &lt;- floor(length(x)*(1:(nclass-1))/nclass)\n  breaks &lt;- c (-Inf, sort(x)[breaks.index], Inf)\n  output &lt;- NULL\n  xbreaks &lt;- NULL\n  x.binned &lt;- as.numeric (cut (x, breaks))\n  for (i in 1:nclass){\n    items &lt;- (1:length(x))[x.binned==i]\n    x.range &lt;- range(x[items])\n    xbar &lt;- mean(x[items])\n    ybar &lt;- mean(y[items])\n    n &lt;- length(items)\n    sdev &lt;- sd(y[items])\n    output &lt;- rbind (output, c(xbar, ybar, n, x.range, 2*sdev/sqrt(n)))\n  }\n  colnames (output) &lt;- c (\"xbar\", \"ybar\", \"n\", \"x.lo\", \"x.hi\", \"2se\")\n  return (list (binned=output, xbreaks=xbreaks))\n}\n\n\n\nPlot binned residuals\n\n\nCode\npred8 &lt;- fitted(fit_8)\nbr8 &lt;- binned_resids(pred8, wells$switch-pred8, nclass=40)$binned\nplot(range(br8[,1]), range(br8[,2],br8[,6],-br8[,6]),\n     xlab=\"Estimated  Pr (switching)\", ylab=\"Average residual\",\n     type=\"n\", main=\"Binned residual plot\", mgp=c(2,.5,0), ylim = c(-0.2, 0.2))\nabline(0,0, col=\"gray\", lwd=.5)\nlines(br8[,1], br8[,6], col=\"gray\", lwd=.5)\nlines(br8[,1], -br8[,6], col=\"gray\", lwd=.5)\npoints(br8[,1], br8[,2], pch=20, cex=.5)\n\n\n\n\n\n\n\n\n\nCode\n#lines(lowess(pred8, wells$switch-pred8), col = \"red\", lty = 3)\n\n\n\nBinned residuals with respect to predictors: Distance (left) and Arsenic level (right)\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Distance\nbr &lt;- binned_resids(wells$dist, wells$switch-pred8, nclass=40)$binned\nplot(range(br[,1]), range(br[,2],br[,6],-br[,6]),\n     xlab=\"Distance to nearest safe well\", ylab=\"Average residual\",\n     type=\"n\", main=\"Binned residual plot\", mgp=c(2,.5,0), ylim = c(-0.2, 0.2))\nabline(0,0, col=\"gray\", lwd=.5)\nn_within_bin &lt;- length(wells$switch)/nrow(br)\nlines(br[,1], br[,6], col=\"gray\", lwd=.5)\nlines(br[,1], -br[,6], col=\"gray\", lwd=.5)\npoints(br[,1], br[,2], pch=20, cex=.5)\nlines(lowess(wells$dist, wells$switch-pred8), col = \"red\", lty = 3)\n\n#Arsenic\nbr &lt;- binned_resids(wells$arsenic, wells$switch-pred8, nclass=40)$binned\nplot(range(0,br[,1]), range(br[,2],br[,6],-br[,6]),\n     xlab=\"Arsenic level\", ylab=\"Average residual\",\n     type=\"n\", main=\"Binned residual plot\", mgp=c(2,.5,0), ylim = c(-0.2, 0.2))\nabline (0,0, col=\"gray\", lwd=.5)\nlines (br[,1], br[,6], col=\"gray\", lwd=.5)\nlines (br[,1], -br[,6], col=\"gray\", lwd=.5)\npoints (br[,1], br[,2], pch=20, cex=.5)\nlines(lowess(wells$arsenic, wells$switch-pred8), col = \"red\", lty = 3)\n\n\n\n\n\n\n\n\n\n\nGelman et al’s final model with log-arsenic, p. 254.\n\n\nCode\nwells$log_arsenic &lt;- log(wells$arsenic)\nwells$c_log_arsenic &lt;- wells$log_arsenic - mean(wells$log_arsenic)\n\nfit_9 &lt;- stan_glm(switch ~ c_dist100 + c_log_arsenic + c_educ4 +\n                      c_dist100:c_educ4 + c_log_arsenic:c_educ4,\n                  family = binomial(link=\"logit\"), data = wells, refresh = 0)\nprint(fit_9, digits = 2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      switch ~ c_dist100 + c_log_arsenic + c_educ4 + c_dist100:c_educ4 + \n       c_log_arsenic:c_educ4\n observations: 3020\n predictors:   6\n------\n                      Median MAD_SD\n(Intercept)            0.34   0.04 \nc_dist100             -1.01   0.11 \nc_log_arsenic          0.91   0.07 \nc_educ4                0.18   0.04 \nc_dist100:c_educ4      0.35   0.11 \nc_log_arsenic:c_educ4  0.06   0.07 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nPlot the model to understand the distance:education interaction, left on the log-odds scale, right on the probability scale.\n\n\nCode\npar(mfrow = c(1, 2))\n\n## Plot, with outcome on log-odds scale ----\nplot(NULL, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Mean-centered dist100\",\n     ylab = \"log-odds (Pr(switching | distance))\", \n     xlim = c(-1, 3.5), ylim = c(-6, 6))\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_9)\nx &lt;- seq(from = -0.5, to = 3, by = 0.1)\nedu &lt;- quantile(wells$c_educ4, c(0.05, 0.5, 0.95))\nxx &lt;- expand.grid(c_dist100 = x, c_educ4 = edu)\nxx$dxe &lt;- xx$c_dist100*xx$c_educ4\n# At arsenic = 0, so cf[3] and cf[5] are gone\nxx$ypred &lt;- cf[1] + cf[2]*xx$c_dist100 + cf[4]*xx$c_educ4 + cf[5]*xx$dxe\nhead(xx)\n\n\n  c_dist100   c_educ4       dxe     ypred\n1      -0.5 -1.207119 0.6035596 0.8293254\n2      -0.4 -1.207119 0.4828477 0.6869369\n3      -0.3 -1.207119 0.3621358 0.5445483\n4      -0.2 -1.207119 0.2414238 0.4021598\n5      -0.1 -1.207119 0.1207119 0.2597712\n6       0.0 -1.207119 0.0000000 0.1173826\n\n\nCode\n# Add lines for model prediction with three levels of education (5, 50, 95th)\nlines(xx$c_dist100[xx$c_educ4 == edu[1]], xx$ypred[xx$c_educ4 == edu[1]], lty = 1)\nlines(xx$c_dist100[xx$c_educ4 == edu[2]], xx$ypred[xx$c_educ4 == edu[2]], lty = 2)\nlines(xx$c_dist100[xx$c_educ4 == edu[3]], xx$ypred[xx$c_educ4 == edu[3]], lty = 3)\n\n# Add text to plot\n#text(x = -1, y = 0, labels = paste(\"As:\", round(arsenic[1], 2)), pos = 4, cex = 0.8)\n#text(x = -1, y = 0.7, labels = paste(\"As:\", round(arsenic[2], 2)), pos = 4, cex = 0.8)\n#text(x = -1, y = 2.2, labels = paste(\"As:\", round(arsenic[3], 2)), pos = 4, cex = 0.8)\n\n\n## Plot data, probability scale see Fig. 13.10 (left) in Gelman et al. ----\nplot(NULL, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Mean-centered dist100\",\n     ylab = \"Pr(switching | distance)\", \n     xlim = c(-1, 3.5), ylim = c(0, 1))\n\n# Model predictions, in data-frame called xx\ncf &lt;- coef(fit_9)\nx &lt;- seq(from = -0.5, to = 3, by = 0.1)\nedu &lt;- quantile(wells$c_educ4, c(0.05, 0.5, 0.95))\nxx &lt;- expand.grid(c_dist100 = x, c_educ4 = edu)\nxx$dxe &lt;- xx$c_dist100*xx$c_educ4\n# At arsenic = 0, so cf[3] and cf[5] are gone\nxx$ypred &lt;- plogis(cf[1] + cf[2]*xx$c_dist100 + cf[4]*xx$c_educ4 + cf[5]*xx$dxe)\nhead(xx)\n\n\n  c_dist100   c_educ4       dxe     ypred\n1      -0.5 -1.207119 0.6035596 0.6962123\n2      -0.4 -1.207119 0.4828477 0.6652852\n3      -0.3 -1.207119 0.3621358 0.6328698\n4      -0.2 -1.207119 0.2414238 0.5992065\n5      -0.1 -1.207119 0.1207119 0.5645800\n6       0.0 -1.207119 0.0000000 0.5293120\n\n\nCode\n# Add lines for model prediction with three levels of education (5, 50, 95th)\nlines(xx$c_dist100[xx$c_educ4 == edu[1]], xx$ypred[xx$c_educ4 == edu[1]], lty = 1)\nlines(xx$c_dist100[xx$c_educ4 == edu[2]], xx$ypred[xx$c_educ4 == edu[2]], lty = 2)\nlines(xx$c_dist100[xx$c_educ4 == edu[3]], xx$ypred[xx$c_educ4 == edu[3]], lty = 3)\n\n# Add text to plot\nme &lt;- mean(wells$educ)\ntext(x = 1, y = 0.02, labels = paste(\"Edu:\", round(edu[1] + me, 1), \"y\"), pos = 4, cex = 0.8)\ntext(x = 1, y = 0.4, labels = paste(\"Edu:\", round(edu[2] + me, 1), \"y\"), pos = 4, cex = 0.8)\ntext(x = 1, y = 0.6, labels = paste(\"Edu:\", round(edu[3] + me, 1), \"y\"), pos = 4, cex = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ns9 &lt;- data.frame(fit_9)\nplot(density(s9$c_dist100.c_educ4), main = \"DistxEdu interaction\", \n     xlab = \"c_dist100:c_educ4\")\nabline(v = 0, col = \"blue\")\nlines(quantile(s9$c_dist100.c_educ4, prob = c(0.025, 0.975)), c(0.1, 0.1), col = \"blue\")\npoints(median(s9$c_dist100.c_educ4), 0.1, pch = 21, bg = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "16-stat25.html#practice",
    "href": "16-stat25.html#practice",
    "title": "16  S: Logistic Regression",
    "section": "Practice",
    "text": "Practice\nThe practice problems are labeled Easy (E), Medium (M), and Hard (H), (as in McElreath (2020)).\n\nEasy\n\n16E1.\n\nWhen is it safe to interpret the odds ratio (OR) as a relative risk (RR).\nIs it always the case that OR &gt; RR?\n\n\n\n\n16E2.\nWhich of the following statements about logistic regression are true?\nSelect one or more alternatives:\n\nAssume that residuals are normally distributed\nEstimates probabilities\nHas higher power than linear regression\nCan only be used with multiple predictors\nIs defined for outcomes in the interval [-1, 1]\nInvolve a logit link function\nUsed to model binary outcomes\nCan be used to fit an intercept only model to estimate a single proportion\nThe exponentiated intercept, \\(exp(b_0)\\) can be interpreted as an odds ratio\nRegression coefficients (“slopes”) divided by 4, \\(b_1/4\\), can be interpreted as an upper limit to the probability difference associated with a one-unit difference in the predictor\n\n\n\n\n\nMedium\n\n16M1. Make sense of this: “In logistic regression variables are interacting with them selves: The effect of a variable X on the probability of outcome depends on the level of X!”\n\n\n\n16M2. Figure below is from the Bangladesh well-switching data discussed above.\n\nEstimate by eye the predicted difference in probability of switching wells between people living 200 m compared to 0 m (next to) the nearest safe well.\nEstimate by eye the odds ratio for the same comparison, between people living 200 compared to 0 m from the nearest well.\n\n\n\n\nCode\n# function to jitter data points\njitter_binary &lt;- function(a, jitt = 0.05){\n  ifelse(a == 0, runif(length(a), 0, jitt), runif(length(a), 1- jitt, 1))\n}\n\nwells$switch_jitter &lt;- jitter_binary(wells$switch)\n\n# Plot jittered data\nplot(wells$dist100, wells$switch_jitter, pch = 16, col = rgb(0, 0, 1, 0.1), \n     xlab = \"Distance (in 100-m) to nearest safe well\",\n     ylab = \"Pr(switching | distance)\")\ncf &lt;- coef(fit_1)\nx &lt;- seq(from = 0, to = 4, by = 0.1)\nypred &lt;- plogis(cf[1] + cf[2]*x)\nlines(x, ypred)\n\n# Grid lines\nfor (j in seq(0, 3.5, 0.25)) { lines(c(j, j), c(0, 1), lty =2, col = \"grey\")}\nfor (j in seq(0, 1.0, 0.1)) { lines(c(0, 3.5), c(j, j), lty =2, col = \"grey\")}\n\n\n\n\n\n\n\n\n\n\n\n16M3. Below regression estimate for the model displayed in 16M2. Use the coefficients to calculate answers to 16M2\n\n\n\nCode\nprint(fit_1, digits = 2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      switch ~ dist100\n observations: 3020\n predictors:   2\n------\n            Median MAD_SD\n(Intercept)  0.61   0.06 \ndist100     -0.62   0.09 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\n\nHard\n\n16H1. Revisit the well-switching data discussed above. Fit a model predicting well-switching from distance and arsenic-level as well as their interaction. Visualize and interpret the result.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "16-stat25.html#session-info",
    "href": "16-stat25.html#session-info",
    "title": "16  S: Logistic Regression",
    "section": "(Session Info)",
    "text": "(Session Info)\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=Swedish_Sweden.utf8  LC_CTYPE=Swedish_Sweden.utf8   \n[3] LC_MONETARY=Swedish_Sweden.utf8 LC_NUMERIC=C                   \n[5] LC_TIME=Swedish_Sweden.utf8    \n\ntime zone: Europe/Stockholm\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] rstanarm_2.32.1 Rcpp_1.0.14    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     dplyr_1.1.4          farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] shinystan_2.6.0      promises_1.3.3       shinyjs_2.1.0       \n[10] digest_0.6.37        mime_0.13            lifecycle_1.0.4     \n[13] StanHeaders_2.32.10  survival_3.7-0       magrittr_2.0.3      \n[16] posterior_1.6.1      compiler_4.4.2       rlang_1.1.6         \n[19] tools_4.4.2          igraph_2.1.4         yaml_2.3.10         \n[22] knitr_1.50           htmlwidgets_1.6.4    pkgbuild_1.4.8      \n[25] curl_6.4.0           plyr_1.8.9           RColorBrewer_1.1-3  \n[28] dygraphs_1.1.1.6     abind_1.4-8          miniUI_0.1.2        \n[31] grid_4.4.2           stats4_4.4.2         xts_0.14.1          \n[34] xtable_1.8-4         inline_0.3.21        ggplot2_3.5.2       \n[37] scales_1.4.0         gtools_3.9.5         MASS_7.3-61         \n[40] cli_3.6.5            rmarkdown_2.29       reformulas_0.4.1    \n[43] generics_0.1.4       RcppParallel_5.1.10  rstudioapi_0.17.1   \n[46] reshape2_1.4.4       minqa_1.2.8          rstan_2.32.7        \n[49] stringr_1.5.1        shinythemes_1.2.0    splines_4.4.2       \n[52] bayesplot_1.13.0     parallel_4.4.2       matrixStats_1.5.0   \n[55] base64enc_0.1-3      vctrs_0.6.5          V8_6.0.4            \n[58] boot_1.3-31          Matrix_1.7-1         jsonlite_2.0.0      \n[61] crosstalk_1.2.1      glue_1.8.0           nloptr_2.2.1        \n[64] codetools_0.2-20     distributional_0.5.0 DT_0.33             \n[67] stringi_1.8.7        gtable_0.3.6         later_1.4.2         \n[70] QuickJSR_1.8.0       lme4_1.1-37          tibble_3.3.0        \n[73] colourpicker_1.3.0   pillar_1.10.2        htmltools_0.5.8.1   \n[76] R6_2.6.1             Rdpack_2.6.4         evaluate_1.0.3      \n[79] shiny_1.11.1         lattice_0.22-6       markdown_2.0        \n[82] rbibutils_2.3        backports_1.5.0      threejs_0.3.4       \n[85] httpuv_1.6.16        rstantools_2.4.0     gridExtra_2.3       \n[88] nlme_3.1-166         checkmate_2.3.2      xfun_0.52           \n[91] zoo_1.8-14           pkgconfig_2.0.3     \n\n\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>S: Logistic Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amrhein, V., Greenland, S., & McShane, B. (2019). Retire\nstatisticial significance. Nature, 567, 305–307.\n\n\nBanack, H. R., & Kaufman, J. S. (2014). The obesity paradox:\nUnderstanding the effect of obesity on mortality among individuals with\ncardiovascular disease. Preventive Medicine, 62,\n96–102.\n\n\nBaron, R. M., & Kenny, D. A. (1986). The moderator–mediator variable\ndistinction in social psychological research: Conceptual, strategic, and\nstatistical considerations. Journal of Personality and Social\nPsychology, 51(6), 1173.\n\n\nBlitzstein, J. K., & Hwang, J. (2019). Introduction to\nprobability. Chapman; Hall/CRC.\n\n\nBox, G. E. (1979). Robustness in the strategy of scientific model\nbuilding. In Robustness in statistics (pp. 201–236). Elsevier.\n\n\nBroman, K., Cetinkaya-Rundel, M., Nussbaum, A., Paciorek, C., Peng, R.,\nTurek, D., & Wickham, H. (2017). Recommendations to funding agencies\nfor supporting reproducible research. American Statistical\nAssociation, 2, 1–4.\n\n\nChristensen, L., Turner, L. A., & Johnson, R. B. (2023).\nRandomized designs in psychological research.\n\n\nClayton, A. (2021). Bernoulli’s fallacy: Statistical illogic and the\ncrisis of modern science. Columbia University Press.\n\n\nElwert, F., & Winship, C. (2014). Endogenous selection bias: The\nproblem of conditioning on a collider variable. Annual Review of\nSociology, 40, 31–53.\n\n\nEriksson, C., Hilding, A., Pyko, A., Bluhm, G., Pershagen, G., &\nÖstenson, C.-G. (2014). Long-term aircraft noise exposure and body mass\nindex, waist circumference, and type 2 diabetes: A prospective study.\nEnvironmental Health Perspectives, 122(7), 687–694.\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other\nstories. Cambridge University Press.\n\n\nGilovich, T. (2008). How we know what isn’t so. Simon;\nSchuster.\n\n\nGuber, D. (1999). Getting what you pay for. Journal of Statistics\nEducation, 7(2).\n\n\nHaack, S. (2011). Defending science-within reason: Between scientism\nand cynicism. Prometheus Books.\n\n\nHand, D. J. (2014). The improbability principle: Why coincidences,\nmiracles, and rare events happen every day. Scientific\nAmerican/Farrar, Straus; Giroux.\n\n\nHernán, M. A. (2018). The c-word: Scientific euphemisms do not improve\ncausal inference from observational data. American Journal of Public\nHealth, 108(5), 616–619.\n\n\nHolland, P. W. (1986). Statistics and causal inference. Journal of\nthe American Statistical Association, 81(396), 945–960.\n\n\nHowell, D. C. (2012). Statistical methods for psychology.\nCengage Learning.\n\n\nJaynes, E. T. (1985). Bayesian methods: General background (J.\nH. Justice, Ed.). Cambridge University Press.\n\n\nKantowitz, B. H., Roediger III, H. L., & Elmes, D. G. (2014).\nExperimental psychology. Cengage Learning.\n\n\nLambert, M. C., Cartledge, G., Heward, W. L., & Lo, Y. (2006).\nEffects of response cards on disruptive behavior and academic responding\nduring math lessons by fourth-grade urban students. Journal of\nPositive Behavior Interventions, 8(2), 88–99.\n\n\nMcElreath, R. (2020). Statistical rethinking: A bayesian course with\nexamples in r and stan. Chapman; Hall/CRC.\n\n\nMichal, A. L., & Shah, P. (2024). A practical significance bias in\nlaypeople’s evaluation of scientific findings. Psychological\nScience, 35(4), 315–327.\n\n\nMichell, J. (1997). Quantitative science and the definition of\nmeasurement in psychology. British Journal of Psychology,\n88(3), 355–383.\n\n\nMorgan, S. L., & Winship, C. (2015). Counterfactuals and causal\ninference. Cambridge University Press.\n\n\nMorgenstern, H., Wakefield, J., et al. (2021). Ecologic studies and\nanalysis. Modern Epidemiology. 4th Ed. Wolters Kluwer.\n\n\nMountain, L. (2006). Safety cameras: Stealth tax or life-savers?\nSignificance, 3(3), 111–113.\n\n\nNewbold, D. J., Laumann, T. O., Hoyt, C. R., Hampton, J. M., Montez, D.\nF., Raut, R. V., Ortega, M., Mitra, A., Nielsen, A. N., Miller, D. B.,\net al. (2020). Plasticity and spontaneous activity pulses in disused\nhuman brain circuits. Neuron, 107(3), 580–589.\n\n\nNorton, H. J., & Divine, G. (2015). Simpson’s paradox… and how to\navoid it. Significance, 12(4), 40–43.\n\n\nPearl, J. (2014). Comment: Understanding simpson’s paradox. The\nAmerican Statistician, 68(1), 8–13.\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation:\nGraphical causal models for observational data. Advances in Methods\nand Practices in Psychological Science, 1(1), 27–42.\n\n\nRothman, K. J. (2012). Epidemiology: An introduction. Oxford\nuniversity press.\n\n\nSagarin, B. J., West, S. G., Ratnikov, A., Homan, W. K., Ritchie, T. D.,\n& Hansen, E. J. (2014). Treatment noncompliance in randomized\nexperiments: Statistical approaches and design issues. Psychological\nMethods, 19(3), 317.\n\n\nSchellenberg, E. G. (2004). Music lessons enhance IQ. Psychological\nScience, 15(8), 511–514.\n\n\nSmith, P. L., & Little, D. R. (2018). Small is beautiful: In defense\nof the small-n design. Psychonomic Bulletin & Review,\n25(6), 2083–2101.\n\n\nSnow, J. (1855). On the mode of communication of cholera. John\nChurchill.\n\n\nStansfeld, S. A., Berglund, B., Clark, C., Lopez-Barrio, I., Fischer,\nP., Öhrström, E., Haines, M. M., Head, J., Hygge, S., Van Kamp, I., et\nal. (2005). Aircraft and road traffic noise and children’s cognition and\nhealth: A cross-national study. The Lancet, 365(9475),\n1942–1949.\n\n\nSteiner, P. M., Shadish, W. R., & Sullivan, K. J. (2023).\nFrameworks for causal inference in psychological science.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103(2684), 677–680.\n\n\nTirado, C., Gerdfeldter, B., & Nilsson, M. E. (2021). Individual\ndifferences in the ability to access spatial information in lag-clicks.\nThe Journal of the Acoustical Society of America,\n149(5), 2963–2975.\n\n\nVan Hedger, S. C., Nusbaum, H. C., Clohisy, L., Jaeggi, S. M.,\nBuschkuehl, M., & Berman, M. G. (2019). Of cricket chirps and car\nhorns: The effect of nature sounds on cognitive performance.\nPsychonomic Bulletin & Review, 26(2), 522–530.\n\n\nWest, S. G., & Thoemmes, F. (2010). Campbell’s and rubin’s\nperspectives on causal inference. Psychological Methods,\n15(1), 18.\n\n\nWilkinson, L. (1999). APA task force on statistical inference.\nStatistical methods in psychology journals: Guidelines and explanations.\nAmerican Psychologist, 54(8), 594–604.\n\n\nWilson, J. P., & Rule, N. O. (2015). Facial trustworthiness predicts\nextreme criminal-sentencing outcomes. Psychological Science,\n26(8), 1325–1331.",
    "crumbs": [
      "References"
    ]
  }
]